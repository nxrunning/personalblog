[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Nien Xiang Tou, PhD",
    "section": "",
    "text": "Data and numbers make me tick. This blog documents my personal projects and my learning journey in programming languages such as R and Python. I write about various topics including statistics, data science, data visualisation, coding and sport science.\nAll views expressed here are my own."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Data visualisation\n\n\nFootball\n\n\n\nThis post presents the use of the reactable package in R to create an interactive table to visualise data on the FIFA World Cup 22 participating teams.\n\n\n\nNien Xiang Tou\n\n\nNov 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nThis post presents the use of both bayesian and frequentist approaches to solve the famous Monty Hall problem.\n\n\n\nNien Xiang Tou\n\n\nApr 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSport science\n\n\n\nThis post employs the concept of critical spped to analyse the pacing strategy in the 1500m event finals at the Tokyo Olympics.\n\n\n\nNien Xiang Tou\n\n\nAug 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\nThis post presents how Bayesian inference explains why different individuals appraise similar evidence differently.\n\n\n\nNien Xiang Tou\n\n\nMar 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData visualisation\n\n\nFootball\n\n\n\nThis post visualises the playing styles of English Premier League football teams with the use of radar charts.\n\n\n\nNien Xiang Tou\n\n\nNov 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSport science\n\n\n\nThis post examines the prevalence of the end spurt in marathon running based on data from Standard Chartered Singapore Marathon 2019.\n\n\n\nNien Xiang Tou\n\n\nJul 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFootball\n\n\nData visualisation\n\n\n\nThis post examines whether Liverpool’s title winning success is attributed to luck or genuine performance based on the expected goals metric.\n\n\n\nNien Xiang Tou\n\n\nJul 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData visualisation\n\n\n\nThis article documents the visualisation of Singapore’s Covid-19 clusters on the map using R programming\n\n\n\nNien Xiang Tou\n\n\nMay 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSport science\n\n\n\nThis post examines whether variation in marathon running pace differs among runners of different performance levels.\n\n\n\nNien Xiang Tou\n\n\nMay 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science\n\n\n\nThis post documents the use of Python to scrape popular running books from Goodreads using the BeautifulSoup package.\n\n\n\nNien Xiang Tou\n\n\nMay 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSport science\n\n\n\nThis post writes about the phenomenon of reduced blood lactate concentration found with increasing altitude.\n\n\n\nNien Xiang Tou\n\n\nApr 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData visualisation\n\n\n\nThis article visualises how the covid-19 pandemic has evolved in certain countries using animated line plots in R.\n\n\n\nNien Xiang Tou\n\n\nMar 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science\n\n\nFootball\n\n\n\nThis article documents a walk-through on web scraping data on English Premier League teams from Wikipedia and visualising the data using R.\n\n\n\nNien Xiang Tou\n\n\nFeb 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science\n\n\nFootball\n\n\n\nThis post examines and compares the competitiveness of different English Premier LEague seasons based on coefficient of variation metric.\n\n\n\nNien Xiang Tou\n\n\nFeb 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science\n\n\nFootball\n\n\n\nThis post explores the use of logistic regression to examine whether football possession can predict wins in football with data from EPL 18/19.\n\n\n\nNien Xiang Tou\n\n\nAug 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSport science\n\n\n\nThis post highlights the importance of study perception in the sport science domain, especially in the context of exercise regulation\n\n\n\nNien Xiang Tou\n\n\nAug 3, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nien Xiang Tou",
    "section": "",
    "text": "Read more about me here."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Your-perception-is-your-reality/index.html",
    "href": "posts/Your-perception-is-your-reality/index.html",
    "title": "Your perception is your reality",
    "section": "",
    "text": "We experience a wide range of sensations during exercise. This article highlights the importance of studying perception, especially in the context of regulation of exercise.\nHave you ever stood at the end of a marathon, and witnessed how many people sprinted across the finish line? How is it possible for somebody who slowed down or was walking halfway through the race and yet managed to run the fastest right at the end of the race?\nThis is the end-spurt phenomenon commonly seen in endurance events among elites as well as the recreational athletes. Such phenomenon challenges the concept of fatigue, and this was exactly what sparked my interest in pursuing my PhD study that aims to better understand individual’s perceptual responses in self-regulation of exercise.\nTraditionally, it has been thought that fatigue stems from your physiological limits. From such a perspective, why somebody slows down during a race is completely due to physiological reasons such as depletion of muscle glycogen, lactate, environmental stress or the inability of your heart to supply enough oxygen to the muscles. As a result, the athlete is unable to recruit more muscle fibers to meet the exercise demands and hence slowing down. However, if such hypothesis is true, it is not possible for the athlete to speed up at the end of the race when he or she is supposed to feel the most exhausted."
  },
  {
    "objectID": "posts/Your-perception-is-your-reality/index.html#the-central-governor",
    "href": "posts/Your-perception-is-your-reality/index.html#the-central-governor",
    "title": "Your perception is your reality",
    "section": "The Central Governor",
    "text": "The Central Governor\nThis paradox was first challenged by famous South African sport scientist, Professor Tim Noakes, who also authored the running bible, Lore of Running. In 1996, he delivered the prestigious J. B. Wolffe Memorial Lecture at the American College of Sports Medicine (ACSM) conference. He pointed out that most people did not die running a marathon, which meant they could have ran faster during the race. Therefore, the traditional physiological model was flawed. He later went on to publish a new theoretical model to explain regulation of exercise performance, termed the “Central Governor”, which remains a hot debating topic in the sports science domain to date.\nThis model postulates that exercise performance is regulated by a centralized control center, the brain. Exercise performance is regulated in a manner based on constant feedback between the body and the brain. As we push the limits of our body, the brain generates a whole range of sensations consciously felt by us. The sensations felt during exercise are not the consequences of fatigue, but a protective mechanism to ensure that we complete our exercise in a safe manner. In short, the brain stops us from exercising to the point that we kill ourselves."
  },
  {
    "objectID": "posts/Your-perception-is-your-reality/index.html#why-study-perception",
    "href": "posts/Your-perception-is-your-reality/index.html#why-study-perception",
    "title": "Your perception is your reality",
    "section": "Why study perception?",
    "text": "Why study perception?\nIt is important to understand that in the context of an endurance event such as a marathon, athletes regulate their exercise performance in an anticipatory manner. This means individuals including novice runners naturally try to pace themselves in a sustainable manner to complete the race. Research have empirically shown that athletes slow down way before they reach the critical core temperature, and before the glycogen tank completely empties. Regulation of exercise is based on the perceived sensations during exercise, which help to provide feedback on finding the balance between safety and performance.\nWhy is there a need to study perceptual responses in sports given the tools to monitor a myriad of objective physiological measures such as heart rate, oxygen consumption and blood lactate? This is because when you think you have reached your point of exhaustion during exercise, the truth is you didn’t.\n\nIt is a perceived failure, and not a true failure.\n\nIt is quite impossible to run yourself to true failure in the context of exercise performance. Your true performance probably only happens when you face a life-and-death situation, such as being chased by a lion. This differentiation between a perceived and true failure is very important, as perceptual responses can be influenced by many other factors besides physiological variables.\nThis is the reason why placebo and psychological effects can affect performance. These effects alter your perceptual responses, and hence your performance. In the words of an Italian sport physiologist, Professor Samuele Marcora, effort perception is claimed to be the “cardinal exercise stopper”. Therefore, whatever factors that can influence your perception can ultimately influence your exercise performance.\nYour perception is in fact your reality."
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Nien Xiang Tou",
    "section": "",
    "text": "Please find my research work below.\n\n\nLiu, X., Tou, N. X., Gao, Q., Gwee, X., Wee, S. L., & Ng, T. P. (2022). Frailty and risk of cardiovascular disease and mortality. PloS One, 17(9), e0272527. https://doi.org/10.1371/journal.pone.0272527\nTou, N. X., Wee, S. L., Pang, B. W. J., Lau, L. K., Jabbar, K. A., Seah, W. T., ... & Ng, T. P. (2022). Association of fat mass index versus appendicular lean mass index with physical function–The Yishun Study. Aging and Health Research, 2(3), 100097. https://doi.org/10.1016/j.ahr.2022.100097\nChen, K. K., Lee, S. Y., Pang, B. W. J., Lau, L. K., Jabbar, K. A., Seah, W. T., Tou, N. X., ... & Wee, S. L. (2022). Associations of low handgrip strength and hand laterality with cognitive function and functional mobility–the Yishun Study. BMC Geriatrics, 22(1), 1-8. https://doi.org/10.1186/s12877-022-03363-2\nYu, C. C., Tou, N. X., & Low, J. A. (2022). A comparative study on mental health and adaptability between older and younger adults during the COVID-19 circuit breaker in Singapore. BMC Public Health, 22(1), 1-11. https://doi.org/10.1186/s12889-022-12857-y\nTou, N. X., Wee, S. L., Pang, B. W. J., Lau, L. K., Jabbar, K. A., Seah, W. T., ... & Ng, T. P. (2021). Associations of fat mass and muscle function but not lean mass with cognitive impairment: The Yishun Study. Plos One, 16(8), e0256702. https://doi.org/10.1371/journal.pone.0256702\nTou, N. X., Wee, S. L., Seah, W. T., Ng, D. H. M., Pang, B. W. J., Lau, L. K., & Ng, T. P. (2021). Effectiveness of Community-Delivered Functional Power Training Program for Frail and Pre-frail Community-Dwelling Older Adults: a Randomized Controlled Study. Prevention Science, 22, 1048-1059. https://doi.org/10.1007/s11121-021-01221-y​\nChoo, P. L., Tou, N. X., Pang, B. W. J., Lau, L. K., Jabbar, K. A., Seah, W. T., ... & Wee, S. L. (2021). Timed up and go (tug) reference values and predictive cutoffs for fall risk and disability in singaporean community-dwelling adults: Yishun Cross-Sectional Study and Singapore Longitudinal Aging Study. Journal of the American Medical Directors Association, 22(8), 1640-1645. https://doi.org/10.1016/j.jamda.2021.03.002\n​Tou, N. X., Kee, Y. H., Koh, K. T., Camiré, M., & Chow, J. Y. (2020). Singapore teachers’ attitudes towards the use of information and communication technologies in physical education. European Physical Education Review, 26(2), 481-494. https://doi.org/10.1177/1356336X19869734\n\n\n\nTou, N. X., Balasekaran, G., & Barbosa, T. M. (2021). Influence Of Exercise Experience On Perception Of Prescribed And Preferred Exercise Intensity. Medicine & Science in Sports & Exercise, 53(8S), 10-11. http://doi.org/10.1249/01.mss.0000759152.24965.1c\nTou, N. X., Balasekaran, G., & Barbosa, T. M. (2020). Effects of mental fatigue on maximal exercise test performance in physically active and sedentary adults. Medicine & Science in Sports & Exercise, 52(7S), 626. https://doi.org/10.1249/01.mss.0000681104.26974.97 \nTou, N.X., Balasekaran, G., Thor, D., Tan, I., & Lim, Z. Y. (2017). Validation Of A 10 point OMNI Rating Of Perceived Exertion Colored Scale. Medicine & Science in Sports & Exercise, 49(5S), 754. https://doi.org/10.1249/01.mss.0000519006.11251.49\n\n\n\nTowards Data Science. Combining Python and R for FIFA Football World Ranking Analysis. https://towardsdatascience.com/combining-python-and-r-for-fifa-football-world-ranking-analysis-d71bb6ceacdb"
  },
  {
    "objectID": "posts/Can-possession-predict-wins-in-football/index.html",
    "href": "posts/Can-possession-predict-wins-in-football/index.html",
    "title": "Can possession predict wins in football?",
    "section": "",
    "text": "Ball possession is an important performance indicator in football. This article explores the use of logistic regression model to examine a binary classification problem: whether ball possession predicts match win or not.\nBall possession in football is seen as an imperative aspect of the game. Many successful teams are often characterised by dominating long periods of possession. The first team that sprints to your mind with such nature of play is likely the Spanish giants, Barcelona. They are often defined by their “tiki-taka” style of play, which constitutes short passing and movement to maintain possession.\nPast research have claimed that possession is a critical performance indicator that can differentiate successful teams. Better performing teams tend to maintain significantly longer possession as compared to the less successful teams. In the context of football, it does not matter if you can possess the ball for extended periods but failing to win the match. This leads to an important question, can possession predict wins in football?\nIn view of the imminent English Premier League (EPL) season, let’s revisit last EPL season’s data to answer this question! The data set consists of all results from the total 380 matches played throughout the season. Each row represents the match statistics of a team in a given match. You may see a preview of the data below."
  },
  {
    "objectID": "posts/Can-possession-predict-wins-in-football/index.html#do-better-teams-have-greater-possession",
    "href": "posts/Can-possession-predict-wins-in-football/index.html#do-better-teams-have-greater-possession",
    "title": "Can possession predict wins in football?",
    "section": "Do better teams have greater possession?",
    "text": "Do better teams have greater possession?\nAs mentioned above, it has been claimed that ball possession is a performance indicator that can distinguish the more successful teams. Firstly, let’s visualise the possession of each team ranked by their ball possession!\n\n\n\n\n\nNo surprise that the EPL champions, Manchester City, with the most points tallied also had the most dominating ball possession among all the teams. From the graph, we can see that there is some truth to the claim, with the “top six” teams all ahead in the ball possession rankings as compared to the rest of the league. The highest ball possession recorded was a whopping 82% by Manchester City during match day 29 away to Bournemouth. However, the score did not truly reflect such one-sided dominance, with City edging Bournemouth by just a goal.\n\n\n\nEach data point represents the possession and match outcome of a team. 1 denotes a match win, while 0 denotes otherwise (draw or loss)."
  },
  {
    "objectID": "posts/Can-possession-predict-wins-in-football/index.html#does-ball-possession-translate-into-results",
    "href": "posts/Can-possession-predict-wins-in-football/index.html#does-ball-possession-translate-into-results",
    "title": "Can possession predict wins in football?",
    "section": "Does ball possession translate into results?",
    "text": "Does ball possession translate into results?\nIn the data visualization above, we can see that all results are classified into either wins or otherwise (draw or loss). With so much emphasis on ball possession in today’s football, this scatter plot seems to tell us otherwise, with no clear threshold of possession level that guarantees a win. We can see that a winning team can have close to 20% of possession while a non-winning team can have close to 80% of possession. For example, Crystal Palace pulled off a surprising win (3-2) against Manchester City at the Etihad Stadium on match day 18, with only 21% of ball possession during the match. Therefore, is ball possession a reliable predictor of match wins?"
  },
  {
    "objectID": "posts/Can-possession-predict-wins-in-football/index.html#logistic-regression",
    "href": "posts/Can-possession-predict-wins-in-football/index.html#logistic-regression",
    "title": "Can possession predict wins in football?",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nThis is a classification problem, as we use possession as an independent variable to predict the match result. The problem is simplified to be a binary classification scenario, whereby the result can only have two outcomes: either win or otherwise (i.e. draw or loss). Since the dependent variable is categorical in nature, logistic regression is employed to help us answer our question. In this example, we make use of the StatsModel module, which is an open-sourced Python module encompassing several statistical functions. You may see the summary of the logistic regressions result below.\n\n\nThe results showed that ball possession is a significant predictor of match wins. This suggests that possession is indeed a meaningful indicator of a match outcome. The coefficient of the possession variable is 0.0335, and this implies that when ball possession level increases by 1%, the odds of winning the match increase by 3.41%. Greater ball possession could translate to more creation of goal-scoring opportunities. Possessing the ball more also means reducing opponent’s chances of scoring since a team will need to have the ball first before they can even try to score. Such situations could contribute to a greater likelihood of winning the match."
  },
  {
    "objectID": "posts/Can-possession-predict-wins-in-football/index.html#how-accurate-is-this-model",
    "href": "posts/Can-possession-predict-wins-in-football/index.html#how-accurate-is-this-model",
    "title": "Can possession predict wins in football?",
    "section": "How accurate is this model?",
    "text": "How accurate is this model?\nIn this example, the logistic regression model was trained with the data from all matches in EPL season 18/19. We could deploy the model to predict a match outcome given an input amount of possession levels. The model predicts by calculating the probability of either outcomes (value between 0 and 1). In this example, if a given possession level has a probability of winning greater than 0.5, it will be predicted that the team wins the match and vice versa.\nIn the field of machine learning and data science, the accuracy rate of a classification model can often be interpreted through a confusion matrix. A confusion matrix is a simple table that shows the accuracy of the model by comparing the predicted values with the actual values in the data set. You may see the confusion matrix for this example below.\n\n\n\nConfusion matrix of logistic regression model. Note- 1 denotes win, 0 denotes otherwise (draw or loss).\n\n\nThe confusion matrix compares all 760 predicted outcomes with the actual recorded outcomes. From the matrix, we can see that 372 cases were correctly predicted to be non-wins, also termed the true negatives. 204 cases were incorrectly predicted to be non-wins when they actually produced match wins in reality. Such cases are termed false negatives. 79 cases were also incorrectly predicted to be winning outcomes when they were actually non-wins, also known as the false positives. Lastly, 105 cases were correctly predicted to be classified as match wins, which are regarded as the true positives.\n\nAccuracy rate = (372+105)/760 * 100\n\nThis model was found to be 62.76% accurate! I will leave it to your personal judgement whether this model is acceptable. Obviously, football is a game with so many variables that can influence the outcome of a match. Therefore, it is not surprising to expect such accuracy of a simple model with a single predictor variable. Nevertheless, this post showcases that ball possession is a significant variable that can help us predict match outcomes. However, possession itself does not tell the full story."
  },
  {
    "objectID": "posts/Is-the-EPL-getting-more-competitive/index.html",
    "href": "posts/Is-the-EPL-getting-more-competitive/index.html",
    "title": "Is the English Premier League getting more competitive?",
    "section": "",
    "text": "Is the English Premier League more or less competitive as compared to a decade ago? This article examines and compares the competitiveness of different seasons based on the coefficient of variation metric.\nPresently, Liverpool is en route to being runaway champions in the English Premier League (EPL), with 22 points separating them from second-placed Manchester City. Incredibly, the premier league leaders have gone a total of 42 games unbeaten at the moment, with their last defeat happening at the Ethihad Stadium on 3rd January 2019.\nInevitably, this invites a comparison between Liverpool and the “Invincible” Arsenal team. The latter currently holds the record for the longest unbeaten EPL run with 49 games in 2003/2004. To date, the record has stood for close to 16 years. Can Liverpool surpass this incredible record? In an online article, former Arsenal player Robert Pires commented that if Liverpool goes on to break the record, the league was much harder in the past as compared to present.\nIs he right to say that? This article examines whether the present EPL is indeed getting easier as compared to many years ago. First, let’s visualise how many points were required to become champions across the different past seasons since 2000/2001.\nBased on the figure, we can clearly see that teams need at least 80 points to win the titles. The “Invincible” Arsenal team achieved a total of 90 points in 2003/2004 to lift the championship trophy. However, it is interesting to point out that 90 points would be insufficient to win the league over the past three seasons. Liverpool fans would be especially frustrated that despite accumulating 97 points in 2018/2019 season, they were still that tiny bit shy of the title. Hence, is the league really becoming less competitive like what Robert Pires said?"
  },
  {
    "objectID": "posts/Is-the-EPL-getting-more-competitive/index.html#coefficient-of-variation",
    "href": "posts/Is-the-EPL-getting-more-competitive/index.html#coefficient-of-variation",
    "title": "Is the English Premier League getting more competitive?",
    "section": "Coefficient of variation",
    "text": "Coefficient of variation\nWith the aim to evaluate the competitiveness of the league, we will use the coefficient of variation (CV) metric. CV is calculated by dividing the standard deviation of a dataset by its mean. It simply represents the relative spread/variance of the data, and ranges between 0 to 1. In finance, CV is used to assess the volatility of stocks. In this article’s context, we compare the competitiveness of different EPL seasons based on each season’s CV. If a league is more competitive, we will expect that the point total of teams to be closer to one another, hence a lower CV. On the other hand, a higher CV represents less competitiveness as there is greater spread in point total among teams.\n\nThe figure above illustrates the CV across different seasons. Generally, we see there’s relatively greater variation in more recent seasons as compared to the past. For example, there’s 29% variation in 2003/2004 season as compared to 39% variation in 2018/2019. Based on the graph, the most competitive season was probably in 2010/2011 with the least variation of 25%. This suggests that there’s some truth in Robert Pires’s comments.\n\nNext, let’s further analyse the competitiveness of the league based on different standings of the table. The figure above illustrates the CV of the top six teams across different seasons. This tells a slightly different story as compared to the previous graph. The higher CVs were observed in 2003/2004 and 2004/2005 seasons, which suggests greater dispersion in point total among the top teams. This implies relatively less of a close battle for the title in these two seasons. In contrast, we see a clear trend of decreasing CV from season 2005/2006 onwards till about 2015/2016. This coincides with the shift of the traditional “Top Four” (Arsenal, Manchester United, Chelsea, Liverpool) dominance to the emergence of the “Big Six” with Tottenham Hotspur and Manchester City joining in to challenge for the title.\nAlthough there’s an increase in CV among the top six teams in recent seasons, it is still comparatively less than the “Invincible” season. This suggests that it is much more competitive among the top teams in recent seasons as compared to the past. This also indicates that challenging for a champions league qualifying spot is much tougher now than two decades ago.\nIn conclusion, while the league may be more competitive overall in the past, there is less competition among the top teams when comes to challenging for the title or qualifying for the champions league as compared to present seasons.\nDo you agree with Robert Pires’s comments?"
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "",
    "text": "Web scraping opens doors to a myriad of data sources online. Be it text or numbers, web scraping allows us to extract online data into spreadsheets in our computer. With reference to code from FC RSTATS, this article documents a walk through on how to scrape football data from the web and visualising the data using open-source programming platform, R."
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#required-libraries",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#required-libraries",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "1. Required Libraries",
    "text": "1. Required Libraries\nlibrary(rvest)\nlibrary(ggplot2)\nlibrary(plyr)\nlibrary(ggrepel)\nThe above are the required libraries in this walk-through. The two main libraries are rvest and ggplot2, which are used to perform web scraping and plotting graphs respectively.The plyr library helps us in data cleaning while the ggrepel library helps to avoid overlapping of labels in the graphs."
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#webpage",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#webpage",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "2. Webpage",
    "text": "2. Webpage\nurl <- \"https://en.wikipedia.org/wiki/2019%E2%80%9320_Premier_League\"\nIn this article, we are scraping data from a Wikipedia page, showing information on the present English Premier League. You may access the page through the url link specified above. Specifically, I am interested in extracting information on the goals scored (GF) and goals conceded (GA) of each team from the table below.\n\n\n\nScreenshot of the table in which data would be scraped"
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#scrape-the-data",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#scrape-the-data",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "3. Scrape the data",
    "text": "3. Scrape the data\nscraped_page <- read_html(url)\n\n#Scraping the Team Column\nTeams <- scraped_page %>%  \n    html_nodes(\"h2+ .wikitable th+ td\") %>%  \n    html_text() %>%  \n    as.character()\n\n#Scraping the GF column\nGoals_for <- scraped_page %>%\n  html_nodes(\"h2+ .wikitable td:nth-child(7) , th:nth-child(7) abbr\") %>%\n  html_text() %>%\n  as.numeric\n\n#Scraping the GA column\nGoals_against<- scraped_page %>%\n  html_nodes(\"h2+ .wikitable td:nth-child(8)\") %>%\n  html_text() %>%\n  as.numeric\nFirst, we read the web page and save the data to the variable named scraped_page. Based on the above table, we are interested in three columns: Team, GF and GA. Each column is scraped separately and saved to a variable name (i.e. Teams, Goals_for, Goals_against). The codes look identical to one another since we are just repeating the scraping process. The difference is to specify the location of the data to be scraped. The respective locations for each location are specified in red above. For example, “h2+ .wikitable th+ td” tells the code to locate the team column in the scraped page. Each location can be easily found through the use of SelectorGadget. Click the link to see more details."
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#cleaning-the-data",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#cleaning-the-data",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "4. Cleaning the data",
    "text": "4. Cleaning the data\n#Removing \"\\n\" in the teams \nTeams <- gsub(\"\\n\",\"\",Teams)\n\n#Remove NA case\nGoals_for <- na.omit(Goals_for)\n\n#Combining all scraped data into a dataframe\ndf <- data.frame(Teams, Goals_for, Goals_against)\n\n#Renaming the values of Liverpool and Manchester City \ndf$Teams <- mapvalues(df$Teams, from=c(\"Liverpool (Q)\",\"Manchester City[a]\"), to=c(\"Liverpool\", \"Manchester City\"))\nIf you preview each of the variable, you will find some slight discrepancies. For example, many of the teams have a ‘\\n’ in their names. Hence, the above code helps to clean the data and combines them into a nice spreadsheet below."
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#data-visualisation",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#data-visualisation",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "5. Data Visualisation",
    "text": "5. Data Visualisation\nFollowing the code from FC RSTATS, we can create a scatter plot as below. Each point represents each team in the league. The x and y axes represent the goals scored and goals conceded respectively. The plot is divided into quadrants by the two dotted lines. The horizontal dotted line represents the average goals conceded while the vertical horizontal dotted line represents the average goals scored.\nBased on this simple graph, we can see that the teams located on the bottom right are the better performing teams with both strong attack and defence. Conversely, the teams in the top left quadrant have the worst performance."
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#dividing-teams-into-four-clusters",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#dividing-teams-into-four-clusters",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "6. Dividing teams into four clusters",
    "text": "6. Dividing teams into four clusters\nTo better visualise the separation in performance, we can represent each quadrant in different colours. First, we create a new column in the spreadsheet and name it cluster. The code below will separate each team into their respective clusters based on their goal stats.\n#Creating a cluster column\ndf$Cluster = 0\n\n#Using for loop and ifelse statements to create different clusters \nfor(i in 1: nrow(df)) {\n  if (df$Goals_for[i] > mean(df$Goals_for) & df$Goals_against[i] < mean(df$Goals_against)) {\n    df$Cluster[i] = 1\n  } else if (df$Goals_for[i] < mean(df$Goals_for) & df$Goals_against[i] < mean(df$Goals_against)) {\n    df$Cluster[i] = 2 \n  } else if (df$Goals_for[i] > mean(df$Goals_for) & df$Goals_against[i] > mean(df$Goals_against)) {\n    df$Cluster[i] = 3\n  } else {\n    df$Cluster[i] = 4\n  }\n}\n\n#Making the cluster columns a factor\ndf$Cluster = factor(df$Cluster, levels = c(1, 2, 3, 4), \n                    labels = \n                    c(\"Strong attack\\nStrong defence\",\n                    \"Poor attack\\nStrong defence\",\n                    \"Strong attack\\nPoor defence\",\n                    \"Poor attack\\nPoor defence\"))\n                    \nNext, we will use the ggplot2 library to visualise our data. The comments in green help to explain each respective line of code.\n#Specifying the variables and using colours to represent each cluster\nggplot(df, aes(x = Goals_for, y = Goals_against, label = Teams, colour = Cluster))+\n  #Plotting each team on the graph using a geometric point\n  geom_point()+\n  #Plotting the vertical dotted line to represent the average goals scored\n  geom_vline(xintercept=mean(Goals_for), linetype=\"dashed\", \n  alpha = 0.4, colour = \"red\") +\n  #Plotting the horizontal dotted line to represent the average goals conceded\n  geom_hline(yintercept=mean(Goals_against), linetype=\"dashed\", \n  alpha = 0.4, colour = \"red\") +\n  #Geom_text_repel helps to avoid overlapping of labels in the points\n  geom_text_repel(aes(Goals_for, Goals_against, label = Teams), \n                  size = 2, colour = \"black\", fontface = \"bold\")+\n  #Labelling of axes and title\n  labs(title = \"EPL 2019/2020: Goals For vs Goals Against\",\n       x = \"Goals For\", y = \"Goals Against\",\n       colour = \" \")+\n  #Specifying position of the legend\n  theme(legend.position = \"top\")\n\n#Save the plot in your working directory\nggsave(\"output.jpeg\", dpi = 300)\n                    \nThe resulting plot is below.\n\n\n\n\n\nFull code for this article can be accessed through my github.\nPS: If you actually count the number of points on the plot, there are only 19 instead of 20. This is because Newcastle and Watford happen to have exactly the same goal stats at the time of writing."
  },
  {
    "objectID": "posts/Animated-data-viz-covid19/index.html",
    "href": "posts/Animated-data-viz-covid19/index.html",
    "title": "Animated Data Visualisation of the Covid-19 Pandemic using R",
    "section": "",
    "text": "Number of infected cases and deaths around the world continues to rise daily. This article visualises how the pandemic has evolved in certain countries using animated line plots on R."
  },
  {
    "objectID": "posts/Animated-data-viz-covid19/index.html#china-breakout",
    "href": "posts/Animated-data-viz-covid19/index.html#china-breakout",
    "title": "Animated Data Visualisation of the Covid-19 Pandemic using R",
    "section": "China Breakout",
    "text": "China Breakout\nWhilethe true origin of the virus remains debatable, the very first reported covid-19 case was detected in Wuhan City, Hubei Province of China. Since then, the numbers have skyrocketed. Using the numbers reported in the daily situation reports by World Health Organization (WHO), let’s visualise how the outbreak has developed since 1st February 2020 using gganimate on R.\n\n\n\n\n\nThe figure above illustrates how the number of cases increased day by day. The exact dates are reflected in the subtitle. Over the period of almost two months, the number of confirmed cases has increased from 11821 to 81601 at the point of writing. From the graph, we can see a sharp increase in cases on 14th February. It can also be seen that the rate of increase seems to decline since March."
  },
  {
    "objectID": "posts/Animated-data-viz-covid19/index.html#southeast-asia",
    "href": "posts/Animated-data-viz-covid19/index.html#southeast-asia",
    "title": "Animated Data Visualisation of the Covid-19 Pandemic using R",
    "section": "Southeast Asia",
    "text": "Southeast Asia\nNext, let’s visualise how the virus has developed closer to home. The figure below illustrates the breakout in certain Southeast Asian countries.\n\n\n\n\n\nAt the start of February, Singapore was leading the charts over its neighbours. Things have changed very quickly since early March. The number of reported cases in Malaysia surpassed Singapore on 15th March. Presently, the pandemic is faring much worse in many other Southeast Asian countries as compared to Singapore."
  },
  {
    "objectID": "posts/Animated-data-viz-covid19/index.html#rest-of-the-world",
    "href": "posts/Animated-data-viz-covid19/index.html#rest-of-the-world",
    "title": "Animated Data Visualisation of the Covid-19 Pandemic using R",
    "section": "Rest of the World",
    "text": "Rest of the World\nCovid-19 was announced a pandemic on 11th March 2020 given the alarming levels of the virus outbreak. In such a connected world today, it was hard for any countries to get off the hook. Let’s take a look at some other notable countries being hit.\n\n\n\n\n\nThe graph clearly shows the trajectories of the virus spread differ between countries. For example, there was a sharp increase in number of cases in South Korea during the second half of February. The rate of increase quickly slowed in March. On the other hand, Italy is still facing an exponential manner of spread at the moment. Based on the figures, we can infer the extent of success in countries’ attempt to contain the virus.\nData visualisation code and dataset can be found on my github."
  },
  {
    "objectID": "posts/lactate-paradox/index.html",
    "href": "posts/lactate-paradox/index.html",
    "title": "Lactate Paradox",
    "section": "",
    "text": "Blood lactate is associated with fatigue during maximal exercise. This post writes about the phenomenon of reduced blood lactate concentration found with increasing altitude.\nEnergy production during physical work comes from three energy systems: 1) adenosine triphosphate creatine phosphate (ATP-CP), 2) anaerobic glycolysis, and 3) aerobic system. All three systems work concurrently to produce energy, but dominance of any one system is dependent on exercise intensity and duration. Supply of energy for muscle contraction during endurance exercise primarily comes from either aerobic or anaerobic systems. The key difference between the two systems is that aerobic system requires oxygen in the production of energy but anaerobic system does not.\nIt has been traditionally believed that endurance exercise performance is limited by one’s maximal aerobic capacity (VO2max). Exercising beyond this capacity will shift the energy supply to become more reliant on the anaerobic energy system instead. Consequently, low oxygenation in muscles leads to accumulation of blood lactate, or more commonly known as lactic acid. In a classic study, high levels of blood lactate was found in frog muscles that were stimulated to contract to failure. This has led to the hypothesis that blood lactate is associated with physical fatigue."
  },
  {
    "objectID": "posts/lactate-paradox/index.html#exercising-in-altitude",
    "href": "posts/lactate-paradox/index.html#exercising-in-altitude",
    "title": "Lactate Paradox",
    "section": "Exercising in Altitude",
    "text": "Exercising in Altitude\nIf you have been in the mountains, you will notice that breathing gets more difficult with advancing altitude levels. Naturally, exercise performance also becomes more exhaustive under such conditions. Since blood lactate is associated with anaerobic energy pathways, higher lactate concentrations are expected at high altitude, whereby oxygen levels are lower. Indeed, this is so among individuals who first experienced altitude. However, peak blood lactate concentration was found to decrease with increasing altitude among individuals who were acclimatised to altitude.\nThe phenomenon of lower peak blood lactate concentration under greater hypoxic conditions at high altitude levels is puzzling to physiologists. This phenomenon is termed the “lactate paradox”. The paradox was confirmed in Operation Everest II, a study that examined acclimatisation effects of participants at altitude levels equivalent to the summit of Mount Everest, approximately 8848 metres high. It was discovered that the blood lactate concentrations among acclimatised subjects during maximal exercise at high altitude level were not significantly different as compared to during rest at sea levels.\nTo date, the lactate paradox remains poorly understood. Nevertheless, this phenomenon suggests that blood lactate is not necessarily associated with physical fatigue. Hence, the theoretical belief that endurance exercise performance is limited by accumulation of blood lactate in skeletal muscles is unlikely correct."
  },
  {
    "objectID": "posts/web-scraping-goodreads/index.html",
    "href": "posts/web-scraping-goodreads/index.html",
    "title": "Web Scraping Goodreads with BeautifulSoup: Popular runnning books",
    "section": "",
    "text": "Data is incredibly important to every analyst or data scientist. Web scraping is a valuable skill that allows us to access the limitless sources of data online. In my previous blogpost, I have documented the use of R to scrape football data from a wikipedia page. This post presents my attempt on scraping information of popular running books from Goodreads using Python programming language."
  },
  {
    "objectID": "posts/web-scraping-goodreads/index.html#required-tools",
    "href": "posts/web-scraping-goodreads/index.html#required-tools",
    "title": "Web Scraping Goodreads with BeautifulSoup: Popular runnning books",
    "section": "Required Tools",
    "text": "Required Tools\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\nimport pandas as pd\nWe will require four library packages for this project. The request package helps with making HTTP requests from the website of interest. The BeautifulSoup package is a popular web scrapping python library and this helps us to perform the main work. The urllib.parse helps us to join URL addresses and the pandas library helps us to tidy the data scraped into a nice data frame."
  },
  {
    "objectID": "posts/web-scraping-goodreads/index.html#website",
    "href": "posts/web-scraping-goodreads/index.html#website",
    "title": "Web Scraping Goodreads with BeautifulSoup: Popular runnning books",
    "section": "Website",
    "text": "Website\nThis page on Goodreads presents the top 50 books related to running (see below). It showcases some information of each book such as the title, author names and ratings. However, if you want to read more detailed description of each book, you will have to click on the link of the book title.\n\n\n\nScreenshot of website"
  },
  {
    "objectID": "posts/web-scraping-goodreads/index.html#set-up",
    "href": "posts/web-scraping-goodreads/index.html#set-up",
    "title": "Web Scraping Goodreads with BeautifulSoup: Popular runnning books",
    "section": "Set up",
    "text": "Set up\nIf you right click on most web pages, you may inspect its html codes. The codes are structured with several tags, classes and attributes that serve different purposes. Generally, web scraping locates the data that you are interested to extract based on information from these codes. The codes below help us to extract the website’s html and also create a BeautifulSoup object that we will further wrangle with. You may also examine the html file exported.\n# Specifying website url\nbase_site = \"https://www.goodreads.com/shelf/show/running\" \n\n# Make http request\nresponse = requests.get(base_site)\n\n# Get the html from webpage\nhtml = response.content\n\n# Creating a BeautifulSoup object with the use of a parser\nsoup = BeautifulSoup(html, \"lxml\")\n\n# Exporting html file\nwith open('popularrunningbooks.html', 'wb') as file:\n    file.write(soup.prettify('utf-8'))\nThe aim of this web scraping project was to extract relevant information regarding each of these 50 books: 1) book title, 2) author name(s), 3) book rating, 4) book pages, 5) book description. The general workflow to retrieve these information follows the same steps as if we were to manually do it. This involves us clicking on each of the book links and extract the data of interest. Hence, the very first step to help us automate this process is to extract this list of book links from the BeautifulSoup object we created earlier.\nHtml codes are generally built within many layers, similar to putting a present in several layers of gift boxes. Therefore, scraping data is akin to unwrapping the present layer by layer. Typically, a website’s content is hidden under the ‘div’ tag, which represents the outermost layer of the box. Hence, this is usually the starting point to unwrap our “present”. We could also specify the class and ID to help us better locate the data that we want. In this case, the book links are within the class “elementList”.\n# First layer: The element that contains all the data\ndivs = soup.find_all(\"div\", {\"class\": \"elementList\"})\n\n# Second layer: Extracting html tags that contain the links\nlinks = [div.find('a') for div in divs]\nThe url information of each book links are located in the links. However, each of the url extracted is only a partial web address. For example, the corresponding partial url link for the book “Born to Run” looks like ‘/book/show/6289283-born-to-run’. In order to get the full url, we will use the urljoin method from the urllib.parse package to join our base site web address with each of these partial url links.\n# Extracting the partial links  \nrelative_url = [link['href'] for link in links]  \n\n# Computing the full url addresses \nfull_url = [urljoin(base_site, relativeurl) for relativeurl in relative_url]\nIf you inspect the full_url list , some unnecessary non-book links were accidentally extracted as well. Hence, the code below will help to overcome this problem.\n# Filter only the book links\nbook_url = [url for url in full_url if \"https://www.goodreads.com/book/show\" in url]"
  },
  {
    "objectID": "posts/web-scraping-goodreads/index.html#scraping-information-of-each-book",
    "href": "posts/web-scraping-goodreads/index.html#scraping-information-of-each-book",
    "title": "Web Scraping Goodreads with BeautifulSoup: Popular runnning books",
    "section": "Scraping information of each book",
    "text": "Scraping information of each book\nFinally, we have arrived at the main web scraping work. Imagine clicking on each of the book links and retrieve the data we need. Programming helps us to automate this process. First, we create five empty lists, whereby each list will store its respective information.\nbook_description = []\nbook_author = []\nbook_title = []\nbook_rating = []\nbook_pages = []\nThe scraping process involves some similar steps stated earlier, whereby we have to retrieve the html code of each book link and locate the information we need. The same steps will be repeated for every link. The for-loop below helps us to perform this repetitive work.\n#creating a loop counter\ni = 0\n\n#Loop through all 50 books\nfor url in book_url:\n    \n    #connect to url page\n    note_resp = requests.get(url)\n    \n    #checking if the request is successful\n    if note_resp.status_code == 200:\n        print(\"URL{}: {}\".format(i+1, url))\n    \n    else:\n        print('Status code{}: Skipping URL #{}: {}'.format(note_resp.status_code, i+1, url))\n        i = i+1\n        continue\n    \n    #get HTML from url page\n    note_html = note_resp.content\n    \n    #create beautifulsoup object for url page\n    note_soup = BeautifulSoup(note_html, 'html.parser')\n    \n    #Extract Author particulars\n    author_divs = note_soup.find_all(\"div\",{\"class\":\"authorName__container\"})\n    author_text = author_divs[0].find_all('a')[0].text\n    book_author.append(author_text)\n    \n    #Extract title particulars\n    title_divs = note_soup.find_all(\"div\", {\"class\": \"last col\"})\n    title_text = title_divs[0].find_all('h1')[0].text\n    book_title.append(title_text)\n    \n    #Extract rating particulars\n    rating_divs = note_soup.find_all(\"div\", {\"class\": \"uitext stacked\", \"id\": \"bookMeta\"})\n    rating_text = rating_divs[0].find_all(\"span\", {\"itemprop\": \"ratingValue\"})[0].text\n    book_rating.append(rating_text)\n    \n    #Extracting page particulars\n    page_divs = note_soup.find_all(\"div\", {\"class\": \"row\"})\n    try:\n        page_text = page_divs[0].find_all(\"span\", {\"itemprop\": \"numberOfPages\"})[0].text.strip('pages')\n    except IndexError:\n        page_text = 0\n    book_pages.append(page_text)\n    \n    #Extracting description particulars\n    description_divs = note_soup.find_all(\"div\", {\"class\": \"readable stacked\", \"id\": \"description\"})\n    try:\n        description_text = description_divs[0].find_all(\"span\")[1].text\n    except IndexError:\n        try:\n            description_text = description_divs[0].find_all(\"span\")[0].text\n        except IndexError:\n            description_text = \"Nil\"\n    book_description.append(description_text)\n        \n    #Incremeting the loop counter\n    i = i+1\nIt will take a couple of minutes to scrape through all 50 links. Most of the raw data look messy, and hence require some cleaning up. After some tidying, we can use the pandas package to organise all the data into a data frame (see below).\n\nYou may also sort the data frame based on its ratings using the sort_values method. That will inform us that the highest rated book is “The Rise of the Ultra Runners: A Journey to the Edge of Human Endurance” by Adharanand Finn with an average 4.45 rating. Finally, we can export all these data into a nice csv file for ease of viewing on Excel, using the to_csv method.\n# Export dataframe into csv file\nsorted_book_df.to_csv(\"top running books.csv\")\nHope you enjoy this blog post and full code can be found here. Similar codes can be used to scrape other book lists on Goodreads. For my running friends, you may check out the final csv file over here."
  },
  {
    "objectID": "posts/Pace-variation-in-marathon-running/index.html",
    "href": "posts/Pace-variation-in-marathon-running/index.html",
    "title": "Pace variation in Marathon running: Analysing data from Standard Chartered Singapore Marathon 2019",
    "section": "",
    "text": "Pacing strategy is an important determinant of performance, especially in endurance events such as the marathon. Does pacing profile differ across runners of different levels? This blog post examines the pace variation of Singaporean male and female runners in the Standard Chartered Singapore Marathon (SCSM) 2019."
  },
  {
    "objectID": "posts/Pace-variation-in-marathon-running/index.html#art-of-pacing",
    "href": "posts/Pace-variation-in-marathon-running/index.html#art-of-pacing",
    "title": "Pace variation in Marathon running: Analysing data from Standard Chartered Singapore Marathon 2019",
    "section": "Art of Pacing",
    "text": "Art of Pacing\nPacing strategy in long-distance running refers to the distribution of running speed across the whole duration of event. Finding an optimised pacing strategy in a marathon event is no easy feat, as the distance often poses many uncertainties and unexpected challenges. Too conservative start may lead to a sub-optimal finish, while an aggressive start may result in early fatigue and excessive slowing down during latter stages of the race.\nGenerally, there are three types of pacing profiles: 1) negative, 2) positive, and 3) even pacing strategy. First, negative pacing refers to an increase in speed over the duration of event. This will mean running the second half of the race faster than the first half. Second, positive pacing is characterised by a decline in speed instead. Third, even pacing represents keeping relatively the same speed throughout the event. Such even pacing was observed in Eliud Kipchoge’s shattering of the 2 hour marathon barrier last year. If you look at his time splits, he and his pacing team ran each 5km within a range of 14:10 - 14:14. This suggests that adopting an even pacing profile in the marathon may be the most ideal strategy to hit your race goals.\nRealistically, this is a tall order and gets increasingly harder to achieve as the race distance increases. Very often, we see drastic changes in running speed during endurance events. For example, we see more race participants walking in the second half of the marathon as compared to the first half. This is an expected phenomenon since fatigue kicks in as the race progresses. However, is such variation in pace similar across all levels of runners?\nWith the aim to answer this question, I scraped the time splits of all Singaporean runners who finished the full marathon in last year’s SCSM. The web scraping was performed using the pandas and BeautifulSoup package in python. You may find the full web scraping code here.\n\n\n\n\n\nBased on the official results from the race website, a total of 3929 Singaporean men and 911 women completed the full marathon. Clearly, the marathon is not as popular among females than their male counterparts. Data on each 5km time split was retrieved for each runner. However, several runners were found with missing data for certain distances. Hence, for runners with missing time splits, it was assumed that their pace remained the same as the last recorded time split. Runners with missing data for the first 5km and 10km were considered invalid and removed from the dataset. Thus, I ended up with a total of 3886 men and 902 women for the analysis.\nThe coefficient of variation (CV) was used to measure the variation in each runner’s pace throughout the race. This was simply computed by dividing the standard deviation of all time splits by the mean speed. Higher CVs represent greater variation in pace and lower CVs indicate more consistency in running speed throughout the race. The CV was compared among three groups of runners in both gender groups: fast, mid-pack and slow. The fast group represents the top 20%, the mid-pack represents the middle 20%, and the slow group represents the bottom 20%. The notched box plots above illustrate the average running speed distribution of each group separated by genders. Evidently, the speeds differed significantly across groups for both genders.\n\n\n\n\n\nThe graph above presents the average CV scores in percentages of respective groups for both genders. We can see a trend that pace variation increases with slower marathon running times. Using a statistical test (one-way ANOVA) informed us that the differences in pace variation across groups were significant. The top 20% Singaporean male runners had an average of 12.11% in their marathon pace variation, and this was significantly less than the mid-pack (14.93%) and the slow group (15.59%). However, the mid-pack and slow groups showed comparably similar variation in pace. Similarly, for the females, the fast group (8.14%) showed significantly less variation in their pace than their slower counterparts. The mid-pack group (12.8%) also differed significantly as compared to the slow group (15.32%)."
  },
  {
    "objectID": "posts/Pace-variation-in-marathon-running/index.html#importance-of-consistency-in-pacing",
    "href": "posts/Pace-variation-in-marathon-running/index.html#importance-of-consistency-in-pacing",
    "title": "Pace variation in Marathon running: Analysing data from Standard Chartered Singapore Marathon 2019",
    "section": "Importance of consistency in pacing",
    "text": "Importance of consistency in pacing\nThese numbers highlighted that better performing runners were not only faster in their speed, they also exhibited considerably less variation in their marathon running pace. Obviously, many factors can influence a marathon performance. One aspect is definitely optimising your pacing strategy during the race. This means selecting a pace that you can sustain consistently with minimal variation throughout the race. This is an important takeaway for all levels of runners. Hence, do practise your pacing strategy during training, and carefully plan your pace for race day. It will probably make a difference!\nData processing, analysis and visualisation were performed on R. Full code and datasets can be found here."
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "",
    "text": "Contact tracing plays a key role in infectious disease management, and related cases form a cluster. Since the coronavirus outbreak, several clusters have formed all over Singapore. This blog post documents my attempt to visualise the clusters on the Singapore map using R programming language."
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#required-tools",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#required-tools",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "Required Tools",
    "text": "Required Tools\nlibrary(rvest)\nlibrary(ggplot2)\nlibrary(ggmap)\nThree libraries are required for this project. rvest does the web scraping and ggplot2 is required for graph plotting. More importantly, ggmap helps us to conveniently locate the coordinates of each cluster address with the geocode function."
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#google-maps-api",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#google-maps-api",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "Google Maps API",
    "text": "Google Maps API\nregister_google(key = \"insert your API key here\")\nFirst, we need to register an authorised Google API key in order to use the geocode function. You may register for your free API key over here."
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#cluster-list",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#cluster-list",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "Cluster List",
    "text": "Cluster List\nThe list of clusters used in this blog post was from the Covid-19 Singapore Dashboard created by Upcode Academy. The dashboard displays several visualisations on data related to the pandemic outbreak in Singapore. On the bottom right of the page, we could see the clusters and their respective number of cases. Data on the cluster list was scraped through the use of rvest package. The web scraping process is similar to the code I have documented in my previous blogpost.\n\n\n\nOrganising the clusters into a data frame\n\n\nA total of 47 clusters was scraped from the dashboard and organised into a dataframe. The figure above presents the name of each cluster and its respective number of cases. The list includes clusters that were identified early on during the outbreak (e.g. Yong Thai Hang medical shop, Grand Hyatt Hotel) as well as the foreign worker dormitories, where numbers have skyrocketed during the last 2 months (e.g. S11 Dormitory). Unfortunately, the numbers for certain clusters are not updated (especially for the dormitories). Nevertheless, it contains majority of the local clusters identified so far.\nIn order to plot each cluster on the map, we require the coordinates of each cluster address. Google identifies each address by its latitude and longitude position, which we can conveniently retrieve via the geocode function. Before doing this, we had to tidy up some of these cluster names in order to better locate them. For example, specifying “Grand Hyatt Hotel Singapore” instead of “Grand Hyatt Hotel” returns the local coordinates instead of a foreign location. Editing of all cluster names can be found in the full code.\nA for-loop was then used to extract the coordinate positions for each cluster and appended to the data frame.\n# Computing latitude and longitude of each location\nfor (i in 1:nrow(cluster_df)) {\n  cluster_name = cluster_df[i, c(\"clusters\")]\n  cluster_geocode = geocode(cluster_name)\n  cluster_df[i, c(\"lon\")] <- cluster_geocode$lon\n  cluster_df[i, c(\"lat\")] <- cluster_geocode$lat\n}"
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#singapore-map",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#singapore-map",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "Singapore Map",
    "text": "Singapore Map\n# Locating the latitude and longitude position of Singapore\n# Lon = 103.8198, Lat = 1.352083\nsingapore_city <- geocode(\"Singapore\")\n\n# Retrieves the map image of the location\nsingapore_map <- get_map(singapore_city, zoom = 12,  maptype = \"roadmap\")\n\n# Visualising the map \nggmap(singapore_map)\nThe map visualisation encompasses plotting each cluster on the map. Hence, we need to prepare the background map first. The coordinates of the map were retrieved through the geocode function. Next, the get_map function helps to get the image of the map. We could specify the amount of zoom as well as the map type. There are several map types, and “roadmap” represents the typical google map we see. Lastly, ggmap function helps us to visualise the map (see below).\n\n\n\nVisualisation of Singapore map using ggmap"
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#visualising-the-clusters",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#visualising-the-clusters",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "Visualising the clusters",
    "text": "Visualising the clusters\n# Map the clusters on singapore map\nggmap(singapore_map, \n      base_layer = ggplot(data = cluster_df,\n                          aes(x=lon, y = lat, color = cases))) +\n  geom_point(size = 1.5,\n             alpha = 0.8)+\n  labs(title = \"Covid-19 Clusters in Singapore\",\n       caption = \"Data from Covid-19 Singapore Dashboard\",\n       color = \"Number of cases\")+\n  scale_colour_gradient(low = \"blue\", high = \"red\", na.value = NA)+\n  theme_void()\nNext, visualisation of the clusters was done by simply adding the scatterplot layer to the map layer. Specifying the color to be dependent on the number of cases helps to differentiate the magnitude of the clusters. You may set the gradient colours using scale_colour_gradient. In this case, most clusters are relatively small (< 50) except the foreign worker dormitories. Unfortunately, as mentioned earlier, the numbers for the dormitories are not updated. You may see the final plot below. Do take note that certain clusters are missing from the plot due to the zoom size of the map selected.\n\n\n\nMap visualisation of Covid-19 clusters in Singapore using ggmap.\n\n\nFull code for this article can be accessed through my github."
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#expected-goals",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#expected-goals",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "Expected Goals",
    "text": "Expected Goals\nEvaluation of a team’s underlying performance at goal is made possible with the expected goals (xG) metric. The metric defines the probability of a shot being scored based on multiple factors such as angle, distance and type of assist. This is well-explained with various examples in the video link here by Opta, a football data analytics company. Such measure provides us with further insights beyond the match statistics that are typically shown.\nFootball history has demonstrated that goals could be scored by players of different positions, including the goalkeeper. Some came from clear goal-scoring chances, while some were scored from very unexpected situations. For example, Asmir Begovic, goalkeeper of Stoke City, scored from his own penalty box in 2013. The xG metric would have informed us that the attempt had very low odds of hitting the back of the net. It is reasonable to consider that such abnormal and unexpected goals were just teams being lucky.\nTheoretically, the team with better goal-scoring opportunities should register more goals. However, this is not the case in reality due to various factors and interaction effects in a football game. Therefore, comparison between expected and actual goals could help us differentiate chance occurrences and deserving performances. Scoring much more actual than expected goals would suggest a lucky outing for the team.\nxG is not commonly reported because such information is not readily available online, as the probabilities are computed based on trained models using historical data. Some sites charge fees for these data. Fortunately, Understat.com provides free access to xG statistics based on their algorithms. Data was scraped using BeautifulSoup package in python to examine whether Liverpool’s impressive season so far (31 fixtures) was a fluke or genuine prowess. You may find the web scraping code template on my github here.\n The area plot above illustrates the comparison between actual and expected goals for Liverpool this season so far. The first important point is observing how xG approximates actual goals closely most of the time, which highlights the predictive value of the metric. Another interpretation of the plot is that Liverpool was expected to score at least one goal in most matches (29 to be precise). This is not surprising given that they are second in the league’s goal rankings. This suggests that Liverpool was quite consistent in producing goal-scoring opportunities.\nThe visible grey areas indicate that Liverpool registered more actual goals than expected in some occasions. While the disparity between the two areas is mostly quite small, such differences suggest that Liverpool had luck on their side on top of their performances. The most notable gap was on matchday 15, whereby Liverpool faced Everton in the Merseyside derby. Liverpool scored 5 goals, twice the xG statistic of 2.41. Aggregation of xG revealed that Liverpool was expected to score 64.25 goals, which is less than the 70 goals they actually scored."
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#section",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#section",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "",
    "text": "Similarly, we could use the opposition’s xG metric to assess how lucky Liverpool was defensively. The figure above illustrates the comparison between actual and expected goals against. The visible blue areas indicate that Liverpool conceded less actual goals than expected, implying good fortune in these occasions. However, the visible red areas suggest that Liverpool was also unlucky during some fixtures as well. Aggregation of expected goals against revealed that Liverpool was expected to concede a total of 29.38 goals over 31 matches so far. In reality, Liverpool had the best defensive record in the league with a total of 21 conceded. Once again, the difference between the two measures suggests that luck was in Liverpool’s favour as well from the defensive perspective.  Expected Goal Difference\nSubtracting the expected goals against from xG would compute the expected goal difference for a given fixture. The visualisation above presents the actual versus expected goal difference (xDiff) for all the fixtures. A positive xDiff indicates that the team should win the game, while negative xDiff indicates an expected loss, and zero implies an expected draw. This can inform us whether the team was deserving of its result.\nAccording to the figure above, we could clearly see that Liverpool had a deserving loss on matchday 28. Liverpool lost convincingly to Watford by three goals. In that fixture, Liverpool registered their lowest xG of 0.21 and the highest expected goal against of 2.71. These statistics clearly showed that Liverpool had a very poor performance.\nWe could also use threshold values of +0.5 and -0.5 to determine whether Liverpool was anticipated to win and lose the game respectively based on their performance. The data revealed that 24 out of 31 games had an xDiff greater than 0.5, and only 2 out of 31 games had an xDiff less than -0.5. Such statistics strongly support Liverpool’s consistency in their performance throughout the season. In reality, Liverpool won a total of 28 games and lost 1 single game."
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#conclusion",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#conclusion",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, xG is a valuable metric to assess the underlying performance of a team beyond the results. As the saying goes, better to be lucky than good. Indeed, the data revealed that Liverpool was lucky both offensively and defensively during certain occasions, albeit usually by small margins. However, saying that Liverpool’s success was a lucky win based on these results will be a complete misinterpretation. The xDiff data clearly shows that Liverpool exhibited strong performance consistently throughout the season and deserved their results. Therefore, a fair conclusion will be that Liverpool’s title winning success was largely attributed to their consistent strong performance, but they also had some luck on their side as well."
  },
  {
    "objectID": "posts/Liverpool-success-lucky-or-worthy-winners/index.html",
    "href": "posts/Liverpool-success-lucky-or-worthy-winners/index.html",
    "title": "Liverpool’s success: Lucky or worthy winners?",
    "section": "",
    "text": "Liverpool was announced the champions of English Premier League (EPL) season 19/20 with a record of seven games remaining. 23 points separate them from second-placed Manchester City. This post examines whether such overwhelming dominance over their rivals is attributed to sheer luck or genuine strength of the team through visualisation of expected goal data.\nFootball results do not necessarily reflect the performance of respective teams accurately. It is not always the case that the winning team has higher number of goal-scoring opportunities or greater possession than their opposition. Sometimes, teams walk away with a result merely due to good fortune. How do we differentiate between luck and performance?"
  },
  {
    "objectID": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#expected-goals",
    "href": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#expected-goals",
    "title": "Liverpool’s success: Lucky or worthy winners?",
    "section": "Expected Goals",
    "text": "Expected Goals\nEvaluation of a team’s underlying performance at goal is made possible with the expected goals (xG) metric. The metric defines the probability of a shot being scored based on multiple factors such as angle, distance and type of assist. This is well-explained with various examples in the video link here by Opta, a football data analytics company. Such measure provides us with further insights beyond the match statistics that are typically shown.\nFootball history has demonstrated that goals could be scored by players of different positions, including the goalkeeper. Some came from clear goal-scoring chances, while some were scored from very unexpected situations. For example, Asmir Begovic, goalkeeper of Stoke City, scored from his own penalty box in 2013. The xG metric would have informed us that the attempt had very low odds of hitting the back of the net. It is reasonable to consider that such abnormal and unexpected goals were just teams being lucky.\nTheoretically, the team with better goal-scoring opportunities should register more goals. However, this is not the case in reality due to various factors and interaction effects in a football game. Therefore, comparison between expected and actual goals could help us differentiate chance occurrences and deserving performances. Scoring much more actual than expected goals would suggest a lucky outing for the team.\nxG is not commonly reported because such information is not readily available online, as the probabilities are computed based on trained models using historical data. Some sites charge fees for these data. Fortunately, Understat.com provides free access to xG statistics based on their algorithms. Data was scraped using BeautifulSoup package in python to examine whether Liverpool’s impressive season so far (31 fixtures) was a fluke or genuine prowess. You may find the web scraping code template on my github here.\n The area plot above illustrates the comparison between actual and expected goals for Liverpool this season so far. The first important point is observing how xG approximates actual goals closely most of the time, which highlights the predictive value of the metric. Another interpretation of the plot is that Liverpool was expected to score at least one goal in most matches (29 to be precise). This is not surprising given that they are second in the league’s goal rankings. This suggests that Liverpool was quite consistent in producing goal-scoring opportunities.\nThe visible grey areas indicate that Liverpool registered more actual goals than expected in some occasions. While the disparity between the two areas is mostly quite small, such differences suggest that Liverpool had luck on their side on top of their performances. The most notable gap was on matchday 15, whereby Liverpool faced Everton in the Merseyside derby. Liverpool scored 5 goals, twice the xG statistic of 2.41. Aggregation of xG revealed that Liverpool was expected to score 64.25 goals, which is less than the 70 goals they actually scored.\n\nSimilarly, we could use the opposition’s xG metric to assess how lucky Liverpool was defensively. The figure above illustrates the comparison between actual and expected goals against. The visible blue areas indicate that Liverpool conceded less actual goals than expected, implying good fortune in these occasions. However, the visible red areas suggest that Liverpool was also unlucky during some fixtures as well. Aggregation of expected goals against revealed that Liverpool was expected to concede a total of 29.38 goals over 31 matches so far. In reality, Liverpool had the best defensive record in the league with a total of 21 conceded. Once again, the difference between the two measures suggests that luck was in Liverpool’s favour as well from the defensive perspective."
  },
  {
    "objectID": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#section",
    "href": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#section",
    "title": "Liverpool’s success: Lucky or worthy winners?",
    "section": "",
    "text": "Similarly, we could use the opposition’s xG metric to assess how lucky Liverpool was defensively. The figure above illustrates the comparison between actual and expected goals against. The visible blue areas indicate that Liverpool conceded less actual goals than expected, implying good fortune in these occasions. However, the visible red areas suggest that Liverpool was also unlucky during some fixtures as well. Aggregation of expected goals against revealed that Liverpool was expected to concede a total of 29.38 goals over 31 matches so far. In reality, Liverpool had the best defensive record in the league with a total of 21 conceded. Once again, the difference between the two measures suggests that luck was in Liverpool’s favour as well from the defensive perspective.  Expected Goal Difference\nSubtracting the expected goals against from xG would compute the expected goal difference for a given fixture. The visualisation above presents the actual versus expected goal difference (xDiff) for all the fixtures. A positive xDiff indicates that the team should win the game, while negative xDiff indicates an expected loss, and zero implies an expected draw. This can inform us whether the team was deserving of its result.\nAccording to the figure above, we could clearly see that Liverpool had a deserving loss on matchday 28. Liverpool lost convincingly to Watford by three goals. In that fixture, Liverpool registered their lowest xG of 0.21 and the highest expected goal against of 2.71. These statistics clearly showed that Liverpool had a very poor performance.\nWe could also use threshold values of +0.5 and -0.5 to determine whether Liverpool was anticipated to win and lose the game respectively based on their performance. The data revealed that 24 out of 31 games had an xDiff greater than 0.5, and only 2 out of 31 games had an xDiff less than -0.5. Such statistics strongly support Liverpool’s consistency in their performance throughout the season. In reality, Liverpool won a total of 28 games and lost 1 single game."
  },
  {
    "objectID": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#conclusion",
    "href": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#conclusion",
    "title": "Liverpool’s success: Lucky or worthy winners?",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, xG is a valuable metric to assess the underlying performance of a team beyond the results. As the saying goes, better to be lucky than good. Indeed, the data revealed that Liverpool was lucky both offensively and defensively during certain occasions, albeit usually by small margins. However, saying that Liverpool’s success was a lucky win based on these results will be a complete misinterpretation. The xDiff data clearly shows that Liverpool exhibited strong performance consistently throughout the season and deserved their results. Therefore, a fair conclusion will be that Liverpool’s title winning success was largely attributed to their consistent strong performance, but they also had some luck on their side as well."
  },
  {
    "objectID": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#similarly-we-could-use-the-oppositions-xg-metric-to-assess-how-lucky-liverpool-was-defensively.-the-figure-above-illustrates-the-comparison-between-actual-and-expected-goals-against.-the-visible-blue-areas-indicate-that-liverpool-conceded-less-actual-goals-than-expected-implying-good-fortune-in-these-occasions.-however-the-visible-red-areas-suggest-that-liverpool-was-also-unlucky-during-some-fixtures-as-well.-aggregation-of-expected-goals-against-revealed-that-liverpool-was-expected-to-concede-a-total-of-29.38-goals-over-31-matches-so-far.-in-reality-liverpool-had-the-best-defensive-record-in-the-league-with-a-total-of-21-conceded.-once-again-the-difference-between-the-two-measures-suggests-that-luck-was-in-liverpools-favour-as-well-from-the-defensive-perspective.-expected-goal-difference",
    "href": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#similarly-we-could-use-the-oppositions-xg-metric-to-assess-how-lucky-liverpool-was-defensively.-the-figure-above-illustrates-the-comparison-between-actual-and-expected-goals-against.-the-visible-blue-areas-indicate-that-liverpool-conceded-less-actual-goals-than-expected-implying-good-fortune-in-these-occasions.-however-the-visible-red-areas-suggest-that-liverpool-was-also-unlucky-during-some-fixtures-as-well.-aggregation-of-expected-goals-against-revealed-that-liverpool-was-expected-to-concede-a-total-of-29.38-goals-over-31-matches-so-far.-in-reality-liverpool-had-the-best-defensive-record-in-the-league-with-a-total-of-21-conceded.-once-again-the-difference-between-the-two-measures-suggests-that-luck-was-in-liverpools-favour-as-well-from-the-defensive-perspective.-expected-goal-difference",
    "title": "Liverpool’s success: Lucky or worthy winners?",
    "section": "Similarly, we could use the opposition’s xG metric to assess how lucky Liverpool was defensively. The figure above illustrates the comparison between actual and expected goals against. The visible blue areas indicate that Liverpool conceded less actual goals than expected, implying good fortune in these occasions. However, the visible red areas suggest that Liverpool was also unlucky during some fixtures as well. Aggregation of expected goals against revealed that Liverpool was expected to concede a total of 29.38 goals over 31 matches so far. In reality, Liverpool had the best defensive record in the league with a total of 21 conceded. Once again, the difference between the two measures suggests that luck was in Liverpool’s favour as well from the defensive perspective.  Expected Goal Difference",
    "text": "Similarly, we could use the opposition’s xG metric to assess how lucky Liverpool was defensively. The figure above illustrates the comparison between actual and expected goals against. The visible blue areas indicate that Liverpool conceded less actual goals than expected, implying good fortune in these occasions. However, the visible red areas suggest that Liverpool was also unlucky during some fixtures as well. Aggregation of expected goals against revealed that Liverpool was expected to concede a total of 29.38 goals over 31 matches so far. In reality, Liverpool had the best defensive record in the league with a total of 21 conceded. Once again, the difference between the two measures suggests that luck was in Liverpool’s favour as well from the defensive perspective.  Expected Goal Difference\nSubtracting the expected goals against from xG would compute the expected goal difference for a given fixture. The visualisation above presents the actual versus expected goal difference (xDiff) for all the fixtures. A positive xDiff indicates that the team should win the game, while negative xDiff indicates an expected loss, and zero implies an expected draw. This can inform us whether the team was deserving of its result.\nAccording to the figure above, we could clearly see that Liverpool had a deserving loss on matchday 28. Liverpool lost convincingly to Watford by three goals. In that fixture, Liverpool registered their lowest xG of 0.21 and the highest expected goal against of 2.71. These statistics clearly showed that Liverpool had a very poor performance.\nWe could also use threshold values of +0.5 and -0.5 to determine whether Liverpool was anticipated to win and lose the game respectively based on their performance. The data revealed that 24 out of 31 games had an xDiff greater than 0.5, and only 2 out of 31 games had an xDiff less than -0.5. Such statistics strongly support Liverpool’s consistency in their performance throughout the season. In reality, Liverpool won a total of 28 games and lost 1 single game."
  },
  {
    "objectID": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#expected-goal-difference",
    "href": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#expected-goal-difference",
    "title": "Liverpool’s success: Lucky or worthy winners?",
    "section": "Expected Goal Difference",
    "text": "Expected Goal Difference\nSubtracting the expected goals against from xG would compute the expected goal difference for a given fixture. The visualisation above presents the actual versus expected goal difference (xDiff) for all the fixtures. A positive xDiff indicates that the team should win the game, while negative xDiff indicates an expected loss, and zero implies an expected draw. This can inform us whether the team was deserving of its result.\nAccording to the figure above, we could clearly see that Liverpool had a deserving loss on matchday 28. Liverpool lost convincingly to Watford by three goals. In that fixture, Liverpool registered their lowest xG of 0.21 and the highest expected goal against of 2.71. These statistics clearly showed that Liverpool had a very poor performance.\nWe could also use threshold values of +0.5 and -0.5 to determine whether Liverpool was anticipated to win and lose the game respectively based on their performance. The data revealed that 24 out of 31 games had an xDiff greater than 0.5, and only 2 out of 31 games had an xDiff less than -0.5. Such statistics strongly support Liverpool’s consistency in their performance throughout the season. In reality, Liverpool won a total of 28 games and lost 1 single game."
  },
  {
    "objectID": "posts/End-spurt-in-marathon/index.html",
    "href": "posts/End-spurt-in-marathon/index.html",
    "title": "The end spurt in marathon running: A universal behaviour?",
    "section": "",
    "text": "Pacing in endurance events is often characterised by an increase in power output or running speed towards the end, a behaviour termed the end spurt. This blog post examines the prevalence of such occurrence in Singaporean male and female runners during the Standard Chartered Singapore Marathon (SCSM) 2019."
  },
  {
    "objectID": "posts/End-spurt-in-marathon/index.html#the-end-spurt",
    "href": "posts/End-spurt-in-marathon/index.html#the-end-spurt",
    "title": "The end spurt in marathon running: A universal behaviour?",
    "section": "The End Spurt",
    "text": "The End Spurt\nIn the 2019 Boston Marathon, the fight for the first position in the men’s event came down to an epic sprint battle between Lawrence Cherono and Lelisa Desisa. Eventually, the winner and runner-up of an arduous 42.195 km race were separated by merely two seconds. The behaviour of increasing power output or running speed towards the end of an exercise bout is known as an end spurt. Such pacing behaviour is commonly observed in endurance events among both elite and recreational runners.\nThis observation has been of interest to sport scientists as it challenges the notion of fatigue. In an endurance event such as the marathon, it is expected that runners usually slow down as the distance progresses due to increase in fatigue levels. Thus, it is paradoxical that runners are able to speed up near the finish line when they are supposed to be the most fatigued. An explanation of such paradox is that individuals always exercise with reserves and regulate exercise in an anticipatory manner. As suggested in the central governor theory by renowned sport scientist, professor Timothy Noakes, anticipation is a critical component of exercise regulatory behaviour to avoid any catastrophic event happening during exercise.\nIs such pacing behaviour exhibited by all runners?\nMy previous blog post has examined the pace variation of Singaporean male and female runners of different levels who completed SCSM 2019. Using the same scraped data (see web scraping code here), this blog post examined the prevalence of the end spurt in both gender groups, and comparison was made among three groups of runners: fast, mid-pack and slow. The fast group represents the upper 20th percentile, the mid-pack consists of runners between the 40th and 60th percentile, and the slow group is made up of runners from the lower 20th percentile.\n\n\n\n\n\nFirst, let’s visualise the pacing profiles of each group of runners over the marathon distance, separated by gender. As illustrated in the figure above, the pacing profiles look quite similar across different groups. The observed pattern is characterised by an expected decrease in speed progressively over the course of the race. Interestingly, that is followed by an increase in speed right at the end of the race (last 2.195 km), demonstrating the end spurt behaviour. This seems to suggest that such behaviour can be observed in most runners regardless of their performance levels.\n\n\n\n\n\nTo examine the prevalence of the end spurt, change in average speed between the last section and the average time split at 40 km was first computed. A positive change in speed means that runners ran faster during the last section as compared to their 40 km time splits, and a negative change in speed implies slowing down instead. As shown in the box plots above, the lower quartile is more than zero, indicating a positive change in speed was found in more than 75% of runners in both gender groups. Specifically, 88% of female runners and 86% of male runners increased their speed during the last 2.195 km of the marathon. This demonstrates that the end spurt behaviour was observed in majority of runners.\n\n\n\n\n\nWhile the pacing profiles above suggest that the end spurt behaviour was observed in all levels of runners, the degree of end spurt may differ between the groups. The figure above presents the change in speed as a percentage of the runners’ 40 km time split. Statistical analysis (one-way ANOVA and post-hoc pairwise comparisons) revealed that only the slow group differed from the other two groups for both genders. For the male runners, the increase in speed among the slower runners (8.05%) was significantly less than the mid-pack (12.88%) and the faster runners (13.05%). Similarly, for the female runners, the slow group (8.55%) sped up significantly less than the mid-pack (12.82%) and the fast group (11.44%). This suggests that better performing runners tend to speed up more than their slower peers during the last section of a marathon."
  },
  {
    "objectID": "posts/End-spurt-in-marathon/index.html#anticipation-and-uncertainty",
    "href": "posts/End-spurt-in-marathon/index.html#anticipation-and-uncertainty",
    "title": "The end spurt in marathon running: A universal behaviour?",
    "section": "Anticipation and Uncertainty",
    "text": "Anticipation and Uncertainty\nThese findings demonstrated that the end spurt behaviour in a marathon was highly prevalent among runners. Given that such behaviour was exhibited regardless of gender and performance levels, it has certainly interest me to further think about this phenomenon.\n\nIs the end spurt an innate manner of regulating endurance exercise?\n\nAs mentioned earlier, a plausible explanation behind such phenomenon is that humans always exercise in an anticipatory manner. In most exercise settings, we are aware of the end point, be it expected distance or duration. Naturally, we want to reach this end point optimally and also safely as well. In the context of a marathon race, the performance goal is to reach the finish line as fast as possible. To achieve this, an athlete not only has to run at a fast pace, but also a sustainable one as well. In reality, this is a very challenging task because sustainability of pace is usually uncertain in an endurance event. This is supported by the variation in marathon running pace shown in my previous blog post. However, this uncertainty diminishes with increasing proximity to the end point. For example, an athlete at the 10 km mark will be less confident of sustaining a given pace as compared to the athlete at 40 km instead. Rationally, an athlete will not select a pace that is knowingly unsustainable as that will be detrimental to performance. Therefore, it makes sense that individuals are only inclined to increase their pace when they know they are near to the finish line.\nComparison between different groups of runners showed us that better performing runners exhibited greater end spurts as compared to the slowest group. This either implies that 1) better runners were more conservative in their pacing and hence has greater reserve capacities to tap on for the final push, or 2) better runners were able to dig deeper than their slower counterparts and thus greater ability to speed up.\nIn summary, the end spurt is ubiquitous in an endurance event such as the marathon. It remains unclear why most individuals exhibit such behaviour. The next time you find yourself speeding up near the end of an exercise bout, I hope you can share with me your rationale behind it!\nData processing, analysis and visualisation were performed on R. Full code and datasets can be found here."
  },
  {
    "objectID": "posts/Radar-charts-football-visualisation/index.html",
    "href": "posts/Radar-charts-football-visualisation/index.html",
    "title": "Radar charts in R: Visualising playing styles of EPL football teams",
    "section": "",
    "text": "Playing styles typically differed among football teams. A team’s playing style is often based on a myriad of factors including manager’s football philosophy, tactical strategies, players’ attributes etc. This blog post visualises the playing styles of English Premier League (EPL) football teams with the use of radar charts."
  },
  {
    "objectID": "posts/Radar-charts-football-visualisation/index.html#characterising-playing-styles",
    "href": "posts/Radar-charts-football-visualisation/index.html#characterising-playing-styles",
    "title": "Radar charts in R: Visualising playing styles of EPL football teams",
    "section": "Characterising Playing Styles",
    "text": "Characterising Playing Styles\nWhile it is common to hear that certain teams are either predominantly attacking or defensively minded, such descriptions often sound quite vague. Thanks to Understat, we can use data to try characterising playing styles of football teams in a more objective manner. The understatr package provides match statistics on several major European football leagues from previous seasons, saving us the trouble from performing web scrapping ourselves. In this blog post, three key statistics were used to characterise the match plays of football teams in EPL season 19/20: passes allowed per defensive action (PPDA), deep, and expected goals (xG).\nFirst, PPDA is a measure of a team’s high press intensity and it is calculated using the number of passes made by the attacking team divided by the number of defensive actions (e.g. tackle, interception, challenge, foul). A high pressing team will be expected to have lower PPDA values since pressing strategies will result in greater number of defensive actions and diminish the likelihood of the opposition team stringing many passes successfully. Hence, low PPDA values indicate high intensity of pressing and vice versa.\nSecond, deep refers to the number of passes completed within an estimated 20 yards of goal, excluding crosses. This gives us insights on a team’s behaviour in the final third of the opposition team.\nLast, xG provides information on the goal-scoring opportunities of a team. It represents the accumulative probability of shots being scored in a match. I have used this metric to examine Liverpool’s title-winning success in my previous blog post. In this blog post, the non-penalty expected goals (npxG) metric was used instead to specifically account for goal-scoring opportunities exclusive of penalties, which tend to have high xG values."
  },
  {
    "objectID": "posts/Radar-charts-football-visualisation/index.html#radar-chart-visualisation",
    "href": "posts/Radar-charts-football-visualisation/index.html#radar-chart-visualisation",
    "title": "Radar charts in R: Visualising playing styles of EPL football teams",
    "section": "Radar Chart Visualisation",
    "text": "Radar Chart Visualisation\nRadar charts are useful to visualise multivariate data in a two-dimensional manner. This is also a popular way in displaying football data. Statsbomb, a football analytics company, commonly uses such visualisation to show and compare individual players’ statistics. While there are different packages capable of creating radar charts, this blog post utilised the ggradar package to visualise the three aforementioned statistics.\n\n\n\n\n\nThe figure above is an example of a radar chart, which displays the three statistics for two different teams. All three variables were first standardised and rescaled to a range between 0 and 1 with reference to all the teams in the league. This is to ensure that different variables of differing scales could be mapped onto the same “axes”. As seen in the figure, the chart has three circular gridlines of different sizes and colours. These gridlines represent the common axes shared by all variables and correspond to the rescaled range in percentages. The most inner gridline, in yellow, corresponds to 0%. The middle gridline (in red) corresponds to 50% and the most outer gridline (in black) corresponds to 100%. Low scores on any of the variables are close to the yellow gridline and high scores are more proximal to the black gridline instead.\nRadar charts are useful in helping us to make comparisons quickly. The radar chart above compares the match statistics between Liverpool FC and Newcastle United. The playing style of each team is defined in a triangular shape since we have three variables. With a glance of the chart, the contrast between the two triangles suggests that the playing styles differed between the two teams.\nThe radar chart shows that Liverpool scored above 50% for both npxG and deep statistics, and 0% for PPDA. Conversely, Newcastle scored very low on npxG and deep, but attained 100% for PPDA. Despite using only three variables, these statistics reveal insights on the differing styles between teams. Liverpool is a team that created relatively high number of goal scoring opportunities, and also made several passes in the opposition team’s final third. The 0% for PPDA indicates that they possessed the greatest intensity of high-pressing in the league. On the other hand, Newcastle is on the other end of the spectrum. 100% for PPDA suggests that they pressed relatively less than all the other teams in the league. In addition, they also made the least number of passes near the opposition’s goal. Unsurprisingly, their goal scoring opportunities were on the lower end (5.8%).\n\n\n\n\n\nNext, let’s compare between Liverpool and Manchester City. Contrary to the previous comparison, this radar chart clearly informs us that these two teams are very alike, as evidenced by the two almost similar triangles. Regular audience of the EPL would know that both teams like to be dominant in possession and play exciting attacking football. While they are similar, the runner-ups were superior in creating goal-scoring opportunities and passes in the opponent’s final third as compared to the champions. In fact, they were the best in both statistics across the entire league. On the other hand, Liverpool edged out their rivals in terms of high press intensity.\n\n\n\n\n\nFinally, the last figure presents the radar charts for every single team in the EPL, based on season 19/20 data. The teams are arranged in the order as the final table rankings. Interestingly, worst performing teams in terms of npxG (Crystal Palace), deep and PPDA (Newcastle) were not at the bottom of the league table. This suggests that these three indices are probably not the best predictors of overall football performance. Obviously, it would be somewhat too simplistic to characterise a team’s playing style merely on these three statistics. Nevertheless, they do provide us with some insights on different teams’ playing styles, and radar chart is a good approach to visualise these metrics and make comparisons with ease.\nYou may access the full code for all data processing and visualisation at my github."
  },
  {
    "objectID": "posts/All-about-that-bayes/index.html",
    "href": "posts/All-about-that-bayes/index.html",
    "title": "All about that Bayes: Why individuals appraise similar evidence differently?",
    "section": "",
    "text": "The world is intrinsically ambiguous, and we usually must make decisions and judgements in our daily lives in the face of uncertainty. Some situations are trivial (e.g., should I bring an umbrella? Can I get a prize from the claw machine?) while some can have heavy consequences (e.g., should I consent to a major surgery? Do I take the covid-19 vaccine?). Probability often guides our decisions in such instances. This article presents how Bayesian inference helps us in reasoning everyday problems and explains why different individuals appraise similar evidence differently."
  },
  {
    "objectID": "posts/All-about-that-bayes/index.html#the-central-governor",
    "href": "posts/All-about-that-bayes/index.html#the-central-governor",
    "title": "All about that Bayes: Why individuals appraise similar evidence differently?",
    "section": "The Central Governor",
    "text": "The Central Governor\nThis paradox was first challenged by famous South African sport scientist, Professor Tim Noakes, who also authored the running bible, Lore of Running. In 1996, he delivered the prestigious J. B. Wolffe Memorial Lecture at the American College of Sports Medicine (ACSM) conference. He pointed out that most people did not die running a marathon, which meant they could have ran faster during the race. Therefore, the traditional physiological model was flawed. He later went on to publish a new theoretical model to explain regulation of exercise performance, termed the “Central Governor”, which remains a hot debating topic in the sports science domain to date.\nThis model postulates that exercise performance is regulated by a centralized control center, the brain. Exercise performance is regulated in a manner based on constant feedback between the body and the brain. As we push the limits of our body, the brain generates a whole range of sensations consciously felt by us. The sensations felt during exercise are not the consequences of fatigue, but a protective mechanism to ensure that we complete our exercise in a safe manner. In short, the brain stops us from exercising to the point that we kill ourselves."
  },
  {
    "objectID": "posts/All-about-that-bayes/index.html#why-study-perception",
    "href": "posts/All-about-that-bayes/index.html#why-study-perception",
    "title": "All about that Bayes: Why individuals appraise similar evidence differently?",
    "section": "Why study perception?",
    "text": "Why study perception?\nIt is important to understand that in the context of an endurance event such as a marathon, athletes regulate their exercise performance in an anticipatory manner. This means individuals including novice runners naturally try to pace themselves in a sustainable manner to complete the race. Research have empirically shown that athletes slow down way before they reach the critical core temperature, and before the glycogen tank completely empties. Regulation of exercise is based on the perceived sensations during exercise, which help to provide feedback on finding the balance between safety and performance.\nWhy is there a need to study perceptual responses in sports given the tools to monitor a myriad of objective physiological measures such as heart rate, oxygen consumption and blood lactate? This is because when you think you have reached your point of exhaustion during exercise, the truth is you didn’t.\n\nIt is a perceived failure, and not a true failure.\n\nIt is quite impossible to run yourself to true failure in the context of exercise performance. Your true performance probably only happens when you face a life-and-death situation, such as being chased by a lion. This differentiation between a perceived and true failure is very important, as perceptual responses can be influenced by many other factors besides physiological variables.\nThis is the reason why placebo and psychological effects can affect performance. These effects alter your perceptual responses, and hence your performance. In the words of an Italian sport physiologist, Professor Samuele Marcora, effort perception is claimed to be the “cardinal exercise stopper”. Therefore, whatever factors that can influence your perception can ultimately influence your exercise performance.\nYour perception is in fact your reality."
  },
  {
    "objectID": "posts/All-about-that-bayes/index.html#probability",
    "href": "posts/All-about-that-bayes/index.html#probability",
    "title": "All about that Bayes: Why individuals appraise similar evidence differently?",
    "section": "Probability",
    "text": "Probability\nMany of us should be familiar with the concept of probability, which refers to the likelihood of an event occurring. Probability of any event can be simply calculated by dividing the number of events of interest by the total number of possible outcomes. For example, the probability of attaining a 4 in a six-sided dice roll is 1/6. Given the assumption that the dice is fair, this implies that you expect to roll a 4 in every six rolls.\nComputation of probability is relatively easy when parameters are known. However, this is usually not the case for many situations in our daily lives, in which the parameters are unknown to us. For instance, the probability of winning a prize from a claw machine is harder to ascertain as compared to the dice roll example. Typically, these machines have an operator-adjustable pay out rate that is unknown to players. Thus, how do we determine the probability of receiving a prize from a claw machine?\nWe may examine n number of trials and record the number of times a player successfully wins a prize. For example, we may observe that a player manages to win 2 prizes in a total of 25 trials. Given such observation, we may infer that the probability of winning is 2/25 or 8%.\n\n\n\nProbability density function and cumulative distribution function graphs\n\n\nWe may also model the probability as a continuum using the beta distribution. The beta distribution is a probability distribution on probabilities, defined using two parameters: α and β. In the context of this illustration, α refers to the number of times a prize was won from the claw machine (i.e., 2) and β represents the number of times that a prize was not won (i.e., 23). The figure above presents the probability density function and cumulative distribution function of the beta distribution. The graph on the left informs us that most of the plot’s density is less than 0.2, and we can estimate the 95% confidence interval of the win rate to be approximately between 1% and 21% using the graph on the right. These numbers strongly suggest that the actual likelihood of winning a prize from this claw machine is 20% or less. While it is impossible to deduce the true win rate, both graphs certainly provided more information than a discrete probability value.\nThis approach of observing data and determining the probability of an event occurrence is commonly used in academia and taught in schools. Such interpretation of probability is also known as frequentist inference. As defined by Wikipedia, this method of inference “draws conclusions from sample data by emphasizing the frequency or proportion of the data”. While this may be an objective way of quantifying uncertainty and likelihood, it does not align with how we behave and reason things in our daily lives. For example, given our calculated probability on winning the claw machine above, there are people who may decide to take their chances and others who refrain from it despite seeing the same evidence. The limitation of the frequentist school of thought is the lack of account for differing prior beliefs among different individuals."
  },
  {
    "objectID": "posts/All-about-that-bayes/index.html#bayesian-inference-quantifying-and-updating-our-beliefs",
    "href": "posts/All-about-that-bayes/index.html#bayesian-inference-quantifying-and-updating-our-beliefs",
    "title": "All about that Bayes: Why individuals appraise similar evidence differently?",
    "section": "Bayesian inference: Quantifying and updating our beliefs",
    "text": "Bayesian inference: Quantifying and updating our beliefs\nIn our daily lives, probability is not only used to describe data but also a way to quantify the strength of our beliefs in things about the world (e.g., I think the sun is likely to rise tomorrow, I am confident that the company will still exist in ten years’ time). Our beliefs change consistently as we experience new things and observe new evidence. Another school of thought, Bayesian inference, reflects exactly such process of quantifying and updating our beliefs.\nThe term “Bayesian” originated from an English statistician and Presbyterian minister, Thomas Bayes. He devised the Bayes’ Theorem, which is a concept of conditional probability defined by the formula below. P(A|B) refers to the probability that event A occurs given that event B has already occurred. Bayes Theorem purports that we can work out the probability of P(B|A) if we know the reversed conditional probability, P(A|B).\n\n\n\nBayes’ Theorem\n\n\nThe approach of using Bayes Theorem for subjective probability-based inference was instead popularized by a French scholar, Pierre-Simon Laplace. Such method of inference is termed Bayesian inference, which expresses how the probability of a hypothesis or state of belief should change given new evidence. Now, let us adapt the Bayes Theorem to the context of hypothesis (H) and observed evidence (E).\n\n\n\nApplication of Bayes’ Theorem to Bayesian inference\n\n\nIn the context of Bayesian inference, P(H|E) is referred to as the posterior probability, which represents how strongly we believe in our hypotheses given the evidence we observed. This can be derived using two key components: the likelihood, and the prior probability. First, the likelihood, P(E|H), represents the probability of the evidence given our hypothesis. This component is akin to how we calculate probability using the frequentist approach. Second, the prior probability, P(H), denotes the strength of our belief in our hypothesis before we observed the evidence. This is the key component that separates the frequentist and Bayesian inference approach. Last, we need P(E) to normalize our posterior probability to a value between 0 and 1."
  },
  {
    "objectID": "posts/All-about-that-bayes/index.html#example-what-is-the-probability-of-covid-19-infection-if-you-have-a-positive-antigen-rapid-test",
    "href": "posts/All-about-that-bayes/index.html#example-what-is-the-probability-of-covid-19-infection-if-you-have-a-positive-antigen-rapid-test",
    "title": "All about that Bayes: Why individuals appraise similar evidence differently?",
    "section": "Example: What is the probability of Covid-19 infection if you have a positive antigen rapid test?",
    "text": "Example: What is the probability of Covid-19 infection if you have a positive antigen rapid test?\nLet us suppose you are in Singapore and you are tested positive for Covid-19 infection using an antigen rapid test (ART). As reported in this news article, the ART’s sensitivity (true positive rate) is 82% and its specificity (true negative rate) is 99%. This also indicates that the false positive rate is 1%. Given that we googled and found that the rate of Covid-19 infection in Singapore is approximately 1% at present, what is the probability of infection if you receive a positive swab test?\n\n\n\n\n\nThe table above informs us that the probability of infection is 45.3% after accounting for our prior belief that the rate of infection is 1%. This example also highlights the difference in inference between a frequentist and Bayesian approach. If we do not account for any prior probability, we would have assumed that the probability of infection is equivalent to the likelihood of 82% since that is the true positive rate of the test.\nNow, let us examine the difference in posterior probability if we have a different prior probability. For example, if somebody hypothesizes that the infection rate is higher at 5%, what is the probability of infection given a positive swab test?\n\n\n\n\n\nAs presented in the new table, the new posterior probability increases to 81.2%. Given the same evidence, one who has a higher prior probability is more certain of infection as compared to somebody who has a lower prior probability. This may have implications on mental health since fear and anxiety levels probably differ between 45% and 81% chance of infection."
  },
  {
    "objectID": "posts/All-about-that-bayes/index.html#updating-our-beliefs",
    "href": "posts/All-about-that-bayes/index.html#updating-our-beliefs",
    "title": "All about that Bayes: Why individuals appraise similar evidence differently?",
    "section": "Updating our beliefs",
    "text": "Updating our beliefs\nThe heart of Bayesian inference is the concept of updating our beliefs as we gather new evidence. Importantly, this is a continuous process. Let us take our example one step further and suppose that we took the ART a second time and tested positive again. What will be our new posterior probability?\n\n\n\n\n\nWe change our prior probability to 45.3% based on the first test result and calculate the posterior probability again. The table above informs us that the updated odds of infection is 98.5%. Clearly, we should go to the hospital immediately!"
  },
  {
    "objectID": "posts/All-about-that-bayes/index.html#prior-beliefs-matter",
    "href": "posts/All-about-that-bayes/index.html#prior-beliefs-matter",
    "title": "All about that Bayes: Why individuals appraise similar evidence differently?",
    "section": "Prior beliefs matter",
    "text": "Prior beliefs matter\nThe use of Bayes Theorem demonstrates the importance of prior beliefs in appraising the same evidence. Many of our beliefs and hypotheses about the world differ from person to person. As illustrated in the example, different prior beliefs can result in contrasting posterior probabilities. This plausibly explains why individuals appraise similar evidence differently and thus different behaviours. Thus, it is important to note that probability does not just describe randomness, but also expresses strength of individual beliefs, and their interaction informs our decisions and judgements in daily lives."
  },
  {
    "objectID": "posts/Tokyo-olympics-1500m-analysis/index.html",
    "href": "posts/Tokyo-olympics-1500m-analysis/index.html",
    "title": "Tokyo Olympics: Analysis of the 1500m Event’s Pacing Strategy using the Concept of Critical Speed",
    "section": "",
    "text": "The 1500m event is a demanding middle-distance race in which athletes have to sustain high-intensity efforts over prolonged periods. This blog post employs the concept of critical speed to analyse the pacing strategy in the men’s and women’s 1500m finals at the Tokyo Olympics 2020."
  },
  {
    "objectID": "posts/Tokyo-olympics-1500m-analysis/index.html#introduction",
    "href": "posts/Tokyo-olympics-1500m-analysis/index.html#introduction",
    "title": "Tokyo Olympics: Analysis of the 1500m Event’s Pacing Strategy using the Concept of Critical Speed",
    "section": "Introduction",
    "text": "Introduction\nEnergy output during prolonged cardiorespiratory exercise is produced from a spectrum of both the aerobic (oxidative) and anaerobic (glycolytic) energy systems. The proportion of contribution from either system is often dependent on the exercise intensity levels. Given that venturing into the anaerobic territories is typically associated with fatigue, it has been of interest to scientists, coaches, and athletes to find the optimal exercise intensity to meet the demands of sporting performances.\nNumerous concepts have been established in sports science to try identifying key levels of exercise intensity to predict fatigue and performance. One such concept often used in the world of cycling is the concept of critical power. In layman terms, critical power represents the highest power output that can be sustained for a prolonged period (approximately 30-60 minutes). It has been purported to be a key ‘threshold’ that distinguishes between the heavy and severe intensity domains. The latter domain is characterised by rapid increase in oxygen uptake and blood lactate, and the inability to reach steady states. Any intensity beyond the critical power level will result in the athlete tapping significantly into his or her finite anaerobic capacity. Continuous exercise at such high intensity levels will result in inevitably imminent fatigue.\nThe concept of critical power is relevant to any sporting performances in which athletes have to exercise in the severe intensity domain. A good applicable example in the athletics will be the 1500m event, which requires significant energy contribution from the anaerobic system. In the context of running, the same concept is termed critical speed instead since running is more pertinent to speed than power output. An interesting application of this concept is predicting the time to exhaustion when running beyond the critical speed using the following equation.\nTime to exhaustion = D’/ (S – CS)\nThe concept proposes that when one runs at a given speed (S) above the critical speed (CS), the ability to sustain that speed is limited by the individual’s work capacity (D’). For example, a runner has a CS of 5 m/s and D’ of 200m. If this runner attempts to run at a speed of 7 m/s, it is expected that he or she will reach exhaustion in 100 seconds (as calculated by 200/ (7-5) = 100).\n\n\n\nIllustration of the critical speed concept"
  },
  {
    "objectID": "posts/Tokyo-olympics-1500m-analysis/index.html#calculation-of-critical-speed-cs-and-work-capacity-d",
    "href": "posts/Tokyo-olympics-1500m-analysis/index.html#calculation-of-critical-speed-cs-and-work-capacity-d",
    "title": "Tokyo Olympics: Analysis of the 1500m Event’s Pacing Strategy using the Concept of Critical Speed",
    "section": "Calculation of Critical Speed (CS) and Work Capacity (D’)",
    "text": "Calculation of Critical Speed (CS) and Work Capacity (D’)\nCritical speed (CS) can be computed using the personal records of an individual over different running distances. As illustrated in the figure above, we plot the average speed of one’s best performances against their respective time duration. We will derive a hyperbolic relationship between speed and time. On the left side of the curve, we observe an initial steep decline in the speed that we can sustain as the time duration increases. However, the rate of decline slows and beyond a certain point, any increase in time duration corresponds to minimal changes in the speed. The curve eventually approximates the dotted line, termed the asymptote, which represents the CS. In addition, we can also derive one’s work capacity (D’) from the curve, as denoted by the shaded areas.\nGiven that it requires some effort to interpret a hyperbolic curve, an easier and commonly used method to calculate CS and D’ is to plot the total work done (distance of event) against the time duration, which has a linear relationship. Next, we fit a linear regression line to the values and interpret the values from the straight-line equation. The slope or gradient of the line represents CS, while the y-intercept represents D’. The figure below illustrates an example of this method to calculate the CS and D’ of Timothy Cheruiyot, who is currently first ranking in the men’s 1500m event. I plotted his personal best records between 800-5000m (data from IAAF database) and derived the linear regression equation based on these values. The slope of the equation informs us that his CS is 5.74 m/s, and the y-intercept informs us that his D’ is 261.30 metres.\n\n\n\nEstimation of critical speed and work capacity from a distance-duration graph"
  },
  {
    "objectID": "posts/Tokyo-olympics-1500m-analysis/index.html#application-of-the-critical-speed-concept-to-pacing",
    "href": "posts/Tokyo-olympics-1500m-analysis/index.html#application-of-the-critical-speed-concept-to-pacing",
    "title": "Tokyo Olympics: Analysis of the 1500m Event’s Pacing Strategy using the Concept of Critical Speed",
    "section": "Application of the critical speed concept to pacing",
    "text": "Application of the critical speed concept to pacing\nIn the recent American College of Sports Medicine 2021 Annual Meeting, a research study demonstrated that 1-mile (1600m) world record performances were ran above CS with complete depletion of D’ at the finish line. Since running 1500m is likely akin to running a mile, it is interesting to find out how 1500m event athletes paced themselves in respect to their CS and D’. This blog post examined the pacing profiles of both male and female runners who competed in the 1500m finals at the Tokyo Olympics, whereby runners fought for podium positions instead of world records.\nI adopted the same approach as the research study to calculate the CS and D’ of all the athletes. Data of individual personal best performances between 800-5000m were extracted from the IAAF database, and the linear regression method was used to derive the CS and D’. Given that very old records may not reflect the athlete’s current fitness level accurately, personal bests achieved before 2014 were excluded from the calculation.\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\nTokyo Olympics 1500m Finalists\n\n\n\n\n\n\nCritical Speed(m/s)\nWork Capacity(m)\nResult\nAverage Speed(m/s)\nPercentage ofCritical Speed(%)\n\n\n\n\nMale\n\nJakob Ingebrigtsen\n6.32\n155.88\n03:28:00\n7.21\n114.01\nTimothy Cheruiyot\n5.74\n262.96\n03:29:00\n7.18\n125.10\nAbel Kipsang\n5.98\n199.07\n03:29:00\n7.18\n120.00\nJosh Kerr\n5.85\n194.94\n03:29:00\n7.18\n122.66\nAdel Mechaal\n6.08\n188.71\n03:30:00\n7.14\n117.36\nStewart McSweyn\n6.08\n225.51\n03:31:00\n7.11\n116.91\nCole Hocker\n6.04\n184.32\n03:31:00\n7.11\n117.71\nMichal Rozmys\n5.70\n240.19\n03:32:00\n7.08\n124.13\nJake Heyward\n5.93\n199.59\n03:34:00\n7.01\n118.12\nOliver Hoare\n6.02\n145.21\n03:35:00\n6.98\n116.00\nJake Wightman\n5.60\n264.12\n03:35:00\n6.98\n124.55\nCharles Grethen\n5.70\n227.81\n03:36:00\n6.94\n121.70\nIgnacio Fontes\n5.44\n266.54\n03:38:00\n6.88\n126.51\n\nFemale\n\nFaith Kipyegon\n5.55\n181.89\n03:53:00\n6.44\n116.00\nLaura Muir\n5.40\n203.50\n03:54:00\n6.41\n118.61\nSifan Hassan\n5.63\n167.16\n03:55:00\n6.38\n113.31\nFreweyni Gebreezibeher1\n5.88\n111.76\n03:57:00\n6.33\n107.61\nGabriela Debues-Stafford\n5.48\n163.58\n03:58:00\n6.30\n114.97\nLinden Hall\n5.23\n211.84\n03:59:00\n6.28\n120.05\nWinnie Nanyondo\n4.94\n262.28\n03:59:00\n6.28\n127.25\nNozomi Tanaka\n5.40\n156.39\n03:59:00\n6.28\n116.35\nMarta Perez\n4.96\n248.97\n04:00:00\n6.25\n125.93\nElinor Purrier St. Pierre\n5.37\n176.30\n04:01:00\n6.22\n115.78\nJessica Hull\n5.50\n154.55\n04:02:00\n6.20\n112.78\nCory Ann McGee\n4.92\n269.57\n04:05:00\n6.12\n124.33\nKristiina Maki\n5.19\n197.79\n04:11:00\n5.98\n115.13\n\n\n\nTable: @nxrunning | Data: Tokyo Olympics 2020 & IAAF Database\n\n\n\n\n1 CS and D' values are likely inaccurate due to only two personal best records available\n\n\n\n\n\n\nThe table above presents the CS and D’ values of all 1500m event finalists. Ingebrigtsen and Gebreezibeher have the highest CS values among the males and females, respectively. However, please note that the prediction of CS and D’ for Gebreezibeher may be potentially erroneous due to availability of only two personal best records. In terms of D’, Wightman and McGee possess the highest values within their gender groups. This table clearly suggests that physiology in the context of CS and D’ values can differ among runners at such elite levels.\nThe table also informs us the performances of all runners in the finals. As expected, the intensities of the race for both gender groups were beyond CS. On average, athletes ran at approximately 18% above their CS, which corresponds to a speed that diminished 0.51% of their D’ every second.\n\n\n\nPacing profiles of the top 6 athletes in the Tokyo Olympics Women’s 1500m Finals"
  },
  {
    "objectID": "posts/Tokyo-olympics-1500m-analysis/index.html#analysis-of-womens-race",
    "href": "posts/Tokyo-olympics-1500m-analysis/index.html#analysis-of-womens-race",
    "title": "Tokyo Olympics: Analysis of the 1500m Event’s Pacing Strategy using the Concept of Critical Speed",
    "section": "Analysis of Women’s Race",
    "text": "Analysis of Women’s Race\nThe women’s race was won in 03:53.1 by defending champion, Kipyegon. The silver and bronze medals went to Muir and Hassan, respectively. The figure above presents the pacing behaviours exhibited by the top 6 finishers. All female runners ran at a speed above CS throughout the race, and majority paced themselves in a manner whereby D’ was completely depleted towards the end of the race.\nAll runners started the race relatively close together in a pack and went through the first two laps at approximately 63-65 seconds per 400m. As illustrated in the figure above, the intensity of such pace can be different among the athletes due to their contrasting CS. For example, this pace corresponded to about 20% above CS for Hall while the same running speed was about 15% above CS for Kipyegon instead, who has a more superior CS. This suggests that Hall was working harder as compared to Kipyegon at similar running pace. At the same time, this also means that Hall was diminishing her D’ at a faster rate than Kipyegon. Interestingly, despite Hall having greater D’ (209.39m) than Kipyegon (180.47m), the rate of her D’ decline was greater than Kipyegon due to her lower CS. This suggests that Hall was less likely to sustain the same pace as compared to Kipyegon. The running pack broke into two groups 1000m into the race and Hall expectedly had a difficult time hanging on to the leading group.\nThe critical section of the race was at the 1200m mark, in which runners tend to compete for the final sprint to the finish line. As purported by the concept of CS, information on remaining D’ may tell us the ability of athletes to further push their pace as well as the sustainability of their efforts. The medallists had greater D’ remaining and led the charge in the last lap, with Kipyegon and Muir running the section between 1200 and 1400m at an impressive speed of 27% above CS. In contrast, as shown in the figure, Debues-Stafford started slowing beyond 1200m as her D’ went into the negatives. By the 1400m mark, 5 out of the front 6 runners had emptied their tanks and slowing down of the speed for the final 100m was observed.\nThe figure also highlights certain disagreements between our predictions and actual performances. First, based on the figure, Gebreezibeher ran the entire race at the lowest %CS and had the highest %D’ remaining. However, she was not able to sustain her running speed in the last 100m despite her D’ not completely depleted. This is likely attributed to the inaccuracy of her CS and D’ values being calculated due to lack of available data. The performance suggests that she likely has either lower CS or D’ than what was estimated. Second, the data also informs us that Hassan was expected to sustain the pace that Kipyegon and Muir ran in the last lap. Based on Kipyegon’s running speed of 7.07m/s and their respective remaining D’ values, Kipyegon, Hassan and Muir were estimated to sustain that pace for approximately 20.9s, 20.7s and 15.6s, respectively. However, Muir ‘overperformed’ while Hassan ‘underperformed’ in the race. Obviously, such disagreement could be due to a myriad of other factors that may influence the race performance as well as the inaccuracy of the CS and D’ estimation.\n\n\n\nPacing profiles of the top 6 athletes in the Tokyo Olympics Men’s 1500m Finals"
  },
  {
    "objectID": "posts/Tokyo-olympics-1500m-analysis/index.html#analysis-of-mens-race",
    "href": "posts/Tokyo-olympics-1500m-analysis/index.html#analysis-of-mens-race",
    "title": "Tokyo Olympics: Analysis of the 1500m Event’s Pacing Strategy using the Concept of Critical Speed",
    "section": "Analysis of Men’s Race",
    "text": "Analysis of Men’s Race\nThe men’s race was a crackling race with top 6 finishers smashing the Olympics record. Ingebrigtsen won the race in 03:28.3 and was accompanied by Cheruiyot and Kerr on the podium. The pacing profiles of the top 6 runners are presented in the figure above. Similarly, we observe a general trend in which runners ran at a speed above CS throughout the race with complete depletion of D’ towards the end of the race.\nAmong the top 6 competitors, Cheruiyot started the race at the highest percentage above CS. Although he was working at a relatively higher intensity, his superior D’ compensates for his efforts and he had the highest absolute remaining D’ in the earlier stages of the race. This highlights the importance of examining an individual’s workload with respect to both CS and D’ together to predict fatigue accurately.\nCheruiyot initiated the sprint in the last lap at a pace 25% above his CS, but he was unable to sustain his efforts over the last 100m. Ingebrigtsen displayed his superior physiology as he matched Cheruiyot’s pace and managed to maintain that through the finish line. This suggests that Ingebrigtsen’s present fitness level is likely higher than what we have estimated. Another surprising performance was from Kerr, who was the bronze medallist. As shown in the figure, his average effort over the race was 24% above his estimated CS and he was predicted to empty his tank by 1200m mark. However, he showed an incredible performance in the last lap, running at a speed faster than Cheruiyot and Ingebrigtsen. Based on his estimated D’, his actual efforts would have over-depleted his D’ by 44%. Certainly, he is way fitter than what was estimated."
  },
  {
    "objectID": "posts/Tokyo-olympics-1500m-analysis/index.html#conclusion",
    "href": "posts/Tokyo-olympics-1500m-analysis/index.html#conclusion",
    "title": "Tokyo Olympics: Analysis of the 1500m Event’s Pacing Strategy using the Concept of Critical Speed",
    "section": "Conclusion",
    "text": "Conclusion\nThis blog post has demonstrated the use of the critical speed concept to gain meaningful insights into athlete’s pacing behaviour in the 1500m event. Knowledge of one’s CS and D’ allows us to estimate how hard the athlete is working, and how long he or she can sustain such high-intensity efforts. Albeit a difficult task, accurate information of competitors’ CS and D’ values can help to inform optimal pacing strategy during the race. At the end of the day, the athlete with the most superior physiology tends to win the race, and the concept of critical speed helps us to understand why is it so.\nAll analysis and visualisations were performed using R programming. You may access the data and codes at my github."
  },
  {
    "objectID": "posts/Bayesian-vs-Frequentist-Monty-Hall/index.html",
    "href": "posts/Bayesian-vs-Frequentist-Monty-Hall/index.html",
    "title": "Bayesian versus Frequentist solutions to the Monty Hall Problem",
    "section": "",
    "text": "There are generally two schools of thoughts in defining probability: Bayesian and Frequentism. The former views probability as a degree of our beliefs towards an event occurrence, while the latter views it as a relative frequency of an event occurrence. This post presents the use of both Bayesian and frequentist approaches to solve the famous Monty Hall problem."
  },
  {
    "objectID": "posts/Bayesian-vs-Frequentist-Monty-Hall/index.html#monty-hall-problem",
    "href": "posts/Bayesian-vs-Frequentist-Monty-Hall/index.html#monty-hall-problem",
    "title": "Bayesian versus Frequentist solutions to the Monty Hall Problem",
    "section": "Monty Hall Problem",
    "text": "Monty Hall Problem\nI first chanced upon this probability puzzle in the movie, 21. This puzzle originated from an old American game show Let’s Make a Deal, and was named after the host Monty Hall. Based on Wikipedia, this was made famous in a letter to Marilyn vos Savant’s “Ask Marilyn” column in 1990:\n\nSuppose you’re on a game show, and you’re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what’s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, “Do you want to pick door No. 2?” Is it to your advantage to switch your choice?\n\nMarilyn proposed that you should always switch, as it would double your chance of winning the car from 1/3 to 2/3. This answer was met with huge criticism as people intuitively tend to think that since there are only two doors left, the probability of the car behind either door is 1/2. Hence, the odds are the same regardless of your choice to switch the door. In this blog post, I would demonstrate how we can use either the Bayesian or frequentist approach to answer this paradox."
  },
  {
    "objectID": "posts/Bayesian-vs-Frequentist-Monty-Hall/index.html#thinking-like-a-bayesian",
    "href": "posts/Bayesian-vs-Frequentist-Monty-Hall/index.html#thinking-like-a-bayesian",
    "title": "Bayesian versus Frequentist solutions to the Monty Hall Problem",
    "section": "Thinking like a Bayesian",
    "text": "Thinking like a Bayesian\nThe Bayesian school of thought takes a subjective approach towards probability, and is based on the Bayes’ Theorem. It is fundamentally a concept of conditional probability defined by the formula below. In a nutshell, the resulting probability also known as the posterior probability is derived using three components: the likelihood, the prior probability, and the probability of observing the evidence (to normalize the numerator to a value between 0 and 1). I have written how we can use the Bayes’ Theorem to explain why different individuals appraise similar empirical evidence differently in my previous blog post.\n\n\n\nBayes’ Theorem\n\n\nIn the context of the Monty Hall problem, we are interested in comparing the probability between switching or sticking to our initial choice of door given that the host chose to open a door with a goat behind. Using the example problem presented above, given that door 3 has been revealed to be a goat, the car would be either behind door 1 (our initial choice) or door 2 (if we choose to switch). To solve the problem, let us apply the Bayes’ Theorem formula and compare P (door 1 = car | opens = door 3) and P (door 2 = car | opens = door 3).\n\n\n\nApplication of Bayes’ Theorem to the Monty Hall problem\n\n\n\nPrior Probability - P(A)\nLet’s start with the simplest probability to compute, the prior probability. This refers to our initial expected probability of the car behind either door 1 and door 2 at the start of the game before door 3 was opened. Assuming that the car was randomly assigned, each door has an equal chance of having the car behind. Thus, P (door 1 = car) and P (door 2 = car) have equal probability of 1/3.\n\n\nLikelihood - P(B|A)\nNext, we would compute the likelihood of the host opening door 3 for respective hypotheses. For the first hypothesis, if the car is behind door 1, the host can either open door 2 or 3 to reveal a goat. Hence, P (opens = door 3 | door 1 = car) is 1/2. On the other hand, if the car is behind door 2, the host has no other choice but to open door 3 since it is the only other door with the goat behind. Therefore, for the second hypothesis, P (opens = door 3 | door 2 = car) equals 1.\n\n\nJoint Probability - P(B|A) X P(A)\nWith the likelihood and prior probability values known, we can work out the numerator values of the formula for both hypotheses.\nP (opens = door 3 | door 1 = car) X P (door 1 = car) = 1/2 X 1/3 = 1/6\nP (opens = door 3 | door 2 = car) X P (door 1 = car) = 1 X 1/3 = 1/3\n\n\nProbability of observed evidence - P(B)\nWe can derive the probability of observed evidence by simply adding up the joint probabilities. This value represents the probability that the host chose to open door 3 given the contestant’s selection of door 1.\nP (opens = door 3) = 1/6 + 1/3 = 1/2\n\n\nPosterior Probability - P(A|B)\nFinally, we can solve for our posterior probabilities by inputting all the derived values above into the formula.\nP (door 1 = car | opens = door 3) = (1/6) / (1/2) = 1/3\nP (door 2 = car | opens = door 3) = (1/3) / (1/2) = 2/3\nThe posterior probability values show us that if we stick to our choice of door 1 after the host revealed a goat behind door 3, the chances of winning a car is 1/3. On the contrary, if we choose to take up the offer to switch to door 2, our chances are doubled to 2/3. Therefore, the Bayesian approach supports Marilyn vos Savant’s suggestion to always make the switch if given the choice to."
  },
  {
    "objectID": "posts/Bayesian-vs-Frequentist-Monty-Hall/index.html#thinking-like-a-frequentist",
    "href": "posts/Bayesian-vs-Frequentist-Monty-Hall/index.html#thinking-like-a-frequentist",
    "title": "Bayesian versus Frequentist solutions to the Monty Hall Problem",
    "section": "Thinking like a Frequentist",
    "text": "Thinking like a Frequentist\nIn contrast to a Bayesian approach, as the name suggests, a frequentist determines probability based on sampling frequency instead. For example, if we want to know the probability of getting a ‘heads’ in a coin flip, we may flip the coin for x number of trials and determine the probability based on the frequency distribution of a ‘heads’ occurrence. As purported by Bernoulli’s Law of Large Numbers, the long-run frequency of an event occurrence would converge to its theoretical probability. Therefore, in order to solve the Monty Hall problem, we may run a simulation of the game show puzzle over a large number of trials and compare the frequency of winning a car between the decision to stick to our initial choice and the decision to switch our choices.\nFor this project, I performed a Monte Carlo simulation using python programming language. The Monty Hall game was simulated for a reasonably large number of repetitions and the odds of winning was recorded for respective strategies. The only required package for the simulation is the random package. First, I created three empty lists to store the results for each sample simulation.\n# Create empty lists to store simulation results output\nchance_of_winning_ifswitch_list = []\nchance_of_winning_ifdonotswitch_list = []\npercentage_diff_list = []\nNext, the simulation was set up using ‘for-loops’. In each simulation loop, the locations of the car and goats were randomly assigned and a random door was chosen as the contestant’s initial selection. Given that the game host would always open another door with a goat, the decision to switch would backfire only if the initial selected door happens to have the car prize behind. In other words, the probability of winning if choose not to switch choices is equivalent to the probability of choosing the door with the car in the first place. Therefore, an ‘if-else’ conditional statement was included to check whether the initially selected door has a car or not. If the chosen door does not have a car behind, we assign a value of 1 to represent the event of winning when choose to switch choices. If the chosen door has a car behind, we assign a value of 0 to represent the event of winning when choose not to switch choices.\nTo determine the frequency of winning for each strategy, the odds of winning were calculated based on a sample of 1,000 simulated rounds of the Monty Hall game. This sampling was repeated 10,000 times to determine the distribution of winning percentages for each strategy. A random seed value was set for reproducibility of the results.\n# Create simulation using for-loops\n# Repeat 10000 trials with 1000 samples per trial\n\nfor i in range(10000):    \n    results_list = []    \n\n    for i in range(1000):        \n        door_list = [\"car\", \"goat\", \"goat\"]                 \n        random.shuffle(door_list)        \n        chosen_door_number = random.sample(range(3), 1)        \n        chosen_door = door_list[chosen_door_number[0]]        \n        \n        if chosen_door != \"car\":            \n            results_list.append(1)        \n        else:\n            results_list.append(0)\n    \n    # Compute winning percentage if choose to switch  \n    chance_of_winning_ifswitch = sum(results_list)/len(results_list)*100              \n    \n    # Compute winning percentage if choose not to switch\n    chance_of_winning_ifdonotswitch = 100 - chance_of_winning_ifswitch                    \n    \n    # Compute difference in winning percentage between the two strategies             \n    percentage_diff = chance_of_winning_ifswitch - chance_of_winning_ifdonotswitch  \n             \n    # Append the results to respective lists                \n    chance_of_winning_ifswitch_list.append(chance_of_winning_ifswitch)                             \n    chance_of_winning_ifdonotswitch_list.append(chance_of_winning_ifdonotswitch)    \n    percentage_diff_list.append(percentage_diff)\nThe figure below illustrates the distribution of winning odds for both strategies based on our simulation results. The blue histogram presents the distribution for the strategy to switch while the yellow histogram presents the distribution for the strategy to stick to the initial choice. In accordance with the central limit theorem, the distribution of sample means would approximate a Gaussian or normal distribution when the sample size is large. As expected from our large number of simulation trials, we observe a bell curve that characterises a normal distribution.\n\nThe x-axes of the two histograms clearly indicate the difference in odds of winning between the two strategies. You go home with a car 62-72% of the time if you choose to switch as opposed to 28-38% if you choose not to switch. This is further supported by the 95% confidence intervals computed for both strategies. Within 95% of the sampled winning odds from the 10,000 trials, the frequency of winning if you switch is between 66.646 and 66.705% while sticking to the same door selection would be between 33.295 and 33.354% instead. Since the 95% confidence intervals do not overlap, we can infer that there is a statistically significant difference in winning percentage between the two strategies. Indeed, as illustrated in the third histogram (orange in colour), we are 95% confident that choosing to switch would increase our odds of winning by 33.293 to 33.410%. Therefore, the frequentist approach also supports Marilyn vos Savant’s suggestion to always make the switch if given the choice to."
  },
  {
    "objectID": "posts/Bayesian-vs-Frequentist-Monty-Hall/index.html#conclusion",
    "href": "posts/Bayesian-vs-Frequentist-Monty-Hall/index.html#conclusion",
    "title": "Bayesian versus Frequentist solutions to the Monty Hall Problem",
    "section": "Conclusion",
    "text": "Conclusion\nThis blog post has demonstrated the use of Bayesian and Frequentist approaches to solve the Monty Hall problem. Through the examples above, we can see the different approaches to derive probability values. While both approaches are different, the probabilities derived were similar to one another in the context of the Monty Hall problem. The results clearly suggest that regardless whether you subscribe to a Bayesian or Frequentist paradigm, switching your choice is always a wiser option to increase the chances of winning the car.\nSimulation, analysis and visualisation were performed using Python. Full code can be found here."
  },
  {
    "objectID": "posts/FIFA-WC-22-Reactable/index.html",
    "href": "posts/FIFA-WC-22-Reactable/index.html",
    "title": "Reactable: Visualising FIFA World Cup 22 data in an interactive table",
    "section": "",
    "text": "Data visualisation is a key aspect of communicating our story or message behind the data we have. While it is common to visualise data using a myriad of graphs, tables may be more appropriate or effective at times to present numerous data information. In addition, such mode of data visualisation can be even more effective by allowing readers to interact with the table. This blog post documents my attempt to visualise data on the FIFA World Cup 22 participating teams with interactive tables using R programming language."
  },
  {
    "objectID": "posts/FIFA-WC-22-Reactable/index.html#required-libraries",
    "href": "posts/FIFA-WC-22-Reactable/index.html#required-libraries",
    "title": "Reactable: Visualising FIFA World Cup 22 data in an interactive table",
    "section": "Required Libraries",
    "text": "Required Libraries\nThere are plenty of different packages available in R to create interactive tables. In this blog post, the {reactable} package was used to first build the interactive table and the {reactablefmtr} package was utilised for styling purpose. Besides that, {tidyverse} was used mainly to help with data wrangling.\n\nlibrary(reactable)\nlibrary(reactablefmtr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/FIFA-WC-22-Reactable/index.html#data",
    "href": "posts/FIFA-WC-22-Reactable/index.html#data",
    "title": "Reactable: Visualising FIFA World Cup 22 data in an interactive table",
    "section": "Data",
    "text": "Data\nThis post used a total of three datasets to show information of all FIFA World Cup 22’s participating teams’ historical appearances in the knockout stages of previous editions of the competition:\n\nResults of all World Cup matches played in history (Source: Kaggle)\nParticipating teams in FIFA World Cup 22 and their respective FIFA rankings and points\nCountry flag dataset with URL addresses of country flag images\n\n\nwc_matches <- read.csv(\"wcmatches.csv\")\nwc22_teams <- read.csv(\"wc22teams.csv\")\ncountry_flags<-read.csv(\"country_flags_dataset.csv\")"
  },
  {
    "objectID": "posts/FIFA-WC-22-Reactable/index.html#data-wrangling",
    "href": "posts/FIFA-WC-22-Reactable/index.html#data-wrangling",
    "title": "Reactable: Visualising FIFA World Cup 22 data in an interactive table",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nFirst, data wrangling was performed to retrieve and summarise the required data for visualisation. These data were then joined together as a dataframe for building the interactive table. You may see the full code for data wrangling by unfolding the code chunk below under the code tab. Also, you may see a description of the final dataframe variables under the data tab.\n\nCodeData\n\n\n\n\nCode\n# Modify dataframe into long format to see all participating teams in one column\nwc_matches_v2 <- wc_matches%>%\n  pivot_longer(cols = c(home_team, away_team),\n               names_to = \"Home_away\",values_to = \"Team\")\n\n# Some countries require renaming\nwc_matches_v2[(wc_matches_v2$Team == \"West Germany\"), c(\"Team\")] <- \"Germany\"\nwc_matches_v2[(!is.na(wc_matches_v2$winning_team) & wc_matches_v2$winning_team == \"West Germany\"), c(\"winning_team\")] <- \"Germany\"\n\n# Summarising counts for each country's appearance in different stages of the competition\n\n# Round of 16s\ndf_round16 <- wc_matches_v2|>\n  filter(stage == \"Round of 16\")|>\n  group_by(Team)|>\n  count() |>\n  rename(\"Round_16\" = \"n\")\n# Quarter-finals\ndf_quarter <- wc_matches_v2|>\n  filter(stage == \"Quarterfinals\")|>\n  group_by(Team)|>\n  count()|>\n  rename(\"Quarter_finals\" = \"n\")\n# Semi-finals\ndf_semi <- wc_matches_v2|>\n  filter(stage == \"Semifinals\")|>\n  group_by(Team)|>\n  count()|>\n  rename(\"Semi_finals\" = \"n\")\n# Finals\ndf_final <- wc_matches_v2|>\n  filter(stage == \"Final\")|>\n  group_by(Team)|>\n  count()|>\n  rename(\"Finals\" = \"n\")\n# World cup wins\ndf_wc_wins <- wc_matches_v2|>\n  filter(stage == \"Final\")|>\n  group_by(winning_team)|>\n  count()|>\n  mutate(n = n/2)\n\n# The finals of 1950 competition was played in a round robin format instead and Uruguay was the winner thus adding one count for Uruguay\ndf_wc_wins_v2<-df_wc_wins|>\n  mutate(n = ifelse(winning_team == \"Uruguay\", n+1, n))|>\n  rename(\"Wins\" = \"n\",\n         \"Team\" = \"winning_team\")\n\n# Join all dataframes\nmerged_results_df<-df_round16|>\n  left_join(df_quarter)|>\n  left_join(df_semi)|>\n  left_join(df_final)|>\n  left_join(df_wc_wins_v2)|>\n  replace_na(list(\"Quarter_finals\" = 0,\n                  \"Semi_finals\" = 0,\n                  \"Finals\" = 0,\n                  \"Wins\" = 0))\n\n# Processing of flag data for joining\ncountry_flags_v2 <- country_flags|>\n  rename(\"Team\" = \"Country\")\n\n# Join the data for teams participating in the Fifa world cup 22\n# First filter the teams that are participating in the Fifa world cup 22\nfiltered_df<-merged_results_df|>\n  semi_join(wc22_teams, by = \"Team\")\n# Finally, use left join to merge with the world cup 22 team dataset\nworking_data <- wc22_teams|>\n  left_join(filtered_df)|>\n  left_join(country_flags_v2)|>\n  replace_na(list(\"Round_16\" = 0,\n                  \"Quarter_finals\" = 0,\n                  \"Semi_finals\" = 0,\n                  \"Finals\" = 0,\n                  \"Wins\" = 0))|>\n  select(\"ImageURL\", \"Team\", \"Fifa_rank\", \"Ranking_points\", \n         \"Round_16\",\"Quarter_finals\",\"Semi_finals\", \n         \"Finals\", \"Wins\")|>\n  mutate(Ranking_points = round(Ranking_points, 2))\n\n\n\n\n\n\nRows: 32\nColumns: 9\n$ ImageURL       <chr> \"https://upload.wikimedia.org/wikipedia/commons/1/1a/Fl…\n$ Team           <chr> \"Argentina\", \"Australia\", \"Belgium\", \"Brazil\", \"Cameroo…\n$ Fifa_rank      <int> 3, 38, 2, 1, 43, 41, 31, 12, 10, 44, 5, 4, 11, 61, 20, …\n$ Ranking_points <dbl> 1773.88, 1488.72, 1816.71, 1841.30, 1471.44, 1475.00, 1…\n$ Round_16       <int> 9, 1, 8, 11, 1, 0, 2, 2, 4, 1, 7, 7, 11, 2, 0, 3, 8, 1,…\n$ Quarter_finals <int> 7, 0, 3, 14, 1, 0, 1, 2, 1, 0, 9, 7, 14, 1, 0, 0, 2, 0,…\n$ Semi_finals    <int> 4, 0, 2, 8, 0, 0, 0, 2, 0, 0, 3, 6, 12, 0, 0, 0, 0, 0, …\n$ Finals         <int> 5, 0, 0, 6, 0, 0, 0, 1, 0, 0, 1, 3, 8, 0, 0, 0, 0, 0, 3…\n$ Wins           <dbl> 2, 0, 0, 5, 0, 0, 0, 0, 0, 0, 1, 2, 4, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "posts/FIFA-WC-22-Reactable/index.html#creating-interactive-table",
    "href": "posts/FIFA-WC-22-Reactable/index.html#creating-interactive-table",
    "title": "Reactable: Visualising FIFA World Cup 22 data in an interactive table",
    "section": "Creating Interactive Table",
    "text": "Creating Interactive Table\nNext, turning our dataframe into a table can be simply executed by passing the name of the dataframe as a single argument in the reactable() function. By default, the table would be paginated with ten rows of data per page. The page size could be altered by specifying the defaultPageSize argument.\nImportantly, the table is not static and readers can interact with it. For example, you may sort the data by each column by clicking on the column’s header.\n\n# Removing the ImageURL column from the dataframe for ease of preview\n\nworking_data_v2 <- working_data|>\n  select(-ImageURL)\n\nreactable(working_data_v2,\n          # Default page size is 10 rows per page\n          defaultPageSize = 5)\n\n\n\n\n\n\nAs seen above,the table headers correspond to the variable names in the dataframe by default. We may specify the names for each column under the colDef function. In addition, various arguments (e.g., cell alignment, style) can be passed through this function to customise the columns.\n\nreactable(working_data_v2,\n          # Sorting our table based on the FIFA ranking by default\n          defaultSorted = \"Fifa_rank\",\n          # Defining vertical and horizontal alignment of each column cell to be center by default \n          defaultColDef = colDef(\n            vAlign = \"center\",\n            align = \"center\"\n          ),\n          columns = list(\n          Team = colDef(name = \"Team\",\n                          width = 105),  \n          Fifa_rank = colDef(name = \"FIFA Ranking\"),\n          Ranking_points = colDef(name = \"FIFA Ranking Points\"),\n          Round_16 = colDef(name = \"Round of 16\"),\n          Quarter_finals = colDef(name = \"Quarter-Final\"),\n          Semi_finals = colDef(name = \"Semi-Final\"),\n          Finals = colDef(name = \"Final\"),\n          Wins = colDef(name = \"Champion\")\n          ))"
  },
  {
    "objectID": "posts/FIFA-WC-22-Reactable/index.html#making-our-table-pretty",
    "href": "posts/FIFA-WC-22-Reactable/index.html#making-our-table-pretty",
    "title": "Reactable: Visualising FIFA World Cup 22 data in an interactive table",
    "section": "Making Our Table Pretty",
    "text": "Making Our Table Pretty\nOnce we are satisfied with how the data information are presented, it is time to turn our focus to the aesthetics to make our table more attractive.\n\nInsert images\nThe {reactablefmtr} package provides several additional functions to enhance the styling and formatting of tables built with the {reactable} package. Specifically, the embed_img() function, as the name suggests, enables us to embed images directly from the web into the table. In this blog post, I embedded country flag images into the table with the URL addresses from the country flags dataset.\n\nreactable(working_data,\n          # Sorting our table based on the FIFA ranking by default\n          defaultSorted = \"Fifa_rank\",\n          # Defining vertical and horizontal alignment of each column cell to be center by default \n          defaultColDef = colDef(\n            vAlign = \"center\",\n            align = \"center\"\n          ),\n          columns = list(\n          # Embedding country flag images into the first column. You may specify the height and width of the cell.\n            ImageURL = colDef(cell = embed_img(working_data,width = 45, height = 40,\n                                             horizontal_align = \"center\"), name = \"\"),\n          Team = colDef(name = \"Team\",\n                          width = 105),\n          Fifa_rank = colDef(name = \"FIFA Ranking\"),\n          Ranking_points = colDef(name = \"FIFA Ranking Points\"),\n          Round_16 = colDef(name = \"Round of 16\"),\n          Quarter_finals = colDef(name = \"Quarter-Final\"),\n          Semi_finals = colDef(name = \"Semi-Final\"),\n          Finals = colDef(name = \"Final\"),\n          Wins = colDef(name = \"Champion\")\n          ))\n\n\n\n\n\n\n\n\nInsert bar charts\nAnother useful function from the {reactablefmtr} package is the data_bars() function, which allows us to add bar charts into our table. In this example, I visualised the FIFA ranking points of each country using a bar chart instead of just presenting the actual values. This allows readers to quickly compare the points among different teams in the table.\n\nreactable(working_data,\n          # Sorting our table based on the FIFA ranking by default\n          defaultSorted = \"Fifa_rank\",\n          # Defining vertical and horizontal alignment of each column cell to be center by default \n          defaultColDef = colDef(\n            vAlign = \"center\",\n            align = \"center\"\n          ),\n          columns = list(\n          # Embedding country flag images into the first column. You may specify the height and width of the cell.\n            ImageURL = colDef(cell = embed_img(working_data,width = 45, height = 40,\n                                             horizontal_align = \"center\"), name = \"\"),\n          Team = colDef(name = \"Team\",\n                          width = 105),\n          Fifa_rank = colDef(name = \"FIFA Ranking\"),\n          # Visualising ranking points using bar charts.\n          Ranking_points = colDef(name = \"FIFA Ranking Points\",\n                                    defaultSortOrder = \"desc\",\n                                    align = \"left\",\n                                    width = 180,\n                                    cell = data_bars(working_data,\n                                                     fill_color = \"#FEC310\",\n                                                     bar_height = 10,\n                                                     text_position = \"outside-base\",\n                                                     text_size = 15,\n                                                     background = \"#e1e1e1\")),\n          Round_16 = colDef(name = \"Round of 16\"),\n          Quarter_finals = colDef(name = \"Quarter-Final\"),\n          Semi_finals = colDef(name = \"Semi-Final\"),\n          Finals = colDef(name = \"Final\"),\n          Wins = colDef(name = \"Champion\")\n          ))\n\n\n\n\n\n\n\n\nModifying background colours of cells\nThe last aesthetic improvement covered in this blog post is regarding modification of the background colours of the cells in the table. Thanks to examples by Thomas Mock and Greg Lin, I learnt how to create and apply a colour palette to the table, in which background colours of cells differ based on cell values. This requires writing a function that utilised the ColorRamp() function, which generates a sequence of colours based on a vector of number values between 0 and 1. You may see the chosen colour sequence for this example below.\n\n# Colour scale function by Greg Lin\n# Higher bias values generate more widely spaced colours at the high end\nmake_color_pal <- function(colors, bias = 1) {\n  get_color <- colorRamp(colors, bias = bias)\n  function(x) rgb(get_color(x), maxColorValue = 255)\n}\n\n# Selected colour palette. You may customise your own palette by specifying the colours.\ngood_color <- make_color_pal(c(\"#ffffff\", \"#cfe3f3\", \"#87bbe1\", \"#579fd5\", \"#1077c3\"))\n\n# display the colours\nlibrary(scales)\n# Generating a sequence of numbers between 0 and 1 to create the colour sequence\n# You may specify the number of sequences you need\nseq(0.1, 0.9, length.out = 6) |> \n  good_color() |> \n  show_col()\n\n\n\n\nI applied the above colour palette to the appearance data on the knockout stages of the World Cup competition.\n\nreactable(working_data,\n          # Sorting our table based on the FIFA ranking by default\n          defaultSorted = \"Fifa_rank\",\n          # Defining vertical and horizontal alignment of each column cell to be center by default \n          defaultColDef = colDef(\n            vAlign = \"center\",\n            align = \"center\"\n          ),\n          columns = list(\n          # Embedding country flag images into the first column. You may specify the height and width of the cell.\n            ImageURL = colDef(cell = embed_img(working_data,width = 45, height = 40,\n                                             horizontal_align = \"center\"), name = \"\"),\n          Team = colDef(name = \"Team\",\n                          width = 105),\n          Fifa_rank = colDef(name = \"FIFA Ranking\"),\n          # Visualising ranking points using bar charts.\n          Ranking_points = colDef(name = \"FIFA Ranking Points\",\n                                    defaultSortOrder = \"desc\",\n                                    align = \"left\",\n                                    width = 180,\n                                    cell = data_bars(working_data,\n                                                     fill_color = \"#FEC310\",\n                                                     bar_height = 10,\n                                                     text_position = \"outside-base\",\n                                                     text_size = 15,\n                                                     background = \"#e1e1e1\")),\n          Round_16 = colDef(name = \"Round of 16\",style = function(value){\n                                          value\n                                          normalised <- (value-min(working_data$Round_16))/(max(working_data$Round_16)--min(working_data$Round_16))\n                                          color <- good_color(normalised)\n                                          list(background = color)\n                                        }),\n          Quarter_finals = colDef(\n            name = \"Quarter-Final\",\n            style = function(value){\n                                                value\n                                                normalised <- (value-min(working_data$Quarter_finals))/(max(working_data$Quarter_finals)--min(working_data$Quarter_finals))\n                                                color <- good_color(normalised)\n                                                list(background = color)\n                                              }),\n          Semi_finals = colDef(\n            name = \"Semi-Final\",\n            style = function(value){\n                                             value\n                                             normalised <- (value-min(working_data$Semi_finals))/(max(working_data$Semi_finals)--min(working_data$Semi_finals))\n                                             color <- good_color(normalised)\n                                             list(background = color)\n                                           }),\n          Finals = colDef(\n            name = \"Final\",\n            style = function(value){\n                                        value\n                                        normalised <- (value-min(working_data$Finals))/(max(working_data$Finals)--min(working_data$Finals))\n                                        color <- good_color(normalised)\n                                        list(background = color)\n                                      }),\n          Wins = colDef(\n            name = \"Champion\",\n            style = function(value){\n                                      value\n                                      normalised <- (value-min(working_data$Wins))/(max(working_data$Wins)--min(working_data$Wins))\n                                      color <- good_color(normalised)\n                                      list(background = color)\n                                    })\n          ))"
  },
  {
    "objectID": "posts/FIFA-WC-22-Reactable/index.html#adding-title-subtitle-and-caption",
    "href": "posts/FIFA-WC-22-Reactable/index.html#adding-title-subtitle-and-caption",
    "title": "Reactable: Visualising FIFA World Cup 22 data in an interactive table",
    "section": "Adding Title, Subtitle and Caption",
    "text": "Adding Title, Subtitle and Caption\nLastly, we may apply the finishing touch to our table by inputting an appropriate title, subtitle and caption to aid the reader to better understand the table. This can easily be done with functions from the {reactablefmtr} package, and you may customise various options such as font colour, font size and font weight.\n\nreactable(working_data,\n          # Display the full table in one page\n          pagination = FALSE,\n          # Sorting our table based on the FIFA ranking by default\n          defaultSorted = \"Fifa_rank\",\n          # Defining vertical and horizontal alignment of each column cell to be center by default \n          defaultColDef = colDef(\n            vAlign = \"center\",\n            align = \"center\"\n          ),\n          columns = list(\n          # Embedding country flag images into the first column. You may specify the height and width of the cell.\n          ImageURL = colDef(cell = embed_img(working_data,width = 45, height = 40,\n                                             horizontal_align = \"center\"), name = \"\"),\n          Team = colDef(name = \"Team\",\n                          width = 115),\n          Fifa_rank = colDef(name = \"FIFA Ranking\"),\n          # Visualising ranking points using bar charts.\n          Ranking_points = colDef(name = \"FIFA Ranking Points\",\n                                    defaultSortOrder = \"desc\",\n                                    align = \"left\",\n                                    width = 180,\n                                    cell = data_bars(working_data,\n                                                     fill_color = \"#FEC310\",\n                                                     bar_height = 10,\n                                                     text_position = \"outside-base\",\n                                                     text_size = 15,\n                                                     background = \"#e1e1e1\")),\n          Round_16 = colDef(name = \"Round of 16\",\n                            style = function(value){\n                              value\n                              normalised <- (value-min(working_data$Round_16))/(max(working_data$Round_16)--min(working_data$Round_16))\n                              color <- good_color(normalised)\n                              list(background = color)}),\n          Quarter_finals = colDef(\n            name = \"Quarter-Final\",\n            style = function(value){\n                              value\n                              normalised <- (value-min(working_data$Quarter_finals))/(max(working_data$Quarter_finals)--min(working_data$Quarter_finals))\n                              color <- good_color(normalised)\n                              list(background = color)}),\n          Semi_finals = colDef(\n            name = \"Semi-Final\",\n            style = function(value){\n                              value\n                              normalised <- (value-min(working_data$Semi_finals))/(max(working_data$Semi_finals)--min(working_data$Semi_finals))\n                              color <- good_color(normalised)\n                              list(background = color)}),\n          Finals = colDef(\n            name = \"Final\",\n            style = function(value){\n                              value\n                              normalised <- (value-min(working_data$Finals))/(max(working_data$Finals)--min(working_data$Finals))\n                              color <- good_color(normalised)\n                              list(background = color)}),\n          Wins = colDef(\n            name = \"Champion\",\n            style = function(value){\n                              value\n                              normalised <- (value-min(working_data$Wins))/(max(working_data$Wins)--min(working_data$Wins))\n                              color <- good_color(normalised)\n                              list(background = color)})\n          ))|>\n  # Add title of table\n  add_title(\"FIFA World Cup 2022\",\n            font_color = \"#1077C3\")|>\n  # Add subtitle of table\n  add_subtitle(\"History of participating teams in the knockout stages of the World Cup competition\",\n               font_weight = \"normal\",\n               margin = c(5,0,10,0))|>\n  # Add caption of table\n  add_source(\"Note: Data of knockout stages excluded year 1950 competition as the finals were played in a round robin format instead\",\n             font_style = \"italic\",\n             font_size = 14)|>\n  add_source(\"DATA: KAGGLE| TABLE: TOU NIEN XIANG | NIENXIANGTOU.COM\",\n             font_color = \"#C8C8C8\",\n             margin = c(5,0,0,0))\n\n\nFIFA World Cup 2022\nHistory of participating teams in the knockout stages of the World Cup competition\n\nNote: Data of knockout stages excluded year 1950 competition as the finals were played in a round robin format instead\nDATA: KAGGLE| TABLE: TOU NIEN XIANG | NIENXIANGTOU.COM"
  },
  {
    "objectID": "posts/FIFA-WC-22-Reactable/index.html#beautify-our-table",
    "href": "posts/FIFA-WC-22-Reactable/index.html#beautify-our-table",
    "title": "Reactable: Visualising FIFA World Cup 22 data in an interactive table",
    "section": "Beautify our Table",
    "text": "Beautify our Table\nOnce we are satisfied with how the data information are presented, it is time to turn our focus to the aesthetics to make our table more attractive.\n\nInsert images\nThe {reactablefmtr} package provides several additional functions to enhance the styling and formatting of tables built with the {reactable} package. Specifically, the embed_img() function, as the name suggests, enables us to embed images directly from the web into the table. In this blog post, I embedded country flag images into the table with the URL addresses from the country flags dataset.\n\nreactable(working_data,\n          # Sorting our table based on the FIFA ranking by default\n          defaultSorted = \"Fifa_rank\",\n          # Defining vertical and horizontal alignment of each column cell to be center by default \n          defaultColDef = colDef(\n            vAlign = \"center\",\n            align = \"center\"\n          ),\n          columns = list(\n          # Embedding country flag images into the first column. You may specify the height and width of the cell.\n        ImageURL = colDef(cell = embed_img(working_data,width = 45, height = 40,\n                                        horizontal_align = \"center\"), name = \"\"),\n          Team = colDef(name = \"Team\",\n                          width = 115),\n          Fifa_rank = colDef(name = \"FIFA Ranking\"),\n          Ranking_points = colDef(name = \"FIFA Ranking Points\"),\n          Round_16 = colDef(name = \"Round of 16\"),\n          Quarter_finals = colDef(name = \"Quarter-Final\"),\n          Semi_finals = colDef(name = \"Semi-Final\"),\n          Finals = colDef(name = \"Final\"),\n          Wins = colDef(name = \"Champion\")\n          ))\n\n\n\n\n\n\n\n\nInsert bar charts\nAnother useful function from the {reactablefmtr} package is the data_bars() function, which allows us to add bar charts into our table. In this example, I visualised the FIFA ranking points of each country using a bar chart instead of just presenting the actual values. This allows readers to quickly compare the points among different teams in the table.\n\nreactable(working_data,\n          # Sorting our table based on the FIFA ranking by default\n          defaultSorted = \"Fifa_rank\",\n          # Defining vertical and horizontal alignment of each column cell to be center by default \n          defaultColDef = colDef(\n            vAlign = \"center\",\n            align = \"center\"\n          ),\n          columns = list(\n          # Embedding country flag images into the first column. You may specify the height and width of the cell.\n            ImageURL = colDef(cell = embed_img(working_data,width = 45, height = 40,\n                                             horizontal_align = \"center\"), name = \"\"),\n          Team = colDef(name = \"Team\",\n                          width = 115),\n          Fifa_rank = colDef(name = \"FIFA Ranking\"),\n          # Visualising ranking points using bar charts.\n          Ranking_points = colDef(name = \"FIFA Ranking Points\",\n                                    defaultSortOrder = \"desc\",\n                                    align = \"left\",\n                                    width = 180,\n                                    cell = data_bars(working_data,\n                                                     fill_color = \"#FEC310\",\n                                                     bar_height = 10,\n                                                     text_position = \"outside-base\",\n                                                     text_size = 15,\n                                                     background = \"#e1e1e1\")),\n          Round_16 = colDef(name = \"Round of 16\"),\n          Quarter_finals = colDef(name = \"Quarter-Final\"),\n          Semi_finals = colDef(name = \"Semi-Final\"),\n          Finals = colDef(name = \"Final\"),\n          Wins = colDef(name = \"Champion\")\n          ))\n\n\n\n\n\n\n\n\nModifying background colours of cells\nThe last aesthetic improvement covered in this blog post is regarding modification of the background colours of the cells in the table. Thanks to examples by Thomas Mock and Greg Lin, I learnt how to create and apply a colour palette to the table, in which background colours of cells differ based on cell values. This requires writing a function that utilised the ColorRamp() function, which generates a sequence of colours based on a vector of number values between 0 and 1. You may see the chosen colour sequence for this example below, in which I applied to the appearance data on the knockout stages of the World Cup competition.\n\n# Colour scale function by Greg Lin\n# Higher bias values generate more widely spaced colours at the high end\nmake_color_pal <- function(colors, bias = 1) {\n  get_color <- colorRamp(colors, bias = bias)\n  function(x) rgb(get_color(x), maxColorValue = 255)\n}\n\n# Selected colour palette. You may customise your own palette by specifying the colours.\ngood_color <- make_color_pal(c(\"#ffffff\", \"#cfe3f3\", \"#87bbe1\", \"#579fd5\", \"#1077c3\"))\n\n# display the colours\nlibrary(scales)\n# Generating a sequence of numbers between 0 and 1 to create the colour sequence\n# You may specify the number of sequences you need\nseq(0.1, 0.9, length.out = 6) |> \n  good_color() |> \n  show_col()\n\n\n\n\n\nreactable(working_data,\n          # Sorting our table based on the FIFA ranking by default\n          defaultSorted = \"Fifa_rank\",\n          # Defining vertical and horizontal alignment of each column cell to be center by default \n          defaultColDef = colDef(\n            vAlign = \"center\",\n            align = \"center\"\n          ),\n          columns = list(\n          # Embedding country flag images into the first column. You may specify the height and width of the cell.\n          ImageURL = colDef(cell = embed_img(working_data,width = 45, height = 40,\n                                             horizontal_align = \"center\"), name = \"\"),\n          Team = colDef(name = \"Team\",\n                          width = 115),\n          Fifa_rank = colDef(name = \"FIFA Ranking\"),\n          # Visualising ranking points using bar charts.\n          Ranking_points = colDef(name = \"FIFA Ranking Points\",\n                                    defaultSortOrder = \"desc\",\n                                    align = \"left\",\n                                    width = 180,\n                                    cell = data_bars(working_data,\n                                                     fill_color = \"#FEC310\",\n                                                     bar_height = 10,\n                                                     text_position = \"outside-base\",\n                                                     text_size = 15,\n                                                     background = \"#e1e1e1\")),\n          Round_16 = colDef(name = \"Round of 16\",\n                            style = function(value){\n                              value\n                              normalised <- (value-min(working_data$Round_16))/(max(working_data$Round_16)--min(working_data$Round_16))\n                              color <- good_color(normalised)\n                              list(background = color)}),\n          Quarter_finals = colDef(\n            name = \"Quarter-Final\",\n            style = function(value){\n                              value\n                              normalised <- (value-min(working_data$Quarter_finals))/(max(working_data$Quarter_finals)--min(working_data$Quarter_finals))\n                              color <- good_color(normalised)\n                              list(background = color)}),\n          Semi_finals = colDef(\n            name = \"Semi-Final\",\n            style = function(value){\n                              value\n                              normalised <- (value-min(working_data$Semi_finals))/(max(working_data$Semi_finals)--min(working_data$Semi_finals))\n                              color <- good_color(normalised)\n                              list(background = color)}),\n          Finals = colDef(\n            name = \"Final\",\n            style = function(value){\n                              value\n                              normalised <- (value-min(working_data$Finals))/(max(working_data$Finals)--min(working_data$Finals))\n                              color <- good_color(normalised)\n                              list(background = color)}),\n          Wins = colDef(\n            name = \"Champion\",\n            style = function(value){\n                              value\n                              normalised <- (value-min(working_data$Wins))/(max(working_data$Wins)--min(working_data$Wins))\n                              color <- good_color(normalised)\n                              list(background = color)})\n          ))"
  },
  {
    "objectID": "posts/FIFA-WC-22-Reactable/index.html#useful-links",
    "href": "posts/FIFA-WC-22-Reactable/index.html#useful-links",
    "title": "Reactable: Visualising FIFA World Cup 22 data in an interactive table",
    "section": "Useful links",
    "text": "Useful links\nI have mainly learned how to create the above table from the useful links below.\n\nInteractive data tables for R - Coding examples on various functions of the {reactable} package by its developer, Greg Lin\nReactable - An Interactive Tables Guide - Blog post by Thomas Mock highlighting how to add and format background colours of the table\n\nTables don’t have to be necessarily boring, and many packages are available to help us to spice things up. I look forward to explore more functionality of the {reactable} package!\nThis project is created using R programming language. Full code can be found on my github."
  }
]