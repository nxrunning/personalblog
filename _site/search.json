[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Nien Xiang Tou, PhD",
    "section": "",
    "text": "Data and numbers make me tick. This blog documents my personal projects and my learning journey in programming languages such as R and Python. I write about various topics including statistics, data science, data visualisation, coding and sport science.\nAll views expressed here are my own."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "news\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHarlow Malloc\n\n\nSep 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\nTristan O’Malley\n\n\nAug 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSport science\n\n\n\n\n\n\n\nNien Xiang Tou\n\n\nMay 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science\n\n\n\n\n\n\n\nNien Xiang Tou\n\n\nMay 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSport science\n\n\n\n\n\n\n\nNien Xiang Tou\n\n\nApr 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData visualisation\n\n\n\n\n\n\n\nNien Xiang Tou\n\n\nMar 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science\n\n\nFootball\n\n\n\n\n\n\n\nNien Xiang Tou\n\n\nFeb 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science\n\n\nFootball\n\n\n\n\n\n\n\nNien Xiang Tou\n\n\nFeb 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science\n\n\nFootball\n\n\n\n\n\n\n\nNien Xiang Tou\n\n\nAug 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSport science\n\n\n\n\n\n\n\nNien Xiang Tou\n\n\nAug 3, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nien Xiang Tou",
    "section": "",
    "text": "Read more about me here."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Your-perception-is-your-reality/index.html",
    "href": "posts/Your-perception-is-your-reality/index.html",
    "title": "Your perception is your reality",
    "section": "",
    "text": "We experience a wide range of sensations during exercise. This article highlights the importance of studying perception, especially in the context of regulation of exercise.\nHave you ever stood at the end of a marathon, and witnessed how many people sprinted across the finish line? How is it possible for somebody who slowed down or was walking halfway through the race and yet managed to run the fastest right at the end of the race?\nThis is the end-spurt phenomenon commonly seen in endurance events among elites as well as the recreational athletes. Such phenomenon challenges the concept of fatigue, and this was exactly what sparked my interest in pursuing my PhD study that aims to better understand individual’s perceptual responses in self-regulation of exercise.\nTraditionally, it has been thought that fatigue stems from your physiological limits. From such a perspective, why somebody slows down during a race is completely due to physiological reasons such as depletion of muscle glycogen, lactate, environmental stress or the inability of your heart to supply enough oxygen to the muscles. As a result, the athlete is unable to recruit more muscle fibers to meet the exercise demands and hence slowing down. However, if such hypothesis is true, it is not possible for the athlete to speed up at the end of the race when he or she is supposed to feel the most exhausted."
  },
  {
    "objectID": "posts/Your-perception-is-your-reality/index.html#the-central-governor",
    "href": "posts/Your-perception-is-your-reality/index.html#the-central-governor",
    "title": "Your perception is your reality",
    "section": "The Central Governor",
    "text": "The Central Governor\nThis paradox was first challenged by famous South African sport scientist, Professor Tim Noakes, who also authored the running bible, Lore of Running. In 1996, he delivered the prestigious J. B. Wolffe Memorial Lecture at the American College of Sports Medicine (ACSM) conference. He pointed out that most people did not die running a marathon, which meant they could have ran faster during the race. Therefore, the traditional physiological model was flawed. He later went on to publish a new theoretical model to explain regulation of exercise performance, termed the “Central Governor”, which remains a hot debating topic in the sports science domain to date.\nThis model postulates that exercise performance is regulated by a centralized control center, the brain. Exercise performance is regulated in a manner based on constant feedback between the body and the brain. As we push the limits of our body, the brain generates a whole range of sensations consciously felt by us. The sensations felt during exercise are not the consequences of fatigue, but a protective mechanism to ensure that we complete our exercise in a safe manner. In short, the brain stops us from exercising to the point that we kill ourselves."
  },
  {
    "objectID": "posts/Your-perception-is-your-reality/index.html#why-study-perception",
    "href": "posts/Your-perception-is-your-reality/index.html#why-study-perception",
    "title": "Your perception is your reality",
    "section": "Why study perception?",
    "text": "Why study perception?\nIt is important to understand that in the context of an endurance event such as a marathon, athletes regulate their exercise performance in an anticipatory manner. This means individuals including novice runners naturally try to pace themselves in a sustainable manner to complete the race. Research have empirically shown that athletes slow down way before they reach the critical core temperature, and before the glycogen tank completely empties. Regulation of exercise is based on the perceived sensations during exercise, which help to provide feedback on finding the balance between safety and performance.\nWhy is there a need to study perceptual responses in sports given the tools to monitor a myriad of objective physiological measures such as heart rate, oxygen consumption and blood lactate? This is because when you think you have reached your point of exhaustion during exercise, the truth is you didn’t.\n\nIt is a perceived failure, and not a true failure.\n\nIt is quite impossible to run yourself to true failure in the context of exercise performance. Your true performance probably only happens when you face a life-and-death situation, such as being chased by a lion. This differentiation between a perceived and true failure is very important, as perceptual responses can be influenced by many other factors besides physiological variables.\nThis is the reason why placebo and psychological effects can affect performance. These effects alter your perceptual responses, and hence your performance. In the words of an Italian sport physiologist, Professor Samuele Marcora, effort perception is claimed to be the “cardinal exercise stopper”. Therefore, whatever factors that can influence your perception can ultimately influence your exercise performance.\nYour perception is in fact your reality."
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Nien Xiang Tou",
    "section": "",
    "text": "Please find my research work below.\n\n\nLiu, X., Tou, N. X., Gao, Q., Gwee, X., Wee, S. L., & Ng, T. P. (2022). Frailty and risk of cardiovascular disease and mortality. PloS One, 17(9), e0272527. https://doi.org/10.1371/journal.pone.0272527\nTou, N. X., Wee, S. L., Pang, B. W. J., Lau, L. K., Jabbar, K. A., Seah, W. T., ... & Ng, T. P. (2022). Association of fat mass index versus appendicular lean mass index with physical function–The Yishun Study. Aging and Health Research, 2(3), 100097. https://doi.org/10.1016/j.ahr.2022.100097\nChen, K. K., Lee, S. Y., Pang, B. W. J., Lau, L. K., Jabbar, K. A., Seah, W. T., Tou, N. X., ... & Wee, S. L. (2022). Associations of low handgrip strength and hand laterality with cognitive function and functional mobility–the Yishun Study. BMC Geriatrics, 22(1), 1-8. https://doi.org/10.1186/s12877-022-03363-2\nYu, C. C., Tou, N. X., & Low, J. A. (2022). A comparative study on mental health and adaptability between older and younger adults during the COVID-19 circuit breaker in Singapore. BMC Public Health, 22(1), 1-11. https://doi.org/10.1186/s12889-022-12857-y\nTou, N. X., Wee, S. L., Pang, B. W. J., Lau, L. K., Jabbar, K. A., Seah, W. T., ... & Ng, T. P. (2021). Associations of fat mass and muscle function but not lean mass with cognitive impairment: The Yishun Study. Plos One, 16(8), e0256702. https://doi.org/10.1371/journal.pone.0256702\nTou, N. X., Wee, S. L., Seah, W. T., Ng, D. H. M., Pang, B. W. J., Lau, L. K., & Ng, T. P. (2021). Effectiveness of Community-Delivered Functional Power Training Program for Frail and Pre-frail Community-Dwelling Older Adults: a Randomized Controlled Study. Prevention Science, 22, 1048-1059. https://doi.org/10.1007/s11121-021-01221-y​\nChoo, P. L., Tou, N. X., Pang, B. W. J., Lau, L. K., Jabbar, K. A., Seah, W. T., ... & Wee, S. L. (2021). Timed up and go (tug) reference values and predictive cutoffs for fall risk and disability in singaporean community-dwelling adults: Yishun Cross-Sectional Study and Singapore Longitudinal Aging Study. Journal of the American Medical Directors Association, 22(8), 1640-1645. https://doi.org/10.1016/j.jamda.2021.03.002\n​Tou, N. X., Kee, Y. H., Koh, K. T., Camiré, M., & Chow, J. Y. (2020). Singapore teachers’ attitudes towards the use of information and communication technologies in physical education. European Physical Education Review, 26(2), 481-494. https://doi.org/10.1177/1356336X19869734\n\n\n\nTou, N. X., Balasekaran, G., & Barbosa, T. M. (2021). Influence Of Exercise Experience On Perception Of Prescribed And Preferred Exercise Intensity. Medicine & Science in Sports & Exercise, 53(8S), 10-11. http://doi.org/10.1249/01.mss.0000759152.24965.1c\nTou, N. X., Balasekaran, G., & Barbosa, T. M. (2020). Effects of mental fatigue on maximal exercise test performance in physically active and sedentary adults. Medicine & Science in Sports & Exercise, 52(7S), 626. https://doi.org/10.1249/01.mss.0000681104.26974.97 \nTou, N.X., Balasekaran, G., Thor, D., Tan, I., & Lim, Z. Y. (2017). Validation Of A 10 point OMNI Rating Of Perceived Exertion Colored Scale. Medicine & Science in Sports & Exercise, 49(5S), 754. https://doi.org/10.1249/01.mss.0000519006.11251.49\n\n\n\nTowards Data Science. Combining Python and R for FIFA Football World Ranking Analysis. https://towardsdatascience.com/combining-python-and-r-for-fifa-football-world-ranking-analysis-d71bb6ceacdb"
  },
  {
    "objectID": "posts/Can-possession-predict-wins-in-football/index.html",
    "href": "posts/Can-possession-predict-wins-in-football/index.html",
    "title": "Can possession predict wins in football?",
    "section": "",
    "text": "Ball possession is an important performance indicator in football. This article explores the use of logistic regression model to examine a binary classification problem: whether ball possession predicts match win or not.\nBall possession in football is seen as an imperative aspect of the game. Many successful teams are often characterised by dominating long periods of possession. The first team that sprints to your mind with such nature of play is likely the Spanish giants, Barcelona. They are often defined by their “tiki-taka” style of play, which constitutes short passing and movement to maintain possession.\nPast research have claimed that possession is a critical performance indicator that can differentiate successful teams. Better performing teams tend to maintain significantly longer possession as compared to the less successful teams. In the context of football, it does not matter if you can possess the ball for extended periods but failing to win the match. This leads to an important question, can possession predict wins in football?\nIn view of the imminent English Premier League (EPL) season, let’s revisit last EPL season’s data to answer this question! The data set consists of all results from the total 380 matches played throughout the season. Each row represents the match statistics of a team in a given match. You may see a preview of the data below."
  },
  {
    "objectID": "posts/Can-possession-predict-wins-in-football/index.html#do-better-teams-have-greater-possession",
    "href": "posts/Can-possession-predict-wins-in-football/index.html#do-better-teams-have-greater-possession",
    "title": "Can possession predict wins in football?",
    "section": "Do better teams have greater possession?",
    "text": "Do better teams have greater possession?\nAs mentioned above, it has been claimed that ball possession is a performance indicator that can distinguish the more successful teams. Firstly, let’s visualise the possession of each team ranked by their ball possession!\n\n\n\n\n\nNo surprise that the EPL champions, Manchester City, with the most points tallied also had the most dominating ball possession among all the teams. From the graph, we can see that there is some truth to the claim, with the “top six” teams all ahead in the ball possession rankings as compared to the rest of the league. The highest ball possession recorded was a whopping 82% by Manchester City during match day 29 away to Bournemouth. However, the score did not truly reflect such one-sided dominance, with City edging Bournemouth by just a goal.\n\n\n\nEach data point represents the possession and match outcome of a team. 1 denotes a match win, while 0 denotes otherwise (draw or loss)."
  },
  {
    "objectID": "posts/Can-possession-predict-wins-in-football/index.html#does-ball-possession-translate-into-results",
    "href": "posts/Can-possession-predict-wins-in-football/index.html#does-ball-possession-translate-into-results",
    "title": "Can possession predict wins in football?",
    "section": "Does ball possession translate into results?",
    "text": "Does ball possession translate into results?\nIn the data visualization above, we can see that all results are classified into either wins or otherwise (draw or loss). With so much emphasis on ball possession in today’s football, this scatter plot seems to tell us otherwise, with no clear threshold of possession level that guarantees a win. We can see that a winning team can have close to 20% of possession while a non-winning team can have close to 80% of possession. For example, Crystal Palace pulled off a surprising win (3-2) against Manchester City at the Etihad Stadium on match day 18, with only 21% of ball possession during the match. Therefore, is ball possession a reliable predictor of match wins?"
  },
  {
    "objectID": "posts/Can-possession-predict-wins-in-football/index.html#logistic-regression",
    "href": "posts/Can-possession-predict-wins-in-football/index.html#logistic-regression",
    "title": "Can possession predict wins in football?",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nThis is a classification problem, as we use possession as an independent variable to predict the match result. The problem is simplified to be a binary classification scenario, whereby the result can only have two outcomes: either win or otherwise (i.e. draw or loss). Since the dependent variable is categorical in nature, logistic regression is employed to help us answer our question. In this example, we make use of the StatsModel module, which is an open-sourced Python module encompassing several statistical functions. You may see the summary of the logistic regressions result below.\n\n\nThe results showed that ball possession is a significant predictor of match wins. This suggests that possession is indeed a meaningful indicator of a match outcome. The coefficient of the possession variable is 0.0335, and this implies that when ball possession level increases by 1%, the odds of winning the match increase by 3.41%. Greater ball possession could translate to more creation of goal-scoring opportunities. Possessing the ball more also means reducing opponent’s chances of scoring since a team will need to have the ball first before they can even try to score. Such situations could contribute to a greater likelihood of winning the match."
  },
  {
    "objectID": "posts/Can-possession-predict-wins-in-football/index.html#how-accurate-is-this-model",
    "href": "posts/Can-possession-predict-wins-in-football/index.html#how-accurate-is-this-model",
    "title": "Can possession predict wins in football?",
    "section": "How accurate is this model?",
    "text": "How accurate is this model?\nIn this example, the logistic regression model was trained with the data from all matches in EPL season 18/19. We could deploy the model to predict a match outcome given an input amount of possession levels. The model predicts by calculating the probability of either outcomes (value between 0 and 1). In this example, if a given possession level has a probability of winning greater than 0.5, it will be predicted that the team wins the match and vice versa.\nIn the field of machine learning and data science, the accuracy rate of a classification model can often be interpreted through a confusion matrix. A confusion matrix is a simple table that shows the accuracy of the model by comparing the predicted values with the actual values in the data set. You may see the confusion matrix for this example below.\n\n\n\nConfusion matrix of logistic regression model. Note- 1 denotes win, 0 denotes otherwise (draw or loss).\n\n\nThe confusion matrix compares all 760 predicted outcomes with the actual recorded outcomes. From the matrix, we can see that 372 cases were correctly predicted to be non-wins, also termed the true negatives. 204 cases were incorrectly predicted to be non-wins when they actually produced match wins in reality. Such cases are termed false negatives. 79 cases were also incorrectly predicted to be winning outcomes when they were actually non-wins, also known as the false positives. Lastly, 105 cases were correctly predicted to be classified as match wins, which are regarded as the true positives.\n\nAccuracy rate = (372+105)/760 * 100\n\nThis model was found to be 62.76% accurate! I will leave it to your personal judgement whether this model is acceptable. Obviously, football is a game with so many variables that can influence the outcome of a match. Therefore, it is not surprising to expect such accuracy of a simple model with a single predictor variable. Nevertheless, this post showcases that ball possession is a significant variable that can help us predict match outcomes. However, possession itself does not tell the full story."
  },
  {
    "objectID": "posts/Is-the-EPL-getting-more-competitive/index.html",
    "href": "posts/Is-the-EPL-getting-more-competitive/index.html",
    "title": "Is the English Premier League getting more competitive?",
    "section": "",
    "text": "Is the English Premier League more or less competitive as compared to a decade ago? This article examines and compares the competitiveness of different seasons based on the coefficient of variation metric.\nPresently, Liverpool is en route to being runaway champions in the English Premier League (EPL), with 22 points separating them from second-placed Manchester City. Incredibly, the premier league leaders have gone a total of 42 games unbeaten at the moment, with their last defeat happening at the Ethihad Stadium on 3rd January 2019.\nInevitably, this invites a comparison between Liverpool and the “Invincible” Arsenal team. The latter currently holds the record for the longest unbeaten EPL run with 49 games in 2003/2004. To date, the record has stood for close to 16 years. Can Liverpool surpass this incredible record? In an online article, former Arsenal player Robert Pires commented that if Liverpool goes on to break the record, the league was much harder in the past as compared to present.\nIs he right to say that? This article examines whether the present EPL is indeed getting easier as compared to many years ago. First, let’s visualise how many points were required to become champions across the different past seasons since 2000/2001.\nBased on the figure, we can clearly see that teams need at least 80 points to win the titles. The “Invincible” Arsenal team achieved a total of 90 points in 2003/2004 to lift the championship trophy. However, it is interesting to point out that 90 points would be insufficient to win the league over the past three seasons. Liverpool fans would be especially frustrated that despite accumulating 97 points in 2018/2019 season, they were still that tiny bit shy of the title. Hence, is the league really becoming less competitive like what Robert Pires said?"
  },
  {
    "objectID": "posts/Is-the-EPL-getting-more-competitive/index.html#coefficient-of-variation",
    "href": "posts/Is-the-EPL-getting-more-competitive/index.html#coefficient-of-variation",
    "title": "Is the English Premier League getting more competitive?",
    "section": "Coefficient of variation",
    "text": "Coefficient of variation\nWith the aim to evaluate the competitiveness of the league, we will use the coefficient of variation (CV) metric. CV is calculated by dividing the standard deviation of a dataset by its mean. It simply represents the relative spread/variance of the data, and ranges between 0 to 1. In finance, CV is used to assess the volatility of stocks. In this article’s context, we compare the competitiveness of different EPL seasons based on each season’s CV. If a league is more competitive, we will expect that the point total of teams to be closer to one another, hence a lower CV. On the other hand, a higher CV represents less competitiveness as there is greater spread in point total among teams.\n\nThe figure above illustrates the CV across different seasons. Generally, we see there’s relatively greater variation in more recent seasons as compared to the past. For example, there’s 29% variation in 2003/2004 season as compared to 39% variation in 2018/2019. Based on the graph, the most competitive season was probably in 2010/2011 with the least variation of 25%. This suggests that there’s some truth in Robert Pires’s comments.\n\nNext, let’s further analyse the competitiveness of the league based on different standings of the table. The figure above illustrates the CV of the top six teams across different seasons. This tells a slightly different story as compared to the previous graph. The higher CVs were observed in 2003/2004 and 2004/2005 seasons, which suggests greater dispersion in point total among the top teams. This implies relatively less of a close battle for the title in these two seasons. In contrast, we see a clear trend of decreasing CV from season 2005/2006 onwards till about 2015/2016. This coincides with the shift of the traditional “Top Four” (Arsenal, Manchester United, Chelsea, Liverpool) dominance to the emergence of the “Big Six” with Tottenham Hotspur and Manchester City joining in to challenge for the title.\nAlthough there’s an increase in CV among the top six teams in recent seasons, it is still comparatively less than the “Invincible” season. This suggests that it is much more competitive among the top teams in recent seasons as compared to the past. This also indicates that challenging for a champions league qualifying spot is much tougher now than two decades ago.\nIn conclusion, while the league may be more competitive overall in the past, there is less competition among the top teams when comes to challenging for the title or qualifying for the champions league as compared to present seasons.\nDo you agree with Robert Pires’s comments?"
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "",
    "text": "Web scraping opens doors to a myriad of data sources online. Be it text or numbers, web scraping allows us to extract online data into spreadsheets in our computer. With reference to code from FC RSTATS, this article documents a walk through on how to scrape football data from the web and visualising the data using open-source programming platform, R."
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#required-libraries",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#required-libraries",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "1. Required Libraries",
    "text": "1. Required Libraries\nlibrary(rvest)\nlibrary(ggplot2)\nlibrary(plyr)\nlibrary(ggrepel)\nThe above are the required libraries in this walk-through. The two main libraries are rvest and ggplot2, which are used to perform web scraping and plotting graphs respectively.The plyr library helps us in data cleaning while the ggrepel library helps to avoid overlapping of labels in the graphs."
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#webpage",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#webpage",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "2. Webpage",
    "text": "2. Webpage\nurl <- \"https://en.wikipedia.org/wiki/2019%E2%80%9320_Premier_League\"\nIn this article, we are scraping data from a Wikipedia page, showing information on the present English Premier League. You may access the page through the url link specified above. Specifically, I am interested in extracting information on the goals scored (GF) and goals conceded (GA) of each team from the table below.\n\n\n\nScreenshot of the table in which data would be scraped"
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#scrape-the-data",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#scrape-the-data",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "3. Scrape the data",
    "text": "3. Scrape the data\nscraped_page <- read_html(url)\n\n#Scraping the Team Column\nTeams <- scraped_page %>%  \n    html_nodes(\"h2+ .wikitable th+ td\") %>%  \n    html_text() %>%  \n    as.character()\n\n#Scraping the GF column\nGoals_for <- scraped_page %>%\n  html_nodes(\"h2+ .wikitable td:nth-child(7) , th:nth-child(7) abbr\") %>%\n  html_text() %>%\n  as.numeric\n\n#Scraping the GA column\nGoals_against<- scraped_page %>%\n  html_nodes(\"h2+ .wikitable td:nth-child(8)\") %>%\n  html_text() %>%\n  as.numeric\nFirst, we read the web page and save the data to the variable named scraped_page. Based on the above table, we are interested in three columns: Team, GF and GA. Each column is scraped separately and saved to a variable name (i.e. Teams, Goals_for, Goals_against). The codes look identical to one another since we are just repeating the scraping process. The difference is to specify the location of the data to be scraped. The respective locations for each location are specified in red above. For example, “h2+ .wikitable th+ td” tells the code to locate the team column in the scraped page. Each location can be easily found through the use of SelectorGadget. Click the link to see more details."
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#cleaning-the-data",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#cleaning-the-data",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "4. Cleaning the data",
    "text": "4. Cleaning the data\n#Removing \"\\n\" in the teams \nTeams <- gsub(\"\\n\",\"\",Teams)\n\n#Remove NA case\nGoals_for <- na.omit(Goals_for)\n\n#Combining all scraped data into a dataframe\ndf <- data.frame(Teams, Goals_for, Goals_against)\n\n#Renaming the values of Liverpool and Manchester City \ndf$Teams <- mapvalues(df$Teams, from=c(\"Liverpool (Q)\",\"Manchester City[a]\"), to=c(\"Liverpool\", \"Manchester City\"))\nIf you preview each of the variable, you will find some slight discrepancies. For example, many of the teams have a ‘\\n’ in their names. Hence, the above code helps to clean the data and combines them into a nice spreadsheet below."
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#data-visualisation",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#data-visualisation",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "5. Data Visualisation",
    "text": "5. Data Visualisation\nFollowing the code from FC RSTATS, we can create a scatter plot as below. Each point represents each team in the league. The x and y axes represent the goals scored and goals conceded respectively. The plot is divided into quadrants by the two dotted lines. The horizontal dotted line represents the average goals conceded while the vertical horizontal dotted line represents the average goals scored.\nBased on this simple graph, we can see that the teams located on the bottom right are the better performing teams with both strong attack and defence. Conversely, the teams in the top left quadrant have the worst performance."
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#dividing-teams-into-four-clusters",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#dividing-teams-into-four-clusters",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "6. Dividing teams into four clusters",
    "text": "6. Dividing teams into four clusters\nTo better visualise the separation in performance, we can represent each quadrant in different colours. First, we create a new column in the spreadsheet and name it cluster. The code below will separate each team into their respective clusters based on their goal stats.\n#Creating a cluster column\ndf$Cluster = 0\n\n#Using for loop and ifelse statements to create different clusters \nfor(i in 1: nrow(df)) {\n  if (df$Goals_for[i] > mean(df$Goals_for) & df$Goals_against[i] < mean(df$Goals_against)) {\n    df$Cluster[i] = 1\n  } else if (df$Goals_for[i] < mean(df$Goals_for) & df$Goals_against[i] < mean(df$Goals_against)) {\n    df$Cluster[i] = 2 \n  } else if (df$Goals_for[i] > mean(df$Goals_for) & df$Goals_against[i] > mean(df$Goals_against)) {\n    df$Cluster[i] = 3\n  } else {\n    df$Cluster[i] = 4\n  }\n}\n\n#Making the cluster columns a factor\ndf$Cluster = factor(df$Cluster, levels = c(1, 2, 3, 4), \n                    labels = \n                    c(\"Strong attack\\nStrong defence\",\n                    \"Poor attack\\nStrong defence\",\n                    \"Strong attack\\nPoor defence\",\n                    \"Poor attack\\nPoor defence\"))\n                    \nNext, we will use the ggplot2 library to visualise our data. The comments in green help to explain each respective line of code.\n#Specifying the variables and using colours to represent each cluster\nggplot(df, aes(x = Goals_for, y = Goals_against, label = Teams, colour = Cluster))+\n  #Plotting each team on the graph using a geometric point\n  geom_point()+\n  #Plotting the vertical dotted line to represent the average goals scored\n  geom_vline(xintercept=mean(Goals_for), linetype=\"dashed\", \n  alpha = 0.4, colour = \"red\") +\n  #Plotting the horizontal dotted line to represent the average goals conceded\n  geom_hline(yintercept=mean(Goals_against), linetype=\"dashed\", \n  alpha = 0.4, colour = \"red\") +\n  #Geom_text_repel helps to avoid overlapping of labels in the points\n  geom_text_repel(aes(Goals_for, Goals_against, label = Teams), \n                  size = 2, colour = \"black\", fontface = \"bold\")+\n  #Labelling of axes and title\n  labs(title = \"EPL 2019/2020: Goals For vs Goals Against\",\n       x = \"Goals For\", y = \"Goals Against\",\n       colour = \" \")+\n  #Specifying position of the legend\n  theme(legend.position = \"top\")\n\n#Save the plot in your working directory\nggsave(\"output.jpeg\", dpi = 300)\n                    \nThe resulting plot is below.\n\n\n\n\n\nFull code for this article can be accessed through my github.\nPS: If you actually count the number of points on the plot, there are only 19 instead of 20. This is because Newcastle and Watford happen to have exactly the same goal stats at the time of writing."
  },
  {
    "objectID": "posts/Animated-data-viz-covid19/index.html",
    "href": "posts/Animated-data-viz-covid19/index.html",
    "title": "Animated Data Visualisation of Covid-19",
    "section": "",
    "text": "Number of infected cases and deaths around the world continues to rise daily. This article visualises how the pandemic has evolved in certain countries using animated line plots on R."
  },
  {
    "objectID": "posts/Animated-data-viz-covid19/index.html#china-breakout",
    "href": "posts/Animated-data-viz-covid19/index.html#china-breakout",
    "title": "Animated Data Visualisation of Covid-19",
    "section": "China Breakout",
    "text": "China Breakout\nWhilethe true origin of the virus remains debatable, the very first reported covid-19 case was detected in Wuhan City, Hubei Province of China. Since then, the numbers have skyrocketed. Using the numbers reported in the daily situation reports by World Health Organization (WHO), let’s visualise how the outbreak has developed since 1st February 2020 using gganimate on R.\n\n\n\n\n\nThe figure above illustrates how the number of cases increased day by day. The exact dates are reflected in the subtitle. Over the period of almost two months, the number of confirmed cases has increased from 11821 to 81601 at the point of writing. From the graph, we can see a sharp increase in cases on 14th February. It can also be seen that the rate of increase seems to decline since March."
  },
  {
    "objectID": "posts/Animated-data-viz-covid19/index.html#southeast-asia",
    "href": "posts/Animated-data-viz-covid19/index.html#southeast-asia",
    "title": "Animated Data Visualisation of Covid-19",
    "section": "Southeast Asia",
    "text": "Southeast Asia\nNext, let’s visualise how the virus has developed closer to home. The figure below illustrates the breakout in certain Southeast Asian countries.\n\n\n\n\n\nAt the start of February, Singapore was leading the charts over its neighbours. Things have changed very quickly since early March. The number of reported cases in Malaysia surpassed Singapore on 15th March. Presently, the pandemic is faring much worse in many other Southeast Asian countries as compared to Singapore."
  },
  {
    "objectID": "posts/Animated-data-viz-covid19/index.html#rest-of-the-world",
    "href": "posts/Animated-data-viz-covid19/index.html#rest-of-the-world",
    "title": "Animated Data Visualisation of Covid-19",
    "section": "Rest of the World",
    "text": "Rest of the World\nCovid-19 was announced a pandemic on 11th March 2020 given the alarming levels of the virus outbreak. In such a connected world today, it was hard for any countries to get off the hook. Let’s take a look at some other notable countries being hit.\n\n\n\n\n\nThe graph clearly shows the trajectories of the virus spread differ between countries. For example, there was a sharp increase in number of cases in South Korea during the second half of February. The rate of increase quickly slowed in March. On the other hand, Italy is still facing an exponential manner of spread at the moment. Based on the figures, we can infer the extent of success in countries’ attempt to contain the virus.\nData visualisation code and dataset can be found on my github."
  },
  {
    "objectID": "posts/lactate-paradox/index.html",
    "href": "posts/lactate-paradox/index.html",
    "title": "Lactate Paradox",
    "section": "",
    "text": "Blood lactate is associated with fatigue during maximal exercise. This post writes about the phenomenon of reduced blood lactate concentration found with increasing altitude.\nEnergy production during physical work comes from three energy systems: 1) adenosine triphosphate creatine phosphate (ATP-CP), 2) anaerobic glycolysis, and 3) aerobic system. All three systems work concurrently to produce energy, but dominance of any one system is dependent on exercise intensity and duration. Supply of energy for muscle contraction during endurance exercise primarily comes from either aerobic or anaerobic systems. The key difference between the two systems is that aerobic system requires oxygen in the production of energy but anaerobic system does not.\nIt has been traditionally believed that endurance exercise performance is limited by one’s maximal aerobic capacity (VO2max). Exercising beyond this capacity will shift the energy supply to become more reliant on the anaerobic energy system instead. Consequently, low oxygenation in muscles leads to accumulation of blood lactate, or more commonly known as lactic acid. In a classic study, high levels of blood lactate was found in frog muscles that were stimulated to contract to failure. This has led to the hypothesis that blood lactate is associated with physical fatigue."
  },
  {
    "objectID": "posts/lactate-paradox/index.html#exercising-in-altitude",
    "href": "posts/lactate-paradox/index.html#exercising-in-altitude",
    "title": "Lactate Paradox",
    "section": "Exercising in Altitude",
    "text": "Exercising in Altitude\nIf you have been in the mountains, you will notice that breathing gets more difficult with advancing altitude levels. Naturally, exercise performance also becomes more exhaustive under such conditions. Since blood lactate is associated with anaerobic energy pathways, higher lactate concentrations are expected at high altitude, whereby oxygen levels are lower. Indeed, this is so among individuals who first experienced altitude. However, peak blood lactate concentration was found to decrease with increasing altitude among individuals who were acclimatised to altitude.\nThe phenomenon of lower peak blood lactate concentration under greater hypoxic conditions at high altitude levels is puzzling to physiologists. This phenomenon is termed the “lactate paradox”. The paradox was confirmed in Operation Everest II, a study that examined acclimatisation effects of participants at altitude levels equivalent to the summit of Mount Everest, approximately 8848 metres high. It was discovered that the blood lactate concentrations among acclimatised subjects during maximal exercise at high altitude level were not significantly different as compared to during rest at sea levels.\nTo date, the lactate paradox remains poorly understood. Nevertheless, this phenomenon suggests that blood lactate is not necessarily associated with physical fatigue. Hence, the theoretical belief that endurance exercise performance is limited by accumulation of blood lactate in skeletal muscles is unlikely correct."
  },
  {
    "objectID": "posts/web-scraping-goodreads/index.html",
    "href": "posts/web-scraping-goodreads/index.html",
    "title": "Web Scraping Goodreads with BeautifulSoup: Popular runnning books",
    "section": "",
    "text": "Data is incredibly important to every analyst or data scientist. Web scraping is a valuable skill that allows us to access the limitless sources of data online. In my previous blogpost, I have documented the use of R to scrape football data from a wikipedia page. This post presents my attempt on scraping information of popular running books from Goodreads using Python programming language."
  },
  {
    "objectID": "posts/web-scraping-goodreads/index.html#required-tools",
    "href": "posts/web-scraping-goodreads/index.html#required-tools",
    "title": "Web Scraping Goodreads with BeautifulSoup: Popular runnning books",
    "section": "Required Tools",
    "text": "Required Tools\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\nimport pandas as pd\nWe will require four library packages for this project. The request package helps with making HTTP requests from the website of interest. The BeautifulSoup package is a popular web scrapping python library and this helps us to perform the main work. The urllib.parse helps us to join URL addresses and the pandas library helps us to tidy the data scraped into a nice data frame."
  },
  {
    "objectID": "posts/web-scraping-goodreads/index.html#website",
    "href": "posts/web-scraping-goodreads/index.html#website",
    "title": "Web Scraping Goodreads with BeautifulSoup: Popular runnning books",
    "section": "Website",
    "text": "Website\nThis page on Goodreads presents the top 50 books related to running (see below). It showcases some information of each book such as the title, author names and ratings. However, if you want to read more detailed description of each book, you will have to click on the link of the book title.\n\n\n\nScreenshot of website"
  },
  {
    "objectID": "posts/web-scraping-goodreads/index.html#set-up",
    "href": "posts/web-scraping-goodreads/index.html#set-up",
    "title": "Web Scraping Goodreads with BeautifulSoup: Popular runnning books",
    "section": "Set up",
    "text": "Set up\nIf you right click on most web pages, you may inspect its html codes. The codes are structured with several tags, classes and attributes that serve different purposes. Generally, web scraping locates the data that you are interested to extract based on information from these codes. The codes below help us to extract the website’s html and also create a BeautifulSoup object that we will further wrangle with. You may also examine the html file exported.\n# Specifying website url\nbase_site = \"https://www.goodreads.com/shelf/show/running\" \n\n# Make http request\nresponse = requests.get(base_site)\n\n# Get the html from webpage\nhtml = response.content\n\n# Creating a BeautifulSoup object with the use of a parser\nsoup = BeautifulSoup(html, \"lxml\")\n\n# Exporting html file\nwith open('popularrunningbooks.html', 'wb') as file:\n    file.write(soup.prettify('utf-8'))\nThe aim of this web scraping project was to extract relevant information regarding each of these 50 books: 1) book title, 2) author name(s), 3) book rating, 4) book pages, 5) book description. The general workflow to retrieve these information follows the same steps as if we were to manually do it. This involves us clicking on each of the book links and extract the data of interest. Hence, the very first step to help us automate this process is to extract this list of book links from the BeautifulSoup object we created earlier.\nHtml codes are generally built within many layers, similar to putting a present in several layers of gift boxes. Therefore, scraping data is akin to unwrapping the present layer by layer. Typically, a website’s content is hidden under the ‘div’ tag, which represents the outermost layer of the box. Hence, this is usually the starting point to unwrap our “present”. We could also specify the class and ID to help us better locate the data that we want. In this case, the book links are within the class “elementList”.\n# First layer: The element that contains all the data\ndivs = soup.find_all(\"div\", {\"class\": \"elementList\"})\n\n# Second layer: Extracting html tags that contain the links\nlinks = [div.find('a') for div in divs]\nThe url information of each book links are located in the links. However, each of the url extracted is only a partial web address. For example, the corresponding partial url link for the book “Born to Run” looks like ‘/book/show/6289283-born-to-run’. In order to get the full url, we will use the urljoin method from the urllib.parse package to join our base site web address with each of these partial url links.\n# Extracting the partial links  \nrelative_url = [link['href'] for link in links]  \n\n# Computing the full url addresses \nfull_url = [urljoin(base_site, relativeurl) for relativeurl in relative_url]\nIf you inspect the full_url list , some unnecessary non-book links were accidentally extracted as well. Hence, the code below will help to overcome this problem.\n# Filter only the book links\nbook_url = [url for url in full_url if \"https://www.goodreads.com/book/show\" in url]"
  },
  {
    "objectID": "posts/web-scraping-goodreads/index.html#scraping-information-of-each-book",
    "href": "posts/web-scraping-goodreads/index.html#scraping-information-of-each-book",
    "title": "Web Scraping Goodreads with BeautifulSoup: Popular runnning books",
    "section": "Scraping information of each book",
    "text": "Scraping information of each book\nFinally, we have arrived at the main web scraping work. Imagine clicking on each of the book links and retrieve the data we need. Programming helps us to automate this process. First, we create five empty lists, whereby each list will store its respective information.\nbook_description = []\nbook_author = []\nbook_title = []\nbook_rating = []\nbook_pages = []\nThe scraping process involves some similar steps stated earlier, whereby we have to retrieve the html code of each book link and locate the information we need. The same steps will be repeated for every link. The for-loop below helps us to perform this repetitive work.\n#creating a loop counter\ni = 0\n\n#Loop through all 50 books\nfor url in book_url:\n    \n    #connect to url page\n    note_resp = requests.get(url)\n    \n    #checking if the request is successful\n    if note_resp.status_code == 200:\n        print(\"URL{}: {}\".format(i+1, url))\n    \n    else:\n        print('Status code{}: Skipping URL #{}: {}'.format(note_resp.status_code, i+1, url))\n        i = i+1\n        continue\n    \n    #get HTML from url page\n    note_html = note_resp.content\n    \n    #create beautifulsoup object for url page\n    note_soup = BeautifulSoup(note_html, 'html.parser')\n    \n    #Extract Author particulars\n    author_divs = note_soup.find_all(\"div\",{\"class\":\"authorName__container\"})\n    author_text = author_divs[0].find_all('a')[0].text\n    book_author.append(author_text)\n    \n    #Extract title particulars\n    title_divs = note_soup.find_all(\"div\", {\"class\": \"last col\"})\n    title_text = title_divs[0].find_all('h1')[0].text\n    book_title.append(title_text)\n    \n    #Extract rating particulars\n    rating_divs = note_soup.find_all(\"div\", {\"class\": \"uitext stacked\", \"id\": \"bookMeta\"})\n    rating_text = rating_divs[0].find_all(\"span\", {\"itemprop\": \"ratingValue\"})[0].text\n    book_rating.append(rating_text)\n    \n    #Extracting page particulars\n    page_divs = note_soup.find_all(\"div\", {\"class\": \"row\"})\n    try:\n        page_text = page_divs[0].find_all(\"span\", {\"itemprop\": \"numberOfPages\"})[0].text.strip('pages')\n    except IndexError:\n        page_text = 0\n    book_pages.append(page_text)\n    \n    #Extracting description particulars\n    description_divs = note_soup.find_all(\"div\", {\"class\": \"readable stacked\", \"id\": \"description\"})\n    try:\n        description_text = description_divs[0].find_all(\"span\")[1].text\n    except IndexError:\n        try:\n            description_text = description_divs[0].find_all(\"span\")[0].text\n        except IndexError:\n            description_text = \"Nil\"\n    book_description.append(description_text)\n        \n    #Incremeting the loop counter\n    i = i+1\nIt will take a couple of minutes to scrape through all 50 links. Most of the raw data look messy, and hence require some cleaning up. After some tidying, we can use the pandas package to organise all the data into a data frame (see below).\n\nYou may also sort the data frame based on its ratings using the sort_values method. That will inform us that the highest rated book is “The Rise of the Ultra Runners: A Journey to the Edge of Human Endurance” by Adharanand Finn with an average 4.45 rating. Finally, we can export all these data into a nice csv file for ease of viewing on Excel, using the to_csv method.\n# Export dataframe into csv file\nsorted_book_df.to_csv(\"top running books.csv\")\nHope you enjoy this blog post and full code can be found here. Similar codes can be used to scrape other book lists on Goodreads. For my running friends, you may check out the final csv file over here."
  },
  {
    "objectID": "posts/Pace-variation-in-marathon-running/index.html",
    "href": "posts/Pace-variation-in-marathon-running/index.html",
    "title": "Pace variation in Marathon running: Analysing data from Standard Chartered Singapore Marathon 2019",
    "section": "",
    "text": "Pacing strategy is an important determinant of performance, especially in endurance events such as the marathon. Does pacing profile differ across runners of different levels? This blog post examines the pace variation of Singaporean male and female runners in the Standard Chartered Singapore Marathon (SCSM) 2019."
  },
  {
    "objectID": "posts/Pace-variation-in-marathon-running/index.html#art-of-pacing",
    "href": "posts/Pace-variation-in-marathon-running/index.html#art-of-pacing",
    "title": "Pace variation in Marathon running: Analysing data from Standard Chartered Singapore Marathon 2019",
    "section": "Art of Pacing",
    "text": "Art of Pacing\nPacing strategy in long-distance running refers to the distribution of running speed across the whole duration of event. Finding an optimised pacing strategy in a marathon event is no easy feat, as the distance often poses many uncertainties and unexpected challenges. Too conservative start may lead to a sub-optimal finish, while an aggressive start may result in early fatigue and excessive slowing down during latter stages of the race.\nGenerally, there are three types of pacing profiles: 1) negative, 2) positive, and 3) even pacing strategy. First, negative pacing refers to an increase in speed over the duration of event. This will mean running the second half of the race faster than the first half. Second, positive pacing is characterised by a decline in speed instead. Third, even pacing represents keeping relatively the same speed throughout the event. Such even pacing was observed in Eliud Kipchoge’s shattering of the 2 hour marathon barrier last year. If you look at his time splits, he and his pacing team ran each 5km within a range of 14:10 - 14:14. This suggests that adopting an even pacing profile in the marathon may be the most ideal strategy to hit your race goals.\nRealistically, this is a tall order and gets increasingly harder to achieve as the race distance increases. Very often, we see drastic changes in running speed during endurance events. For example, we see more race participants walking in the second half of the marathon as compared to the first half. This is an expected phenomenon since fatigue kicks in as the race progresses. However, is such variation in pace similar across all levels of runners?\nWith the aim to answer this question, I scraped the time splits of all Singaporean runners who finished the full marathon in last year’s SCSM. The web scraping was performed using the pandas and BeautifulSoup package in python. You may find the full web scraping code here.\n\n\n\n\n\nBased on the official results from the race website, a total of 3929 Singaporean men and 911 women completed the full marathon. Clearly, the marathon is not as popular among females than their male counterparts. Data on each 5km time split was retrieved for each runner. However, several runners were found with missing data for certain distances. Hence, for runners with missing time splits, it was assumed that their pace remained the same as the last recorded time split. Runners with missing data for the first 5km and 10km were considered invalid and removed from the dataset. Thus, I ended up with a total of 3886 men and 902 women for the analysis.\nThe coefficient of variation (CV) was used to measure the variation in each runner’s pace throughout the race. This was simply computed by dividing the standard deviation of all time splits by the mean speed. Higher CVs represent greater variation in pace and lower CVs indicate more consistency in running speed throughout the race. The CV was compared among three groups of runners in both gender groups: fast, mid-pack and slow. The fast group represents the top 20%, the mid-pack represents the middle 20%, and the slow group represents the bottom 20%. The notched box plots above illustrate the average running speed distribution of each group separated by genders. Evidently, the speeds differed significantly across groups for both genders.\n\n\n\n\n\nThe graph above presents the average CV scores in percentages of respective groups for both genders. We can see a trend that pace variation increases with slower marathon running times. Using a statistical test (one-way ANOVA) informed us that the differences in pace variation across groups were significant. The top 20% Singaporean male runners had an average of 12.11% in their marathon pace variation, and this was significantly less than the mid-pack (14.93%) and the slow group (15.59%). However, the mid-pack and slow groups showed comparably similar variation in pace. Similarly, for the females, the fast group (8.14%) showed significantly less variation in their pace than their slower counterparts. The mid-pack group (12.8%) also differed significantly as compared to the slow group (15.32%)."
  },
  {
    "objectID": "posts/Pace-variation-in-marathon-running/index.html#importance-of-consistency-in-pacing",
    "href": "posts/Pace-variation-in-marathon-running/index.html#importance-of-consistency-in-pacing",
    "title": "Pace variation in Marathon running: Analysing data from Standard Chartered Singapore Marathon 2019",
    "section": "Importance of consistency in pacing",
    "text": "Importance of consistency in pacing\nThese numbers highlighted that better performing runners were not only faster in their speed, they also exhibited considerably less variation in their marathon running pace. Obviously, many factors can influence a marathon performance. One aspect is definitely optimising your pacing strategy during the race. This means selecting a pace that you can sustain consistently with minimal variation throughout the race. This is an important takeaway for all levels of runners. Hence, do practise your pacing strategy during training, and carefully plan your pace for race day. It will probably make a difference!\nData processing, analysis and visualisation were performed on R. Full code and datasets can be found here."
  }
]