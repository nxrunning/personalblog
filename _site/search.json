[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Nien Xiang Tou, PhD",
    "section": "",
    "text": "Data and numbers make me tick. This blog documents my personal projects and my learning journey in programming languages such as R and Python. I write about various topics including statistics, data science, data visualisation, coding and sport science.\nAll views expressed here are my own."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "news\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nHarlow Malloc\n\n\nSep 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\nTristan O’Malley\n\n\nAug 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSport science\n\n\n\nThis post examines the prevalence of the end spurt in marathon running based on data from Standard Chartered Singapore Marathon 2019.\n\n\n\nNien Xiang Tou\n\n\nJul 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFootball\n\n\nData visualisation\n\n\n\nThis post examines whether Liverpool’s title winning success is attributed to luck or genuine performance based on the expected goals metric.\n\n\n\nNien Xiang Tou\n\n\nJul 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData visualisation\n\n\n\nThis article documents the visualisation of Singapore’s Covid-19 clusters on the map using R programming\n\n\n\nNien Xiang Tou\n\n\nMay 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSport science\n\n\n\nThis post examines whether variation in marathon running pace differs among runners of different performance levels.\n\n\n\nNien Xiang Tou\n\n\nMay 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science\n\n\n\nThis post documents the use of Python to scrape popular running books from Goodreads using the BeautifulSoup package.\n\n\n\nNien Xiang Tou\n\n\nMay 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSport science\n\n\n\nThis post writes about the phenomenon of reduced blood lactate concentration found with increasing altitude.\n\n\n\nNien Xiang Tou\n\n\nApr 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData visualisation\n\n\n\nThis article visualises how the covid-19 pandemic has evolved in certain countries using animated line plots in R.\n\n\n\nNien Xiang Tou\n\n\nMar 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science\n\n\nFootball\n\n\n\nThis article documents a walk-through on web scraping data on English Premier League teams from Wikipedia and visualising the data using R.\n\n\n\nNien Xiang Tou\n\n\nFeb 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science\n\n\nFootball\n\n\n\nThis post examines and compares the competitiveness of different English Premier LEague seasons based on coefficient of variation metric.\n\n\n\nNien Xiang Tou\n\n\nFeb 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData science\n\n\nFootball\n\n\n\nThis post explores the use of logistic regression to examine whether football possession can predict wins in football with data from EPL 18/19.\n\n\n\nNien Xiang Tou\n\n\nAug 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSport science\n\n\n\nThis post highlights the importance of study perception in the sport science domain, especially in the context of exercise regulation\n\n\n\nNien Xiang Tou\n\n\nAug 3, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nien Xiang Tou",
    "section": "",
    "text": "Read more about me here."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Your-perception-is-your-reality/index.html",
    "href": "posts/Your-perception-is-your-reality/index.html",
    "title": "Your perception is your reality",
    "section": "",
    "text": "We experience a wide range of sensations during exercise. This article highlights the importance of studying perception, especially in the context of regulation of exercise.\nHave you ever stood at the end of a marathon, and witnessed how many people sprinted across the finish line? How is it possible for somebody who slowed down or was walking halfway through the race and yet managed to run the fastest right at the end of the race?\nThis is the end-spurt phenomenon commonly seen in endurance events among elites as well as the recreational athletes. Such phenomenon challenges the concept of fatigue, and this was exactly what sparked my interest in pursuing my PhD study that aims to better understand individual’s perceptual responses in self-regulation of exercise.\nTraditionally, it has been thought that fatigue stems from your physiological limits. From such a perspective, why somebody slows down during a race is completely due to physiological reasons such as depletion of muscle glycogen, lactate, environmental stress or the inability of your heart to supply enough oxygen to the muscles. As a result, the athlete is unable to recruit more muscle fibers to meet the exercise demands and hence slowing down. However, if such hypothesis is true, it is not possible for the athlete to speed up at the end of the race when he or she is supposed to feel the most exhausted."
  },
  {
    "objectID": "posts/Your-perception-is-your-reality/index.html#the-central-governor",
    "href": "posts/Your-perception-is-your-reality/index.html#the-central-governor",
    "title": "Your perception is your reality",
    "section": "The Central Governor",
    "text": "The Central Governor\nThis paradox was first challenged by famous South African sport scientist, Professor Tim Noakes, who also authored the running bible, Lore of Running. In 1996, he delivered the prestigious J. B. Wolffe Memorial Lecture at the American College of Sports Medicine (ACSM) conference. He pointed out that most people did not die running a marathon, which meant they could have ran faster during the race. Therefore, the traditional physiological model was flawed. He later went on to publish a new theoretical model to explain regulation of exercise performance, termed the “Central Governor”, which remains a hot debating topic in the sports science domain to date.\nThis model postulates that exercise performance is regulated by a centralized control center, the brain. Exercise performance is regulated in a manner based on constant feedback between the body and the brain. As we push the limits of our body, the brain generates a whole range of sensations consciously felt by us. The sensations felt during exercise are not the consequences of fatigue, but a protective mechanism to ensure that we complete our exercise in a safe manner. In short, the brain stops us from exercising to the point that we kill ourselves."
  },
  {
    "objectID": "posts/Your-perception-is-your-reality/index.html#why-study-perception",
    "href": "posts/Your-perception-is-your-reality/index.html#why-study-perception",
    "title": "Your perception is your reality",
    "section": "Why study perception?",
    "text": "Why study perception?\nIt is important to understand that in the context of an endurance event such as a marathon, athletes regulate their exercise performance in an anticipatory manner. This means individuals including novice runners naturally try to pace themselves in a sustainable manner to complete the race. Research have empirically shown that athletes slow down way before they reach the critical core temperature, and before the glycogen tank completely empties. Regulation of exercise is based on the perceived sensations during exercise, which help to provide feedback on finding the balance between safety and performance.\nWhy is there a need to study perceptual responses in sports given the tools to monitor a myriad of objective physiological measures such as heart rate, oxygen consumption and blood lactate? This is because when you think you have reached your point of exhaustion during exercise, the truth is you didn’t.\n\nIt is a perceived failure, and not a true failure.\n\nIt is quite impossible to run yourself to true failure in the context of exercise performance. Your true performance probably only happens when you face a life-and-death situation, such as being chased by a lion. This differentiation between a perceived and true failure is very important, as perceptual responses can be influenced by many other factors besides physiological variables.\nThis is the reason why placebo and psychological effects can affect performance. These effects alter your perceptual responses, and hence your performance. In the words of an Italian sport physiologist, Professor Samuele Marcora, effort perception is claimed to be the “cardinal exercise stopper”. Therefore, whatever factors that can influence your perception can ultimately influence your exercise performance.\nYour perception is in fact your reality."
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Nien Xiang Tou",
    "section": "",
    "text": "Please find my research work below.\n\n\nLiu, X., Tou, N. X., Gao, Q., Gwee, X., Wee, S. L., & Ng, T. P. (2022). Frailty and risk of cardiovascular disease and mortality. PloS One, 17(9), e0272527. https://doi.org/10.1371/journal.pone.0272527\nTou, N. X., Wee, S. L., Pang, B. W. J., Lau, L. K., Jabbar, K. A., Seah, W. T., ... & Ng, T. P. (2022). Association of fat mass index versus appendicular lean mass index with physical function–The Yishun Study. Aging and Health Research, 2(3), 100097. https://doi.org/10.1016/j.ahr.2022.100097\nChen, K. K., Lee, S. Y., Pang, B. W. J., Lau, L. K., Jabbar, K. A., Seah, W. T., Tou, N. X., ... & Wee, S. L. (2022). Associations of low handgrip strength and hand laterality with cognitive function and functional mobility–the Yishun Study. BMC Geriatrics, 22(1), 1-8. https://doi.org/10.1186/s12877-022-03363-2\nYu, C. C., Tou, N. X., & Low, J. A. (2022). A comparative study on mental health and adaptability between older and younger adults during the COVID-19 circuit breaker in Singapore. BMC Public Health, 22(1), 1-11. https://doi.org/10.1186/s12889-022-12857-y\nTou, N. X., Wee, S. L., Pang, B. W. J., Lau, L. K., Jabbar, K. A., Seah, W. T., ... & Ng, T. P. (2021). Associations of fat mass and muscle function but not lean mass with cognitive impairment: The Yishun Study. Plos One, 16(8), e0256702. https://doi.org/10.1371/journal.pone.0256702\nTou, N. X., Wee, S. L., Seah, W. T., Ng, D. H. M., Pang, B. W. J., Lau, L. K., & Ng, T. P. (2021). Effectiveness of Community-Delivered Functional Power Training Program for Frail and Pre-frail Community-Dwelling Older Adults: a Randomized Controlled Study. Prevention Science, 22, 1048-1059. https://doi.org/10.1007/s11121-021-01221-y​\nChoo, P. L., Tou, N. X., Pang, B. W. J., Lau, L. K., Jabbar, K. A., Seah, W. T., ... & Wee, S. L. (2021). Timed up and go (tug) reference values and predictive cutoffs for fall risk and disability in singaporean community-dwelling adults: Yishun Cross-Sectional Study and Singapore Longitudinal Aging Study. Journal of the American Medical Directors Association, 22(8), 1640-1645. https://doi.org/10.1016/j.jamda.2021.03.002\n​Tou, N. X., Kee, Y. H., Koh, K. T., Camiré, M., & Chow, J. Y. (2020). Singapore teachers’ attitudes towards the use of information and communication technologies in physical education. European Physical Education Review, 26(2), 481-494. https://doi.org/10.1177/1356336X19869734\n\n\n\nTou, N. X., Balasekaran, G., & Barbosa, T. M. (2021). Influence Of Exercise Experience On Perception Of Prescribed And Preferred Exercise Intensity. Medicine & Science in Sports & Exercise, 53(8S), 10-11. http://doi.org/10.1249/01.mss.0000759152.24965.1c\nTou, N. X., Balasekaran, G., & Barbosa, T. M. (2020). Effects of mental fatigue on maximal exercise test performance in physically active and sedentary adults. Medicine & Science in Sports & Exercise, 52(7S), 626. https://doi.org/10.1249/01.mss.0000681104.26974.97 \nTou, N.X., Balasekaran, G., Thor, D., Tan, I., & Lim, Z. Y. (2017). Validation Of A 10 point OMNI Rating Of Perceived Exertion Colored Scale. Medicine & Science in Sports & Exercise, 49(5S), 754. https://doi.org/10.1249/01.mss.0000519006.11251.49\n\n\n\nTowards Data Science. Combining Python and R for FIFA Football World Ranking Analysis. https://towardsdatascience.com/combining-python-and-r-for-fifa-football-world-ranking-analysis-d71bb6ceacdb"
  },
  {
    "objectID": "posts/Can-possession-predict-wins-in-football/index.html",
    "href": "posts/Can-possession-predict-wins-in-football/index.html",
    "title": "Can possession predict wins in football?",
    "section": "",
    "text": "Ball possession is an important performance indicator in football. This article explores the use of logistic regression model to examine a binary classification problem: whether ball possession predicts match win or not.\nBall possession in football is seen as an imperative aspect of the game. Many successful teams are often characterised by dominating long periods of possession. The first team that sprints to your mind with such nature of play is likely the Spanish giants, Barcelona. They are often defined by their “tiki-taka” style of play, which constitutes short passing and movement to maintain possession.\nPast research have claimed that possession is a critical performance indicator that can differentiate successful teams. Better performing teams tend to maintain significantly longer possession as compared to the less successful teams. In the context of football, it does not matter if you can possess the ball for extended periods but failing to win the match. This leads to an important question, can possession predict wins in football?\nIn view of the imminent English Premier League (EPL) season, let’s revisit last EPL season’s data to answer this question! The data set consists of all results from the total 380 matches played throughout the season. Each row represents the match statistics of a team in a given match. You may see a preview of the data below."
  },
  {
    "objectID": "posts/Can-possession-predict-wins-in-football/index.html#do-better-teams-have-greater-possession",
    "href": "posts/Can-possession-predict-wins-in-football/index.html#do-better-teams-have-greater-possession",
    "title": "Can possession predict wins in football?",
    "section": "Do better teams have greater possession?",
    "text": "Do better teams have greater possession?\nAs mentioned above, it has been claimed that ball possession is a performance indicator that can distinguish the more successful teams. Firstly, let’s visualise the possession of each team ranked by their ball possession!\n\n\n\n\n\nNo surprise that the EPL champions, Manchester City, with the most points tallied also had the most dominating ball possession among all the teams. From the graph, we can see that there is some truth to the claim, with the “top six” teams all ahead in the ball possession rankings as compared to the rest of the league. The highest ball possession recorded was a whopping 82% by Manchester City during match day 29 away to Bournemouth. However, the score did not truly reflect such one-sided dominance, with City edging Bournemouth by just a goal.\n\n\n\nEach data point represents the possession and match outcome of a team. 1 denotes a match win, while 0 denotes otherwise (draw or loss)."
  },
  {
    "objectID": "posts/Can-possession-predict-wins-in-football/index.html#does-ball-possession-translate-into-results",
    "href": "posts/Can-possession-predict-wins-in-football/index.html#does-ball-possession-translate-into-results",
    "title": "Can possession predict wins in football?",
    "section": "Does ball possession translate into results?",
    "text": "Does ball possession translate into results?\nIn the data visualization above, we can see that all results are classified into either wins or otherwise (draw or loss). With so much emphasis on ball possession in today’s football, this scatter plot seems to tell us otherwise, with no clear threshold of possession level that guarantees a win. We can see that a winning team can have close to 20% of possession while a non-winning team can have close to 80% of possession. For example, Crystal Palace pulled off a surprising win (3-2) against Manchester City at the Etihad Stadium on match day 18, with only 21% of ball possession during the match. Therefore, is ball possession a reliable predictor of match wins?"
  },
  {
    "objectID": "posts/Can-possession-predict-wins-in-football/index.html#logistic-regression",
    "href": "posts/Can-possession-predict-wins-in-football/index.html#logistic-regression",
    "title": "Can possession predict wins in football?",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nThis is a classification problem, as we use possession as an independent variable to predict the match result. The problem is simplified to be a binary classification scenario, whereby the result can only have two outcomes: either win or otherwise (i.e. draw or loss). Since the dependent variable is categorical in nature, logistic regression is employed to help us answer our question. In this example, we make use of the StatsModel module, which is an open-sourced Python module encompassing several statistical functions. You may see the summary of the logistic regressions result below.\n\n\nThe results showed that ball possession is a significant predictor of match wins. This suggests that possession is indeed a meaningful indicator of a match outcome. The coefficient of the possession variable is 0.0335, and this implies that when ball possession level increases by 1%, the odds of winning the match increase by 3.41%. Greater ball possession could translate to more creation of goal-scoring opportunities. Possessing the ball more also means reducing opponent’s chances of scoring since a team will need to have the ball first before they can even try to score. Such situations could contribute to a greater likelihood of winning the match."
  },
  {
    "objectID": "posts/Can-possession-predict-wins-in-football/index.html#how-accurate-is-this-model",
    "href": "posts/Can-possession-predict-wins-in-football/index.html#how-accurate-is-this-model",
    "title": "Can possession predict wins in football?",
    "section": "How accurate is this model?",
    "text": "How accurate is this model?\nIn this example, the logistic regression model was trained with the data from all matches in EPL season 18/19. We could deploy the model to predict a match outcome given an input amount of possession levels. The model predicts by calculating the probability of either outcomes (value between 0 and 1). In this example, if a given possession level has a probability of winning greater than 0.5, it will be predicted that the team wins the match and vice versa.\nIn the field of machine learning and data science, the accuracy rate of a classification model can often be interpreted through a confusion matrix. A confusion matrix is a simple table that shows the accuracy of the model by comparing the predicted values with the actual values in the data set. You may see the confusion matrix for this example below.\n\n\n\nConfusion matrix of logistic regression model. Note- 1 denotes win, 0 denotes otherwise (draw or loss).\n\n\nThe confusion matrix compares all 760 predicted outcomes with the actual recorded outcomes. From the matrix, we can see that 372 cases were correctly predicted to be non-wins, also termed the true negatives. 204 cases were incorrectly predicted to be non-wins when they actually produced match wins in reality. Such cases are termed false negatives. 79 cases were also incorrectly predicted to be winning outcomes when they were actually non-wins, also known as the false positives. Lastly, 105 cases were correctly predicted to be classified as match wins, which are regarded as the true positives.\n\nAccuracy rate = (372+105)/760 * 100\n\nThis model was found to be 62.76% accurate! I will leave it to your personal judgement whether this model is acceptable. Obviously, football is a game with so many variables that can influence the outcome of a match. Therefore, it is not surprising to expect such accuracy of a simple model with a single predictor variable. Nevertheless, this post showcases that ball possession is a significant variable that can help us predict match outcomes. However, possession itself does not tell the full story."
  },
  {
    "objectID": "posts/Is-the-EPL-getting-more-competitive/index.html",
    "href": "posts/Is-the-EPL-getting-more-competitive/index.html",
    "title": "Is the English Premier League getting more competitive?",
    "section": "",
    "text": "Is the English Premier League more or less competitive as compared to a decade ago? This article examines and compares the competitiveness of different seasons based on the coefficient of variation metric.\nPresently, Liverpool is en route to being runaway champions in the English Premier League (EPL), with 22 points separating them from second-placed Manchester City. Incredibly, the premier league leaders have gone a total of 42 games unbeaten at the moment, with their last defeat happening at the Ethihad Stadium on 3rd January 2019.\nInevitably, this invites a comparison between Liverpool and the “Invincible” Arsenal team. The latter currently holds the record for the longest unbeaten EPL run with 49 games in 2003/2004. To date, the record has stood for close to 16 years. Can Liverpool surpass this incredible record? In an online article, former Arsenal player Robert Pires commented that if Liverpool goes on to break the record, the league was much harder in the past as compared to present.\nIs he right to say that? This article examines whether the present EPL is indeed getting easier as compared to many years ago. First, let’s visualise how many points were required to become champions across the different past seasons since 2000/2001.\nBased on the figure, we can clearly see that teams need at least 80 points to win the titles. The “Invincible” Arsenal team achieved a total of 90 points in 2003/2004 to lift the championship trophy. However, it is interesting to point out that 90 points would be insufficient to win the league over the past three seasons. Liverpool fans would be especially frustrated that despite accumulating 97 points in 2018/2019 season, they were still that tiny bit shy of the title. Hence, is the league really becoming less competitive like what Robert Pires said?"
  },
  {
    "objectID": "posts/Is-the-EPL-getting-more-competitive/index.html#coefficient-of-variation",
    "href": "posts/Is-the-EPL-getting-more-competitive/index.html#coefficient-of-variation",
    "title": "Is the English Premier League getting more competitive?",
    "section": "Coefficient of variation",
    "text": "Coefficient of variation\nWith the aim to evaluate the competitiveness of the league, we will use the coefficient of variation (CV) metric. CV is calculated by dividing the standard deviation of a dataset by its mean. It simply represents the relative spread/variance of the data, and ranges between 0 to 1. In finance, CV is used to assess the volatility of stocks. In this article’s context, we compare the competitiveness of different EPL seasons based on each season’s CV. If a league is more competitive, we will expect that the point total of teams to be closer to one another, hence a lower CV. On the other hand, a higher CV represents less competitiveness as there is greater spread in point total among teams.\n\nThe figure above illustrates the CV across different seasons. Generally, we see there’s relatively greater variation in more recent seasons as compared to the past. For example, there’s 29% variation in 2003/2004 season as compared to 39% variation in 2018/2019. Based on the graph, the most competitive season was probably in 2010/2011 with the least variation of 25%. This suggests that there’s some truth in Robert Pires’s comments.\n\nNext, let’s further analyse the competitiveness of the league based on different standings of the table. The figure above illustrates the CV of the top six teams across different seasons. This tells a slightly different story as compared to the previous graph. The higher CVs were observed in 2003/2004 and 2004/2005 seasons, which suggests greater dispersion in point total among the top teams. This implies relatively less of a close battle for the title in these two seasons. In contrast, we see a clear trend of decreasing CV from season 2005/2006 onwards till about 2015/2016. This coincides with the shift of the traditional “Top Four” (Arsenal, Manchester United, Chelsea, Liverpool) dominance to the emergence of the “Big Six” with Tottenham Hotspur and Manchester City joining in to challenge for the title.\nAlthough there’s an increase in CV among the top six teams in recent seasons, it is still comparatively less than the “Invincible” season. This suggests that it is much more competitive among the top teams in recent seasons as compared to the past. This also indicates that challenging for a champions league qualifying spot is much tougher now than two decades ago.\nIn conclusion, while the league may be more competitive overall in the past, there is less competition among the top teams when comes to challenging for the title or qualifying for the champions league as compared to present seasons.\nDo you agree with Robert Pires’s comments?"
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "",
    "text": "Web scraping opens doors to a myriad of data sources online. Be it text or numbers, web scraping allows us to extract online data into spreadsheets in our computer. With reference to code from FC RSTATS, this article documents a walk through on how to scrape football data from the web and visualising the data using open-source programming platform, R."
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#required-libraries",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#required-libraries",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "1. Required Libraries",
    "text": "1. Required Libraries\nlibrary(rvest)\nlibrary(ggplot2)\nlibrary(plyr)\nlibrary(ggrepel)\nThe above are the required libraries in this walk-through. The two main libraries are rvest and ggplot2, which are used to perform web scraping and plotting graphs respectively.The plyr library helps us in data cleaning while the ggrepel library helps to avoid overlapping of labels in the graphs."
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#webpage",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#webpage",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "2. Webpage",
    "text": "2. Webpage\nurl <- \"https://en.wikipedia.org/wiki/2019%E2%80%9320_Premier_League\"\nIn this article, we are scraping data from a Wikipedia page, showing information on the present English Premier League. You may access the page through the url link specified above. Specifically, I am interested in extracting information on the goals scored (GF) and goals conceded (GA) of each team from the table below.\n\n\n\nScreenshot of the table in which data would be scraped"
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#scrape-the-data",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#scrape-the-data",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "3. Scrape the data",
    "text": "3. Scrape the data\nscraped_page <- read_html(url)\n\n#Scraping the Team Column\nTeams <- scraped_page %>%  \n    html_nodes(\"h2+ .wikitable th+ td\") %>%  \n    html_text() %>%  \n    as.character()\n\n#Scraping the GF column\nGoals_for <- scraped_page %>%\n  html_nodes(\"h2+ .wikitable td:nth-child(7) , th:nth-child(7) abbr\") %>%\n  html_text() %>%\n  as.numeric\n\n#Scraping the GA column\nGoals_against<- scraped_page %>%\n  html_nodes(\"h2+ .wikitable td:nth-child(8)\") %>%\n  html_text() %>%\n  as.numeric\nFirst, we read the web page and save the data to the variable named scraped_page. Based on the above table, we are interested in three columns: Team, GF and GA. Each column is scraped separately and saved to a variable name (i.e. Teams, Goals_for, Goals_against). The codes look identical to one another since we are just repeating the scraping process. The difference is to specify the location of the data to be scraped. The respective locations for each location are specified in red above. For example, “h2+ .wikitable th+ td” tells the code to locate the team column in the scraped page. Each location can be easily found through the use of SelectorGadget. Click the link to see more details."
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#cleaning-the-data",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#cleaning-the-data",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "4. Cleaning the data",
    "text": "4. Cleaning the data\n#Removing \"\\n\" in the teams \nTeams <- gsub(\"\\n\",\"\",Teams)\n\n#Remove NA case\nGoals_for <- na.omit(Goals_for)\n\n#Combining all scraped data into a dataframe\ndf <- data.frame(Teams, Goals_for, Goals_against)\n\n#Renaming the values of Liverpool and Manchester City \ndf$Teams <- mapvalues(df$Teams, from=c(\"Liverpool (Q)\",\"Manchester City[a]\"), to=c(\"Liverpool\", \"Manchester City\"))\nIf you preview each of the variable, you will find some slight discrepancies. For example, many of the teams have a ‘\\n’ in their names. Hence, the above code helps to clean the data and combines them into a nice spreadsheet below."
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#data-visualisation",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#data-visualisation",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "5. Data Visualisation",
    "text": "5. Data Visualisation\nFollowing the code from FC RSTATS, we can create a scatter plot as below. Each point represents each team in the league. The x and y axes represent the goals scored and goals conceded respectively. The plot is divided into quadrants by the two dotted lines. The horizontal dotted line represents the average goals conceded while the vertical horizontal dotted line represents the average goals scored.\nBased on this simple graph, we can see that the teams located on the bottom right are the better performing teams with both strong attack and defence. Conversely, the teams in the top left quadrant have the worst performance."
  },
  {
    "objectID": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#dividing-teams-into-four-clusters",
    "href": "posts/web-scraping-and-data-visualising-with-R-EPL19/20-goal-stats/index.html#dividing-teams-into-four-clusters",
    "title": "Web Scraping and Data Visualising with R: EPL 19/20 Goal Stats",
    "section": "6. Dividing teams into four clusters",
    "text": "6. Dividing teams into four clusters\nTo better visualise the separation in performance, we can represent each quadrant in different colours. First, we create a new column in the spreadsheet and name it cluster. The code below will separate each team into their respective clusters based on their goal stats.\n#Creating a cluster column\ndf$Cluster = 0\n\n#Using for loop and ifelse statements to create different clusters \nfor(i in 1: nrow(df)) {\n  if (df$Goals_for[i] > mean(df$Goals_for) & df$Goals_against[i] < mean(df$Goals_against)) {\n    df$Cluster[i] = 1\n  } else if (df$Goals_for[i] < mean(df$Goals_for) & df$Goals_against[i] < mean(df$Goals_against)) {\n    df$Cluster[i] = 2 \n  } else if (df$Goals_for[i] > mean(df$Goals_for) & df$Goals_against[i] > mean(df$Goals_against)) {\n    df$Cluster[i] = 3\n  } else {\n    df$Cluster[i] = 4\n  }\n}\n\n#Making the cluster columns a factor\ndf$Cluster = factor(df$Cluster, levels = c(1, 2, 3, 4), \n                    labels = \n                    c(\"Strong attack\\nStrong defence\",\n                    \"Poor attack\\nStrong defence\",\n                    \"Strong attack\\nPoor defence\",\n                    \"Poor attack\\nPoor defence\"))\n                    \nNext, we will use the ggplot2 library to visualise our data. The comments in green help to explain each respective line of code.\n#Specifying the variables and using colours to represent each cluster\nggplot(df, aes(x = Goals_for, y = Goals_against, label = Teams, colour = Cluster))+\n  #Plotting each team on the graph using a geometric point\n  geom_point()+\n  #Plotting the vertical dotted line to represent the average goals scored\n  geom_vline(xintercept=mean(Goals_for), linetype=\"dashed\", \n  alpha = 0.4, colour = \"red\") +\n  #Plotting the horizontal dotted line to represent the average goals conceded\n  geom_hline(yintercept=mean(Goals_against), linetype=\"dashed\", \n  alpha = 0.4, colour = \"red\") +\n  #Geom_text_repel helps to avoid overlapping of labels in the points\n  geom_text_repel(aes(Goals_for, Goals_against, label = Teams), \n                  size = 2, colour = \"black\", fontface = \"bold\")+\n  #Labelling of axes and title\n  labs(title = \"EPL 2019/2020: Goals For vs Goals Against\",\n       x = \"Goals For\", y = \"Goals Against\",\n       colour = \" \")+\n  #Specifying position of the legend\n  theme(legend.position = \"top\")\n\n#Save the plot in your working directory\nggsave(\"output.jpeg\", dpi = 300)\n                    \nThe resulting plot is below.\n\n\n\n\n\nFull code for this article can be accessed through my github.\nPS: If you actually count the number of points on the plot, there are only 19 instead of 20. This is because Newcastle and Watford happen to have exactly the same goal stats at the time of writing."
  },
  {
    "objectID": "posts/Animated-data-viz-covid19/index.html",
    "href": "posts/Animated-data-viz-covid19/index.html",
    "title": "Animated Data Visualisation of the Covid-19 Pandemic using R",
    "section": "",
    "text": "Number of infected cases and deaths around the world continues to rise daily. This article visualises how the pandemic has evolved in certain countries using animated line plots on R."
  },
  {
    "objectID": "posts/Animated-data-viz-covid19/index.html#china-breakout",
    "href": "posts/Animated-data-viz-covid19/index.html#china-breakout",
    "title": "Animated Data Visualisation of the Covid-19 Pandemic using R",
    "section": "China Breakout",
    "text": "China Breakout\nWhilethe true origin of the virus remains debatable, the very first reported covid-19 case was detected in Wuhan City, Hubei Province of China. Since then, the numbers have skyrocketed. Using the numbers reported in the daily situation reports by World Health Organization (WHO), let’s visualise how the outbreak has developed since 1st February 2020 using gganimate on R.\n\n\n\n\n\nThe figure above illustrates how the number of cases increased day by day. The exact dates are reflected in the subtitle. Over the period of almost two months, the number of confirmed cases has increased from 11821 to 81601 at the point of writing. From the graph, we can see a sharp increase in cases on 14th February. It can also be seen that the rate of increase seems to decline since March."
  },
  {
    "objectID": "posts/Animated-data-viz-covid19/index.html#southeast-asia",
    "href": "posts/Animated-data-viz-covid19/index.html#southeast-asia",
    "title": "Animated Data Visualisation of the Covid-19 Pandemic using R",
    "section": "Southeast Asia",
    "text": "Southeast Asia\nNext, let’s visualise how the virus has developed closer to home. The figure below illustrates the breakout in certain Southeast Asian countries.\n\n\n\n\n\nAt the start of February, Singapore was leading the charts over its neighbours. Things have changed very quickly since early March. The number of reported cases in Malaysia surpassed Singapore on 15th March. Presently, the pandemic is faring much worse in many other Southeast Asian countries as compared to Singapore."
  },
  {
    "objectID": "posts/Animated-data-viz-covid19/index.html#rest-of-the-world",
    "href": "posts/Animated-data-viz-covid19/index.html#rest-of-the-world",
    "title": "Animated Data Visualisation of the Covid-19 Pandemic using R",
    "section": "Rest of the World",
    "text": "Rest of the World\nCovid-19 was announced a pandemic on 11th March 2020 given the alarming levels of the virus outbreak. In such a connected world today, it was hard for any countries to get off the hook. Let’s take a look at some other notable countries being hit.\n\n\n\n\n\nThe graph clearly shows the trajectories of the virus spread differ between countries. For example, there was a sharp increase in number of cases in South Korea during the second half of February. The rate of increase quickly slowed in March. On the other hand, Italy is still facing an exponential manner of spread at the moment. Based on the figures, we can infer the extent of success in countries’ attempt to contain the virus.\nData visualisation code and dataset can be found on my github."
  },
  {
    "objectID": "posts/lactate-paradox/index.html",
    "href": "posts/lactate-paradox/index.html",
    "title": "Lactate Paradox",
    "section": "",
    "text": "Blood lactate is associated with fatigue during maximal exercise. This post writes about the phenomenon of reduced blood lactate concentration found with increasing altitude.\nEnergy production during physical work comes from three energy systems: 1) adenosine triphosphate creatine phosphate (ATP-CP), 2) anaerobic glycolysis, and 3) aerobic system. All three systems work concurrently to produce energy, but dominance of any one system is dependent on exercise intensity and duration. Supply of energy for muscle contraction during endurance exercise primarily comes from either aerobic or anaerobic systems. The key difference between the two systems is that aerobic system requires oxygen in the production of energy but anaerobic system does not.\nIt has been traditionally believed that endurance exercise performance is limited by one’s maximal aerobic capacity (VO2max). Exercising beyond this capacity will shift the energy supply to become more reliant on the anaerobic energy system instead. Consequently, low oxygenation in muscles leads to accumulation of blood lactate, or more commonly known as lactic acid. In a classic study, high levels of blood lactate was found in frog muscles that were stimulated to contract to failure. This has led to the hypothesis that blood lactate is associated with physical fatigue."
  },
  {
    "objectID": "posts/lactate-paradox/index.html#exercising-in-altitude",
    "href": "posts/lactate-paradox/index.html#exercising-in-altitude",
    "title": "Lactate Paradox",
    "section": "Exercising in Altitude",
    "text": "Exercising in Altitude\nIf you have been in the mountains, you will notice that breathing gets more difficult with advancing altitude levels. Naturally, exercise performance also becomes more exhaustive under such conditions. Since blood lactate is associated with anaerobic energy pathways, higher lactate concentrations are expected at high altitude, whereby oxygen levels are lower. Indeed, this is so among individuals who first experienced altitude. However, peak blood lactate concentration was found to decrease with increasing altitude among individuals who were acclimatised to altitude.\nThe phenomenon of lower peak blood lactate concentration under greater hypoxic conditions at high altitude levels is puzzling to physiologists. This phenomenon is termed the “lactate paradox”. The paradox was confirmed in Operation Everest II, a study that examined acclimatisation effects of participants at altitude levels equivalent to the summit of Mount Everest, approximately 8848 metres high. It was discovered that the blood lactate concentrations among acclimatised subjects during maximal exercise at high altitude level were not significantly different as compared to during rest at sea levels.\nTo date, the lactate paradox remains poorly understood. Nevertheless, this phenomenon suggests that blood lactate is not necessarily associated with physical fatigue. Hence, the theoretical belief that endurance exercise performance is limited by accumulation of blood lactate in skeletal muscles is unlikely correct."
  },
  {
    "objectID": "posts/web-scraping-goodreads/index.html",
    "href": "posts/web-scraping-goodreads/index.html",
    "title": "Web Scraping Goodreads with BeautifulSoup: Popular runnning books",
    "section": "",
    "text": "Data is incredibly important to every analyst or data scientist. Web scraping is a valuable skill that allows us to access the limitless sources of data online. In my previous blogpost, I have documented the use of R to scrape football data from a wikipedia page. This post presents my attempt on scraping information of popular running books from Goodreads using Python programming language."
  },
  {
    "objectID": "posts/web-scraping-goodreads/index.html#required-tools",
    "href": "posts/web-scraping-goodreads/index.html#required-tools",
    "title": "Web Scraping Goodreads with BeautifulSoup: Popular runnning books",
    "section": "Required Tools",
    "text": "Required Tools\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\nimport pandas as pd\nWe will require four library packages for this project. The request package helps with making HTTP requests from the website of interest. The BeautifulSoup package is a popular web scrapping python library and this helps us to perform the main work. The urllib.parse helps us to join URL addresses and the pandas library helps us to tidy the data scraped into a nice data frame."
  },
  {
    "objectID": "posts/web-scraping-goodreads/index.html#website",
    "href": "posts/web-scraping-goodreads/index.html#website",
    "title": "Web Scraping Goodreads with BeautifulSoup: Popular runnning books",
    "section": "Website",
    "text": "Website\nThis page on Goodreads presents the top 50 books related to running (see below). It showcases some information of each book such as the title, author names and ratings. However, if you want to read more detailed description of each book, you will have to click on the link of the book title.\n\n\n\nScreenshot of website"
  },
  {
    "objectID": "posts/web-scraping-goodreads/index.html#set-up",
    "href": "posts/web-scraping-goodreads/index.html#set-up",
    "title": "Web Scraping Goodreads with BeautifulSoup: Popular runnning books",
    "section": "Set up",
    "text": "Set up\nIf you right click on most web pages, you may inspect its html codes. The codes are structured with several tags, classes and attributes that serve different purposes. Generally, web scraping locates the data that you are interested to extract based on information from these codes. The codes below help us to extract the website’s html and also create a BeautifulSoup object that we will further wrangle with. You may also examine the html file exported.\n# Specifying website url\nbase_site = \"https://www.goodreads.com/shelf/show/running\" \n\n# Make http request\nresponse = requests.get(base_site)\n\n# Get the html from webpage\nhtml = response.content\n\n# Creating a BeautifulSoup object with the use of a parser\nsoup = BeautifulSoup(html, \"lxml\")\n\n# Exporting html file\nwith open('popularrunningbooks.html', 'wb') as file:\n    file.write(soup.prettify('utf-8'))\nThe aim of this web scraping project was to extract relevant information regarding each of these 50 books: 1) book title, 2) author name(s), 3) book rating, 4) book pages, 5) book description. The general workflow to retrieve these information follows the same steps as if we were to manually do it. This involves us clicking on each of the book links and extract the data of interest. Hence, the very first step to help us automate this process is to extract this list of book links from the BeautifulSoup object we created earlier.\nHtml codes are generally built within many layers, similar to putting a present in several layers of gift boxes. Therefore, scraping data is akin to unwrapping the present layer by layer. Typically, a website’s content is hidden under the ‘div’ tag, which represents the outermost layer of the box. Hence, this is usually the starting point to unwrap our “present”. We could also specify the class and ID to help us better locate the data that we want. In this case, the book links are within the class “elementList”.\n# First layer: The element that contains all the data\ndivs = soup.find_all(\"div\", {\"class\": \"elementList\"})\n\n# Second layer: Extracting html tags that contain the links\nlinks = [div.find('a') for div in divs]\nThe url information of each book links are located in the links. However, each of the url extracted is only a partial web address. For example, the corresponding partial url link for the book “Born to Run” looks like ‘/book/show/6289283-born-to-run’. In order to get the full url, we will use the urljoin method from the urllib.parse package to join our base site web address with each of these partial url links.\n# Extracting the partial links  \nrelative_url = [link['href'] for link in links]  \n\n# Computing the full url addresses \nfull_url = [urljoin(base_site, relativeurl) for relativeurl in relative_url]\nIf you inspect the full_url list , some unnecessary non-book links were accidentally extracted as well. Hence, the code below will help to overcome this problem.\n# Filter only the book links\nbook_url = [url for url in full_url if \"https://www.goodreads.com/book/show\" in url]"
  },
  {
    "objectID": "posts/web-scraping-goodreads/index.html#scraping-information-of-each-book",
    "href": "posts/web-scraping-goodreads/index.html#scraping-information-of-each-book",
    "title": "Web Scraping Goodreads with BeautifulSoup: Popular runnning books",
    "section": "Scraping information of each book",
    "text": "Scraping information of each book\nFinally, we have arrived at the main web scraping work. Imagine clicking on each of the book links and retrieve the data we need. Programming helps us to automate this process. First, we create five empty lists, whereby each list will store its respective information.\nbook_description = []\nbook_author = []\nbook_title = []\nbook_rating = []\nbook_pages = []\nThe scraping process involves some similar steps stated earlier, whereby we have to retrieve the html code of each book link and locate the information we need. The same steps will be repeated for every link. The for-loop below helps us to perform this repetitive work.\n#creating a loop counter\ni = 0\n\n#Loop through all 50 books\nfor url in book_url:\n    \n    #connect to url page\n    note_resp = requests.get(url)\n    \n    #checking if the request is successful\n    if note_resp.status_code == 200:\n        print(\"URL{}: {}\".format(i+1, url))\n    \n    else:\n        print('Status code{}: Skipping URL #{}: {}'.format(note_resp.status_code, i+1, url))\n        i = i+1\n        continue\n    \n    #get HTML from url page\n    note_html = note_resp.content\n    \n    #create beautifulsoup object for url page\n    note_soup = BeautifulSoup(note_html, 'html.parser')\n    \n    #Extract Author particulars\n    author_divs = note_soup.find_all(\"div\",{\"class\":\"authorName__container\"})\n    author_text = author_divs[0].find_all('a')[0].text\n    book_author.append(author_text)\n    \n    #Extract title particulars\n    title_divs = note_soup.find_all(\"div\", {\"class\": \"last col\"})\n    title_text = title_divs[0].find_all('h1')[0].text\n    book_title.append(title_text)\n    \n    #Extract rating particulars\n    rating_divs = note_soup.find_all(\"div\", {\"class\": \"uitext stacked\", \"id\": \"bookMeta\"})\n    rating_text = rating_divs[0].find_all(\"span\", {\"itemprop\": \"ratingValue\"})[0].text\n    book_rating.append(rating_text)\n    \n    #Extracting page particulars\n    page_divs = note_soup.find_all(\"div\", {\"class\": \"row\"})\n    try:\n        page_text = page_divs[0].find_all(\"span\", {\"itemprop\": \"numberOfPages\"})[0].text.strip('pages')\n    except IndexError:\n        page_text = 0\n    book_pages.append(page_text)\n    \n    #Extracting description particulars\n    description_divs = note_soup.find_all(\"div\", {\"class\": \"readable stacked\", \"id\": \"description\"})\n    try:\n        description_text = description_divs[0].find_all(\"span\")[1].text\n    except IndexError:\n        try:\n            description_text = description_divs[0].find_all(\"span\")[0].text\n        except IndexError:\n            description_text = \"Nil\"\n    book_description.append(description_text)\n        \n    #Incremeting the loop counter\n    i = i+1\nIt will take a couple of minutes to scrape through all 50 links. Most of the raw data look messy, and hence require some cleaning up. After some tidying, we can use the pandas package to organise all the data into a data frame (see below).\n\nYou may also sort the data frame based on its ratings using the sort_values method. That will inform us that the highest rated book is “The Rise of the Ultra Runners: A Journey to the Edge of Human Endurance” by Adharanand Finn with an average 4.45 rating. Finally, we can export all these data into a nice csv file for ease of viewing on Excel, using the to_csv method.\n# Export dataframe into csv file\nsorted_book_df.to_csv(\"top running books.csv\")\nHope you enjoy this blog post and full code can be found here. Similar codes can be used to scrape other book lists on Goodreads. For my running friends, you may check out the final csv file over here."
  },
  {
    "objectID": "posts/Pace-variation-in-marathon-running/index.html",
    "href": "posts/Pace-variation-in-marathon-running/index.html",
    "title": "Pace variation in Marathon running: Analysing data from Standard Chartered Singapore Marathon 2019",
    "section": "",
    "text": "Pacing strategy is an important determinant of performance, especially in endurance events such as the marathon. Does pacing profile differ across runners of different levels? This blog post examines the pace variation of Singaporean male and female runners in the Standard Chartered Singapore Marathon (SCSM) 2019."
  },
  {
    "objectID": "posts/Pace-variation-in-marathon-running/index.html#art-of-pacing",
    "href": "posts/Pace-variation-in-marathon-running/index.html#art-of-pacing",
    "title": "Pace variation in Marathon running: Analysing data from Standard Chartered Singapore Marathon 2019",
    "section": "Art of Pacing",
    "text": "Art of Pacing\nPacing strategy in long-distance running refers to the distribution of running speed across the whole duration of event. Finding an optimised pacing strategy in a marathon event is no easy feat, as the distance often poses many uncertainties and unexpected challenges. Too conservative start may lead to a sub-optimal finish, while an aggressive start may result in early fatigue and excessive slowing down during latter stages of the race.\nGenerally, there are three types of pacing profiles: 1) negative, 2) positive, and 3) even pacing strategy. First, negative pacing refers to an increase in speed over the duration of event. This will mean running the second half of the race faster than the first half. Second, positive pacing is characterised by a decline in speed instead. Third, even pacing represents keeping relatively the same speed throughout the event. Such even pacing was observed in Eliud Kipchoge’s shattering of the 2 hour marathon barrier last year. If you look at his time splits, he and his pacing team ran each 5km within a range of 14:10 - 14:14. This suggests that adopting an even pacing profile in the marathon may be the most ideal strategy to hit your race goals.\nRealistically, this is a tall order and gets increasingly harder to achieve as the race distance increases. Very often, we see drastic changes in running speed during endurance events. For example, we see more race participants walking in the second half of the marathon as compared to the first half. This is an expected phenomenon since fatigue kicks in as the race progresses. However, is such variation in pace similar across all levels of runners?\nWith the aim to answer this question, I scraped the time splits of all Singaporean runners who finished the full marathon in last year’s SCSM. The web scraping was performed using the pandas and BeautifulSoup package in python. You may find the full web scraping code here.\n\n\n\n\n\nBased on the official results from the race website, a total of 3929 Singaporean men and 911 women completed the full marathon. Clearly, the marathon is not as popular among females than their male counterparts. Data on each 5km time split was retrieved for each runner. However, several runners were found with missing data for certain distances. Hence, for runners with missing time splits, it was assumed that their pace remained the same as the last recorded time split. Runners with missing data for the first 5km and 10km were considered invalid and removed from the dataset. Thus, I ended up with a total of 3886 men and 902 women for the analysis.\nThe coefficient of variation (CV) was used to measure the variation in each runner’s pace throughout the race. This was simply computed by dividing the standard deviation of all time splits by the mean speed. Higher CVs represent greater variation in pace and lower CVs indicate more consistency in running speed throughout the race. The CV was compared among three groups of runners in both gender groups: fast, mid-pack and slow. The fast group represents the top 20%, the mid-pack represents the middle 20%, and the slow group represents the bottom 20%. The notched box plots above illustrate the average running speed distribution of each group separated by genders. Evidently, the speeds differed significantly across groups for both genders.\n\n\n\n\n\nThe graph above presents the average CV scores in percentages of respective groups for both genders. We can see a trend that pace variation increases with slower marathon running times. Using a statistical test (one-way ANOVA) informed us that the differences in pace variation across groups were significant. The top 20% Singaporean male runners had an average of 12.11% in their marathon pace variation, and this was significantly less than the mid-pack (14.93%) and the slow group (15.59%). However, the mid-pack and slow groups showed comparably similar variation in pace. Similarly, for the females, the fast group (8.14%) showed significantly less variation in their pace than their slower counterparts. The mid-pack group (12.8%) also differed significantly as compared to the slow group (15.32%)."
  },
  {
    "objectID": "posts/Pace-variation-in-marathon-running/index.html#importance-of-consistency-in-pacing",
    "href": "posts/Pace-variation-in-marathon-running/index.html#importance-of-consistency-in-pacing",
    "title": "Pace variation in Marathon running: Analysing data from Standard Chartered Singapore Marathon 2019",
    "section": "Importance of consistency in pacing",
    "text": "Importance of consistency in pacing\nThese numbers highlighted that better performing runners were not only faster in their speed, they also exhibited considerably less variation in their marathon running pace. Obviously, many factors can influence a marathon performance. One aspect is definitely optimising your pacing strategy during the race. This means selecting a pace that you can sustain consistently with minimal variation throughout the race. This is an important takeaway for all levels of runners. Hence, do practise your pacing strategy during training, and carefully plan your pace for race day. It will probably make a difference!\nData processing, analysis and visualisation were performed on R. Full code and datasets can be found here."
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "",
    "text": "Contact tracing plays a key role in infectious disease management, and related cases form a cluster. Since the coronavirus outbreak, several clusters have formed all over Singapore. This blog post documents my attempt to visualise the clusters on the Singapore map using R programming language."
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#required-tools",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#required-tools",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "Required Tools",
    "text": "Required Tools\nlibrary(rvest)\nlibrary(ggplot2)\nlibrary(ggmap)\nThree libraries are required for this project. rvest does the web scraping and ggplot2 is required for graph plotting. More importantly, ggmap helps us to conveniently locate the coordinates of each cluster address with the geocode function."
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#google-maps-api",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#google-maps-api",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "Google Maps API",
    "text": "Google Maps API\nregister_google(key = \"insert your API key here\")\nFirst, we need to register an authorised Google API key in order to use the geocode function. You may register for your free API key over here."
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#cluster-list",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#cluster-list",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "Cluster List",
    "text": "Cluster List\nThe list of clusters used in this blog post was from the Covid-19 Singapore Dashboard created by Upcode Academy. The dashboard displays several visualisations on data related to the pandemic outbreak in Singapore. On the bottom right of the page, we could see the clusters and their respective number of cases. Data on the cluster list was scraped through the use of rvest package. The web scraping process is similar to the code I have documented in my previous blogpost.\n\n\n\nOrganising the clusters into a data frame\n\n\nA total of 47 clusters was scraped from the dashboard and organised into a dataframe. The figure above presents the name of each cluster and its respective number of cases. The list includes clusters that were identified early on during the outbreak (e.g. Yong Thai Hang medical shop, Grand Hyatt Hotel) as well as the foreign worker dormitories, where numbers have skyrocketed during the last 2 months (e.g. S11 Dormitory). Unfortunately, the numbers for certain clusters are not updated (especially for the dormitories). Nevertheless, it contains majority of the local clusters identified so far.\nIn order to plot each cluster on the map, we require the coordinates of each cluster address. Google identifies each address by its latitude and longitude position, which we can conveniently retrieve via the geocode function. Before doing this, we had to tidy up some of these cluster names in order to better locate them. For example, specifying “Grand Hyatt Hotel Singapore” instead of “Grand Hyatt Hotel” returns the local coordinates instead of a foreign location. Editing of all cluster names can be found in the full code.\nA for-loop was then used to extract the coordinate positions for each cluster and appended to the data frame.\n# Computing latitude and longitude of each location\nfor (i in 1:nrow(cluster_df)) {\n  cluster_name = cluster_df[i, c(\"clusters\")]\n  cluster_geocode = geocode(cluster_name)\n  cluster_df[i, c(\"lon\")] <- cluster_geocode$lon\n  cluster_df[i, c(\"lat\")] <- cluster_geocode$lat\n}"
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#singapore-map",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#singapore-map",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "Singapore Map",
    "text": "Singapore Map\n# Locating the latitude and longitude position of Singapore\n# Lon = 103.8198, Lat = 1.352083\nsingapore_city <- geocode(\"Singapore\")\n\n# Retrieves the map image of the location\nsingapore_map <- get_map(singapore_city, zoom = 12,  maptype = \"roadmap\")\n\n# Visualising the map \nggmap(singapore_map)\nThe map visualisation encompasses plotting each cluster on the map. Hence, we need to prepare the background map first. The coordinates of the map were retrieved through the geocode function. Next, the get_map function helps to get the image of the map. We could specify the amount of zoom as well as the map type. There are several map types, and “roadmap” represents the typical google map we see. Lastly, ggmap function helps us to visualise the map (see below).\n\n\n\nVisualisation of Singapore map using ggmap"
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#visualising-the-clusters",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#visualising-the-clusters",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "Visualising the clusters",
    "text": "Visualising the clusters\n# Map the clusters on singapore map\nggmap(singapore_map, \n      base_layer = ggplot(data = cluster_df,\n                          aes(x=lon, y = lat, color = cases))) +\n  geom_point(size = 1.5,\n             alpha = 0.8)+\n  labs(title = \"Covid-19 Clusters in Singapore\",\n       caption = \"Data from Covid-19 Singapore Dashboard\",\n       color = \"Number of cases\")+\n  scale_colour_gradient(low = \"blue\", high = \"red\", na.value = NA)+\n  theme_void()\nNext, visualisation of the clusters was done by simply adding the scatterplot layer to the map layer. Specifying the color to be dependent on the number of cases helps to differentiate the magnitude of the clusters. You may set the gradient colours using scale_colour_gradient. In this case, most clusters are relatively small (< 50) except the foreign worker dormitories. Unfortunately, as mentioned earlier, the numbers for the dormitories are not updated. You may see the final plot below. Do take note that certain clusters are missing from the plot due to the zoom size of the map selected.\n\n\n\nMap visualisation of Covid-19 clusters in Singapore using ggmap.\n\n\nFull code for this article can be accessed through my github."
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#expected-goals",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#expected-goals",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "Expected Goals",
    "text": "Expected Goals\nEvaluation of a team’s underlying performance at goal is made possible with the expected goals (xG) metric. The metric defines the probability of a shot being scored based on multiple factors such as angle, distance and type of assist. This is well-explained with various examples in the video link here by Opta, a football data analytics company. Such measure provides us with further insights beyond the match statistics that are typically shown.\nFootball history has demonstrated that goals could be scored by players of different positions, including the goalkeeper. Some came from clear goal-scoring chances, while some were scored from very unexpected situations. For example, Asmir Begovic, goalkeeper of Stoke City, scored from his own penalty box in 2013. The xG metric would have informed us that the attempt had very low odds of hitting the back of the net. It is reasonable to consider that such abnormal and unexpected goals were just teams being lucky.\nTheoretically, the team with better goal-scoring opportunities should register more goals. However, this is not the case in reality due to various factors and interaction effects in a football game. Therefore, comparison between expected and actual goals could help us differentiate chance occurrences and deserving performances. Scoring much more actual than expected goals would suggest a lucky outing for the team.\nxG is not commonly reported because such information is not readily available online, as the probabilities are computed based on trained models using historical data. Some sites charge fees for these data. Fortunately, Understat.com provides free access to xG statistics based on their algorithms. Data was scraped using BeautifulSoup package in python to examine whether Liverpool’s impressive season so far (31 fixtures) was a fluke or genuine prowess. You may find the web scraping code template on my github here.\n The area plot above illustrates the comparison between actual and expected goals for Liverpool this season so far. The first important point is observing how xG approximates actual goals closely most of the time, which highlights the predictive value of the metric. Another interpretation of the plot is that Liverpool was expected to score at least one goal in most matches (29 to be precise). This is not surprising given that they are second in the league’s goal rankings. This suggests that Liverpool was quite consistent in producing goal-scoring opportunities.\nThe visible grey areas indicate that Liverpool registered more actual goals than expected in some occasions. While the disparity between the two areas is mostly quite small, such differences suggest that Liverpool had luck on their side on top of their performances. The most notable gap was on matchday 15, whereby Liverpool faced Everton in the Merseyside derby. Liverpool scored 5 goals, twice the xG statistic of 2.41. Aggregation of xG revealed that Liverpool was expected to score 64.25 goals, which is less than the 70 goals they actually scored."
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#section",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#section",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "",
    "text": "Similarly, we could use the opposition’s xG metric to assess how lucky Liverpool was defensively. The figure above illustrates the comparison between actual and expected goals against. The visible blue areas indicate that Liverpool conceded less actual goals than expected, implying good fortune in these occasions. However, the visible red areas suggest that Liverpool was also unlucky during some fixtures as well. Aggregation of expected goals against revealed that Liverpool was expected to concede a total of 29.38 goals over 31 matches so far. In reality, Liverpool had the best defensive record in the league with a total of 21 conceded. Once again, the difference between the two measures suggests that luck was in Liverpool’s favour as well from the defensive perspective.  Expected Goal Difference\nSubtracting the expected goals against from xG would compute the expected goal difference for a given fixture. The visualisation above presents the actual versus expected goal difference (xDiff) for all the fixtures. A positive xDiff indicates that the team should win the game, while negative xDiff indicates an expected loss, and zero implies an expected draw. This can inform us whether the team was deserving of its result.\nAccording to the figure above, we could clearly see that Liverpool had a deserving loss on matchday 28. Liverpool lost convincingly to Watford by three goals. In that fixture, Liverpool registered their lowest xG of 0.21 and the highest expected goal against of 2.71. These statistics clearly showed that Liverpool had a very poor performance.\nWe could also use threshold values of +0.5 and -0.5 to determine whether Liverpool was anticipated to win and lose the game respectively based on their performance. The data revealed that 24 out of 31 games had an xDiff greater than 0.5, and only 2 out of 31 games had an xDiff less than -0.5. Such statistics strongly support Liverpool’s consistency in their performance throughout the season. In reality, Liverpool won a total of 28 games and lost 1 single game."
  },
  {
    "objectID": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#conclusion",
    "href": "posts/Spatial-visualisation-covid-clusters-sgp/index.html#conclusion",
    "title": "Spatial visualisation with ggmap: Covid-19 clusters in Singapore",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, xG is a valuable metric to assess the underlying performance of a team beyond the results. As the saying goes, better to be lucky than good. Indeed, the data revealed that Liverpool was lucky both offensively and defensively during certain occasions, albeit usually by small margins. However, saying that Liverpool’s success was a lucky win based on these results will be a complete misinterpretation. The xDiff data clearly shows that Liverpool exhibited strong performance consistently throughout the season and deserved their results. Therefore, a fair conclusion will be that Liverpool’s title winning success was largely attributed to their consistent strong performance, but they also had some luck on their side as well."
  },
  {
    "objectID": "posts/Liverpool-success-lucky-or-worthy-winners/index.html",
    "href": "posts/Liverpool-success-lucky-or-worthy-winners/index.html",
    "title": "Liverpool’s success: Lucky or worthy winners?",
    "section": "",
    "text": "Liverpool was announced the champions of English Premier League (EPL) season 19/20 with a record of seven games remaining. 23 points separate them from second-placed Manchester City. This post examines whether such overwhelming dominance over their rivals is attributed to sheer luck or genuine strength of the team through visualisation of expected goal data.\nFootball results do not necessarily reflect the performance of respective teams accurately. It is not always the case that the winning team has higher number of goal-scoring opportunities or greater possession than their opposition. Sometimes, teams walk away with a result merely due to good fortune. How do we differentiate between luck and performance?"
  },
  {
    "objectID": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#expected-goals",
    "href": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#expected-goals",
    "title": "Liverpool’s success: Lucky or worthy winners?",
    "section": "Expected Goals",
    "text": "Expected Goals\nEvaluation of a team’s underlying performance at goal is made possible with the expected goals (xG) metric. The metric defines the probability of a shot being scored based on multiple factors such as angle, distance and type of assist. This is well-explained with various examples in the video link here by Opta, a football data analytics company. Such measure provides us with further insights beyond the match statistics that are typically shown.\nFootball history has demonstrated that goals could be scored by players of different positions, including the goalkeeper. Some came from clear goal-scoring chances, while some were scored from very unexpected situations. For example, Asmir Begovic, goalkeeper of Stoke City, scored from his own penalty box in 2013. The xG metric would have informed us that the attempt had very low odds of hitting the back of the net. It is reasonable to consider that such abnormal and unexpected goals were just teams being lucky.\nTheoretically, the team with better goal-scoring opportunities should register more goals. However, this is not the case in reality due to various factors and interaction effects in a football game. Therefore, comparison between expected and actual goals could help us differentiate chance occurrences and deserving performances. Scoring much more actual than expected goals would suggest a lucky outing for the team.\nxG is not commonly reported because such information is not readily available online, as the probabilities are computed based on trained models using historical data. Some sites charge fees for these data. Fortunately, Understat.com provides free access to xG statistics based on their algorithms. Data was scraped using BeautifulSoup package in python to examine whether Liverpool’s impressive season so far (31 fixtures) was a fluke or genuine prowess. You may find the web scraping code template on my github here.\n The area plot above illustrates the comparison between actual and expected goals for Liverpool this season so far. The first important point is observing how xG approximates actual goals closely most of the time, which highlights the predictive value of the metric. Another interpretation of the plot is that Liverpool was expected to score at least one goal in most matches (29 to be precise). This is not surprising given that they are second in the league’s goal rankings. This suggests that Liverpool was quite consistent in producing goal-scoring opportunities.\nThe visible grey areas indicate that Liverpool registered more actual goals than expected in some occasions. While the disparity between the two areas is mostly quite small, such differences suggest that Liverpool had luck on their side on top of their performances. The most notable gap was on matchday 15, whereby Liverpool faced Everton in the Merseyside derby. Liverpool scored 5 goals, twice the xG statistic of 2.41. Aggregation of xG revealed that Liverpool was expected to score 64.25 goals, which is less than the 70 goals they actually scored.\n\nSimilarly, we could use the opposition’s xG metric to assess how lucky Liverpool was defensively. The figure above illustrates the comparison between actual and expected goals against. The visible blue areas indicate that Liverpool conceded less actual goals than expected, implying good fortune in these occasions. However, the visible red areas suggest that Liverpool was also unlucky during some fixtures as well. Aggregation of expected goals against revealed that Liverpool was expected to concede a total of 29.38 goals over 31 matches so far. In reality, Liverpool had the best defensive record in the league with a total of 21 conceded. Once again, the difference between the two measures suggests that luck was in Liverpool’s favour as well from the defensive perspective."
  },
  {
    "objectID": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#section",
    "href": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#section",
    "title": "Liverpool’s success: Lucky or worthy winners?",
    "section": "",
    "text": "Similarly, we could use the opposition’s xG metric to assess how lucky Liverpool was defensively. The figure above illustrates the comparison between actual and expected goals against. The visible blue areas indicate that Liverpool conceded less actual goals than expected, implying good fortune in these occasions. However, the visible red areas suggest that Liverpool was also unlucky during some fixtures as well. Aggregation of expected goals against revealed that Liverpool was expected to concede a total of 29.38 goals over 31 matches so far. In reality, Liverpool had the best defensive record in the league with a total of 21 conceded. Once again, the difference between the two measures suggests that luck was in Liverpool’s favour as well from the defensive perspective.  Expected Goal Difference\nSubtracting the expected goals against from xG would compute the expected goal difference for a given fixture. The visualisation above presents the actual versus expected goal difference (xDiff) for all the fixtures. A positive xDiff indicates that the team should win the game, while negative xDiff indicates an expected loss, and zero implies an expected draw. This can inform us whether the team was deserving of its result.\nAccording to the figure above, we could clearly see that Liverpool had a deserving loss on matchday 28. Liverpool lost convincingly to Watford by three goals. In that fixture, Liverpool registered their lowest xG of 0.21 and the highest expected goal against of 2.71. These statistics clearly showed that Liverpool had a very poor performance.\nWe could also use threshold values of +0.5 and -0.5 to determine whether Liverpool was anticipated to win and lose the game respectively based on their performance. The data revealed that 24 out of 31 games had an xDiff greater than 0.5, and only 2 out of 31 games had an xDiff less than -0.5. Such statistics strongly support Liverpool’s consistency in their performance throughout the season. In reality, Liverpool won a total of 28 games and lost 1 single game."
  },
  {
    "objectID": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#conclusion",
    "href": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#conclusion",
    "title": "Liverpool’s success: Lucky or worthy winners?",
    "section": "Conclusion",
    "text": "Conclusion\nIn summary, xG is a valuable metric to assess the underlying performance of a team beyond the results. As the saying goes, better to be lucky than good. Indeed, the data revealed that Liverpool was lucky both offensively and defensively during certain occasions, albeit usually by small margins. However, saying that Liverpool’s success was a lucky win based on these results will be a complete misinterpretation. The xDiff data clearly shows that Liverpool exhibited strong performance consistently throughout the season and deserved their results. Therefore, a fair conclusion will be that Liverpool’s title winning success was largely attributed to their consistent strong performance, but they also had some luck on their side as well."
  },
  {
    "objectID": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#similarly-we-could-use-the-oppositions-xg-metric-to-assess-how-lucky-liverpool-was-defensively.-the-figure-above-illustrates-the-comparison-between-actual-and-expected-goals-against.-the-visible-blue-areas-indicate-that-liverpool-conceded-less-actual-goals-than-expected-implying-good-fortune-in-these-occasions.-however-the-visible-red-areas-suggest-that-liverpool-was-also-unlucky-during-some-fixtures-as-well.-aggregation-of-expected-goals-against-revealed-that-liverpool-was-expected-to-concede-a-total-of-29.38-goals-over-31-matches-so-far.-in-reality-liverpool-had-the-best-defensive-record-in-the-league-with-a-total-of-21-conceded.-once-again-the-difference-between-the-two-measures-suggests-that-luck-was-in-liverpools-favour-as-well-from-the-defensive-perspective.-expected-goal-difference",
    "href": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#similarly-we-could-use-the-oppositions-xg-metric-to-assess-how-lucky-liverpool-was-defensively.-the-figure-above-illustrates-the-comparison-between-actual-and-expected-goals-against.-the-visible-blue-areas-indicate-that-liverpool-conceded-less-actual-goals-than-expected-implying-good-fortune-in-these-occasions.-however-the-visible-red-areas-suggest-that-liverpool-was-also-unlucky-during-some-fixtures-as-well.-aggregation-of-expected-goals-against-revealed-that-liverpool-was-expected-to-concede-a-total-of-29.38-goals-over-31-matches-so-far.-in-reality-liverpool-had-the-best-defensive-record-in-the-league-with-a-total-of-21-conceded.-once-again-the-difference-between-the-two-measures-suggests-that-luck-was-in-liverpools-favour-as-well-from-the-defensive-perspective.-expected-goal-difference",
    "title": "Liverpool’s success: Lucky or worthy winners?",
    "section": "Similarly, we could use the opposition’s xG metric to assess how lucky Liverpool was defensively. The figure above illustrates the comparison between actual and expected goals against. The visible blue areas indicate that Liverpool conceded less actual goals than expected, implying good fortune in these occasions. However, the visible red areas suggest that Liverpool was also unlucky during some fixtures as well. Aggregation of expected goals against revealed that Liverpool was expected to concede a total of 29.38 goals over 31 matches so far. In reality, Liverpool had the best defensive record in the league with a total of 21 conceded. Once again, the difference between the two measures suggests that luck was in Liverpool’s favour as well from the defensive perspective.  Expected Goal Difference",
    "text": "Similarly, we could use the opposition’s xG metric to assess how lucky Liverpool was defensively. The figure above illustrates the comparison between actual and expected goals against. The visible blue areas indicate that Liverpool conceded less actual goals than expected, implying good fortune in these occasions. However, the visible red areas suggest that Liverpool was also unlucky during some fixtures as well. Aggregation of expected goals against revealed that Liverpool was expected to concede a total of 29.38 goals over 31 matches so far. In reality, Liverpool had the best defensive record in the league with a total of 21 conceded. Once again, the difference between the two measures suggests that luck was in Liverpool’s favour as well from the defensive perspective.  Expected Goal Difference\nSubtracting the expected goals against from xG would compute the expected goal difference for a given fixture. The visualisation above presents the actual versus expected goal difference (xDiff) for all the fixtures. A positive xDiff indicates that the team should win the game, while negative xDiff indicates an expected loss, and zero implies an expected draw. This can inform us whether the team was deserving of its result.\nAccording to the figure above, we could clearly see that Liverpool had a deserving loss on matchday 28. Liverpool lost convincingly to Watford by three goals. In that fixture, Liverpool registered their lowest xG of 0.21 and the highest expected goal against of 2.71. These statistics clearly showed that Liverpool had a very poor performance.\nWe could also use threshold values of +0.5 and -0.5 to determine whether Liverpool was anticipated to win and lose the game respectively based on their performance. The data revealed that 24 out of 31 games had an xDiff greater than 0.5, and only 2 out of 31 games had an xDiff less than -0.5. Such statistics strongly support Liverpool’s consistency in their performance throughout the season. In reality, Liverpool won a total of 28 games and lost 1 single game."
  },
  {
    "objectID": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#expected-goal-difference",
    "href": "posts/Liverpool-success-lucky-or-worthy-winners/index.html#expected-goal-difference",
    "title": "Liverpool’s success: Lucky or worthy winners?",
    "section": "Expected Goal Difference",
    "text": "Expected Goal Difference\nSubtracting the expected goals against from xG would compute the expected goal difference for a given fixture. The visualisation above presents the actual versus expected goal difference (xDiff) for all the fixtures. A positive xDiff indicates that the team should win the game, while negative xDiff indicates an expected loss, and zero implies an expected draw. This can inform us whether the team was deserving of its result.\nAccording to the figure above, we could clearly see that Liverpool had a deserving loss on matchday 28. Liverpool lost convincingly to Watford by three goals. In that fixture, Liverpool registered their lowest xG of 0.21 and the highest expected goal against of 2.71. These statistics clearly showed that Liverpool had a very poor performance.\nWe could also use threshold values of +0.5 and -0.5 to determine whether Liverpool was anticipated to win and lose the game respectively based on their performance. The data revealed that 24 out of 31 games had an xDiff greater than 0.5, and only 2 out of 31 games had an xDiff less than -0.5. Such statistics strongly support Liverpool’s consistency in their performance throughout the season. In reality, Liverpool won a total of 28 games and lost 1 single game."
  },
  {
    "objectID": "posts/End-spurt-in-marathon/index.html",
    "href": "posts/End-spurt-in-marathon/index.html",
    "title": "The end spurt in marathon running: A universal behaviour?",
    "section": "",
    "text": "Pacing in endurance events is often characterised by an increase in power output or running speed towards the end, a behaviour termed the end spurt. This blog post examines the prevalence of such occurrence in Singaporean male and female runners during the Standard Chartered Singapore Marathon (SCSM) 2019."
  },
  {
    "objectID": "posts/End-spurt-in-marathon/index.html#the-end-spurt",
    "href": "posts/End-spurt-in-marathon/index.html#the-end-spurt",
    "title": "The end spurt in marathon running: A universal behaviour?",
    "section": "The End Spurt",
    "text": "The End Spurt\nIn the 2019 Boston Marathon, the fight for the first position in the men’s event came down to an epic sprint battle between Lawrence Cherono and Lelisa Desisa. Eventually, the winner and runner-up of an arduous 42.195 km race were separated by merely two seconds. The behaviour of increasing power output or running speed towards the end of an exercise bout is known as an end spurt. Such pacing behaviour is commonly observed in endurance events among both elite and recreational runners.\nThis observation has been of interest to sport scientists as it challenges the notion of fatigue. In an endurance event such as the marathon, it is expected that runners usually slow down as the distance progresses due to increase in fatigue levels. Thus, it is paradoxical that runners are able to speed up near the finish line when they are supposed to be the most fatigued. An explanation of such paradox is that individuals always exercise with reserves and regulate exercise in an anticipatory manner. As suggested in the central governor theory by renowned sport scientist, professor Timothy Noakes, anticipation is a critical component of exercise regulatory behaviour to avoid any catastrophic event happening during exercise.\nIs such pacing behaviour exhibited by all runners?\nMy previous blog post has examined the pace variation of Singaporean male and female runners of different levels who completed SCSM 2019. Using the same scraped data (see web scraping code here), this blog post examined the prevalence of the end spurt in both gender groups, and comparison was made among three groups of runners: fast, mid-pack and slow. The fast group represents the upper 20th percentile, the mid-pack consists of runners between the 40th and 60th percentile, and the slow group is made up of runners from the lower 20th percentile.\n\n\n\n\n\nFirst, let’s visualise the pacing profiles of each group of runners over the marathon distance, separated by gender. As illustrated in the figure above, the pacing profiles look quite similar across different groups. The observed pattern is characterised by an expected decrease in speed progressively over the course of the race. Interestingly, that is followed by an increase in speed right at the end of the race (last 2.195 km), demonstrating the end spurt behaviour. This seems to suggest that such behaviour can be observed in most runners regardless of their performance levels.\n\n\n\n\n\nTo examine the prevalence of the end spurt, change in average speed between the last section and the average time split at 40 km was first computed. A positive change in speed means that runners ran faster during the last section as compared to their 40 km time splits, and a negative change in speed implies slowing down instead. As shown in the box plots above, the lower quartile is more than zero, indicating a positive change in speed was found in more than 75% of runners in both gender groups. Specifically, 88% of female runners and 86% of male runners increased their speed during the last 2.195 km of the marathon. This demonstrates that the end spurt behaviour was observed in majority of runners.\n\n\n\n\n\nWhile the pacing profiles above suggest that the end spurt behaviour was observed in all levels of runners, the degree of end spurt may differ between the groups. The figure above presents the change in speed as a percentage of the runners’ 40 km time split. Statistical analysis (one-way ANOVA and post-hoc pairwise comparisons) revealed that only the slow group differed from the other two groups for both genders. For the male runners, the increase in speed among the slower runners (8.05%) was significantly less than the mid-pack (12.88%) and the faster runners (13.05%). Similarly, for the female runners, the slow group (8.55%) sped up significantly less than the mid-pack (12.82%) and the fast group (11.44%). This suggests that better performing runners tend to speed up more than their slower peers during the last section of a marathon."
  },
  {
    "objectID": "posts/End-spurt-in-marathon/index.html#anticipation-and-uncertainty",
    "href": "posts/End-spurt-in-marathon/index.html#anticipation-and-uncertainty",
    "title": "The end spurt in marathon running: A universal behaviour?",
    "section": "Anticipation and Uncertainty",
    "text": "Anticipation and Uncertainty\nThese findings demonstrated that the end spurt behaviour in a marathon was highly prevalent among runners. Given that such behaviour was exhibited regardless of gender and performance levels, it has certainly interest me to further think about this phenomenon.\n\nIs the end spurt an innate manner of regulating endurance exercise?\n\nAs mentioned earlier, a plausible explanation behind such phenomenon is that humans always exercise in an anticipatory manner. In most exercise settings, we are aware of the end point, be it expected distance or duration. Naturally, we want to reach this end point optimally and also safely as well. In the context of a marathon race, the performance goal is to reach the finish line as fast as possible. To achieve this, an athlete not only has to run at a fast pace, but also a sustainable one as well. In reality, this is a very challenging task because sustainability of pace is usually uncertain in an endurance event. This is supported by the variation in marathon running pace shown in my previous blog post. However, this uncertainty diminishes with increasing proximity to the end point. For example, an athlete at the 10 km mark will be less confident of sustaining a given pace as compared to the athlete at 40 km instead. Rationally, an athlete will not select a pace that is knowingly unsustainable as that will be detrimental to performance. Therefore, it makes sense that individuals are only inclined to increase their pace when they know they are near to the finish line.\nComparison between different groups of runners showed us that better performing runners exhibited greater end spurts as compared to the slowest group. This either implies that 1) better runners were more conservative in their pacing and hence has greater reserve capacities to tap on for the final push, or 2) better runners were able to dig deeper than their slower counterparts and thus greater ability to speed up.\nIn summary, the end spurt is ubiquitous in an endurance event such as the marathon. It remains unclear why most individuals exhibit such behaviour. The next time you find yourself speeding up near the end of an exercise bout, I hope you can share with me your rationale behind it!\nData processing, analysis and visualisation were performed on R. Full code and datasets can be found here."
  }
]